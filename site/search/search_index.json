{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"This is the technical documentation site for our open source projects. You can find out more about the team at https://openraven.com/research where there are additional tutorials and original security research papers. We run an open Slack Workspace where you can track our work, ask questions and share your ideas. We release all source code at https://github.com/openraven under an Apache 2.0 license . Projects While we are always working on a number of projects for the company, our primary focus is actively developing and maintaining Magpie . Magpie performs cloud service discovery, non-native data store discovery and cloud security configuration analysis, a set of capabilities generally referred to as Cloud Security Posture Management or CSPM. Magpie is going beyond CSPM to provide a scalable discovery engine and flexible security rules engine suitable for the modern threat landscape like cloud ransomware and cloud supply chain security issues. We also develop and maintain MockingBird , a data generation tool to create data sets for testing data classification tools as well as a set of Cloud Security Configuration Guides that explain how to fix security issues in both Amazon Web Services and Google Cloud Platform . How to Contribute We welcome contributions to our projects and documentation that adhere to our community code of conduct .","title":"Home"},{"location":"#projects","text":"While we are always working on a number of projects for the company, our primary focus is actively developing and maintaining Magpie . Magpie performs cloud service discovery, non-native data store discovery and cloud security configuration analysis, a set of capabilities generally referred to as Cloud Security Posture Management or CSPM. Magpie is going beyond CSPM to provide a scalable discovery engine and flexible security rules engine suitable for the modern threat landscape like cloud ransomware and cloud supply chain security issues. We also develop and maintain MockingBird , a data generation tool to create data sets for testing data classification tools as well as a set of Cloud Security Configuration Guides that explain how to fix security issues in both Amazon Web Services and Google Cloud Platform .","title":"Projects"},{"location":"#how-to-contribute","text":"We welcome contributions to our projects and documentation that adhere to our community code of conduct .","title":"How to Contribute"},{"location":"apache-license/","text":"Apache License Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS Copyright 2021 Open Raven You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Apache license"},{"location":"contribution-guide/","text":"We welcome all contributions to our open source tools including feedback, bug reports and pull-requests. You should first take a look at the current team [backlog}(https://github.com/openraven/magpie/issues). Our backlog is where you can learn about all the tools and features we're working on, features we are planning , what stage they're in, and when we expect to bring them to you. It is also where you should report issues and feature requests. Reporting Issues **Do not open up a GitHub issue if the bug is a security vulnerability, and instead to refer to our security policy below. Ensure the bug was not already reported by searching on GitHub under Issues for the project itself. If you're unable to find an open issue addressing the problem, open a new one. Be sure to include a title and clear description , as much relevant information as possible, and a code sample or an executable test case demonstrating the expected behavior that is not occurring if possible. Did you write a patch that fixes a bug? Open a new GitHub pull request with the patch. Ensure the PR description clearly describes the problem and solution. Include the relevant issue number if applicable. Before submitting, please read the Contributing guide to learn about our contribution policy. Do you intend to add a new feature or change an existing one? Suggest your change in the [Slack workspace) and just start writing code. Do you have questions about the source code? Ask any question about how to use a project on the Slack workspace.","title":"Contribution guide"},{"location":"contribution-guide/#reporting-issues","text":"**Do not open up a GitHub issue if the bug is a security vulnerability, and instead to refer to our security policy below. Ensure the bug was not already reported by searching on GitHub under Issues for the project itself. If you're unable to find an open issue addressing the problem, open a new one. Be sure to include a title and clear description , as much relevant information as possible, and a code sample or an executable test case demonstrating the expected behavior that is not occurring if possible.","title":"Reporting Issues"},{"location":"contribution-guide/#did-you-write-a-patch-that-fixes-a-bug","text":"Open a new GitHub pull request with the patch. Ensure the PR description clearly describes the problem and solution. Include the relevant issue number if applicable. Before submitting, please read the Contributing guide to learn about our contribution policy.","title":"Did you write a patch that fixes a bug?"},{"location":"contribution-guide/#do-you-intend-to-add-a-new-feature-or-change-an-existing-one","text":"Suggest your change in the [Slack workspace) and just start writing code.","title":"Do you intend to add a new feature or change an existing one?"},{"location":"contribution-guide/#do-you-have-questions-about-the-source-code","text":"Ask any question about how to use a project on the Slack workspace.","title":"Do you have questions about the source code?"},{"location":"security-policy/","text":"We take security seriously, we are first and foremost a security company. Like any software ours may have security issues. We take proactive steps to avoid issues such as having security savvy developers, reviewing all pull-requests and running security tools. Please ensure that you are always running the latest release version for all projects as we do not back port bugs or vulnerability fixes. All security vulnerabilities should be reported via email to opensource@openraven.com . Please do not submit vulnerabilities as Github issues. We will look and respond via email to all vulnerability reports as soon as practical (usually within 24 hours) and act on all issues we deem as needing attention. We do not run a bug bounty program but will send some fancy swag and publicly acknowledge researchers that report bugs.","title":"Security policy"},{"location":"Active%20Projects/Magpie/Policy-and-Rules-Identifiers/","text":"Policies have a policyId field and rules have a ruleId field, a reference value which is the unique identifier for a policy and a rule. To solve the problem of uniqueness in a human-readable manner we are inspired by a scheme and process that was developed to manage CVE names and numbers . By standardizing on a naming and numbering scheme the CVE developers were able to create a comprehensive list of publicly known software flaws a globally unique name to identify each vulnerability a basis for discussing priorities and the risks of vulnerabilities a way for disparate products and services to integrate vulnerability information Similarly, by standardizing on a naming and numbering scheme we can create a comprehensive list of known cloud security rules a globally unique name to identify each policy and each rule a basis for discussing policies, vulnerabilities, rules and remediation a way for a user of disparate products and services to integrate policies and rules into their products and services A policyId is made up of two parts magpie naming authority code and type and number seperated by a - (hyphen). example opnrvn-p-1 A ruleId is made up of two parts magpie naming authority code and type and number seperated by a - (hyphen). example opnrvn-r-1 Magpie Naming Authority Code A Magpie naming authority is any individual or organization that wants develop and publish policies and or public rules. Open Raven, the developers of Magpie are a Magpie naming authority using the policyId and ruleId opnrvn. While not required, we recommend people that are developing private rules also register for a namespace to avoid potential local rule clashes. If you would like to be a Magpie rule naming authority submit a PR against this page with your details and your preferred six character string. The current Magpie Naming Authorities and their respective namespaces are Open Raven - opnrvn Type a policy type is specified with a p a rule type is specified with an r Numbers Numbers should start with 1 and incrementally add with whole numbers. Additional Notes everything should be lowercase","title":"Policy and Rules Identifiers"},{"location":"Active%20Projects/Magpie/Policy-and-Rules-Identifiers/#magpie-naming-authority-code","text":"A Magpie naming authority is any individual or organization that wants develop and publish policies and or public rules. Open Raven, the developers of Magpie are a Magpie naming authority using the policyId and ruleId opnrvn. While not required, we recommend people that are developing private rules also register for a namespace to avoid potential local rule clashes. If you would like to be a Magpie rule naming authority submit a PR against this page with your details and your preferred six character string. The current Magpie Naming Authorities and their respective namespaces are Open Raven - opnrvn","title":"Magpie Naming Authority Code"},{"location":"Active%20Projects/Magpie/Policy-and-Rules-Identifiers/#type","text":"a policy type is specified with a p a rule type is specified with an r","title":"Type"},{"location":"Active%20Projects/Magpie/Policy-and-Rules-Identifiers/#numbers","text":"Numbers should start with 1 and incrementally add with whole numbers.","title":"Numbers"},{"location":"Active%20Projects/Magpie/Policy-and-Rules-Identifiers/#additional-notes","text":"everything should be lowercase","title":"Additional Notes"},{"location":"Active%20Projects/Magpie/configuration/","text":"Configuration Magpie allows for complex configurations to be enabled via the YAML-based config file. This file has 3 primary sections: Layers : each of which contain 1 or more plugins and are surrounded by at least 1 FIFO FIFOs : which are either local (in-process Java queues) or Kafka queues Plugins : Each running plugin must be explictly. A plugin-specific configuration object may reside in the config subsection. The simplest Magpie configuration is shown below. This configuration enables AWS discovery with a JSON output to stdout . To write to a file simply redirect the output: ./Active Projects > output.json Log messages are printed to stderr and will still show up as console output. The simplest configuration: layers: enumerate: type: origin queue: default plugins: - Active Projects.aws.discovery output: type: terminal dequeue: default plugins: - Active Projects.json fifos: default: type: local plugins: magpie.aws.discovery: enabled: true config: magpie.json: enabled: true config: magpie.persist: enabled: false config: hostname: localhost port: 5432 databaseName: db_name user: postgres password: postgres Saving data to PostgreSQL By default, Magpie prints all discovered resources to standard out in JSON format. It's possible to modify this behaviour by changing the default configuration. The Magpie Peristence plugin comes bundled with Magpie but is in the disabled state by default. To enable it look at the following lines in the config file (config.yaml). set enabled: true and then modify the login credentials as needed. Magpie will create all required tables at startup. magpie.persist: enabled: false config: hostname: localhost port: 5432 databaseName: db_name user: postgres password: postgres Overriding config.yaml It is possible to override most configuration values via environmental variables. This is most useful as an easy way to script a Magpie instance on a one-per-aws-service basis. To override configuration values, set an environmental variable named MAGPIE_CONFIG and with a specially formed JSON object or array. For example, to perform an S3 only scan using with the default configuration: > MAGPIE_CONFIG=\"{'/plugins/magpie.aws.discovery/config/services': ['s3']}\" ./Active Projects The value of MAGPIE_CONFIG must be a JSON object where the key is a JSON Pointer and the value is legal JSON which should be inserted into the location referenced by the pointer. In the case where multiple overrides are required you may instead use an array of the above formatted objects as such: > MAGPIE_CONFIG=\"[{'/plugins/magpie.aws.discovery/enabled', false }, {'/plugins/magpie.aws.discovery/config/services': ['s3']}]\" ./Active Projects Multiple Overrides If you have multiple values to set it may be easier to set multiple override variables instead of attempting to fit it in a single env var. Magpie will accept any and all environmental variables that match the regex MAGPIE_CONFIG.* . They will be applied in Java's natural String ordering (lexicographic). For example: > export MAGPIE_CONFIG_1=\"[...]\" > export MAGPIE_CONFIG_2=\"[...]\" > ./Active Projects Both variables will be applied, if any duplicate JSON Pointers are provided the last one applied will win.","title":"Configuration"},{"location":"Active%20Projects/Magpie/configuration/#configuration","text":"Magpie allows for complex configurations to be enabled via the YAML-based config file. This file has 3 primary sections: Layers : each of which contain 1 or more plugins and are surrounded by at least 1 FIFO FIFOs : which are either local (in-process Java queues) or Kafka queues Plugins : Each running plugin must be explictly. A plugin-specific configuration object may reside in the config subsection. The simplest Magpie configuration is shown below. This configuration enables AWS discovery with a JSON output to stdout . To write to a file simply redirect the output: ./Active Projects > output.json Log messages are printed to stderr and will still show up as console output. The simplest configuration: layers: enumerate: type: origin queue: default plugins: - Active Projects.aws.discovery output: type: terminal dequeue: default plugins: - Active Projects.json fifos: default: type: local plugins: magpie.aws.discovery: enabled: true config: magpie.json: enabled: true config: magpie.persist: enabled: false config: hostname: localhost port: 5432 databaseName: db_name user: postgres password: postgres","title":"Configuration"},{"location":"Active%20Projects/Magpie/configuration/#saving-data-to-postgresql","text":"By default, Magpie prints all discovered resources to standard out in JSON format. It's possible to modify this behaviour by changing the default configuration. The Magpie Peristence plugin comes bundled with Magpie but is in the disabled state by default. To enable it look at the following lines in the config file (config.yaml). set enabled: true and then modify the login credentials as needed. Magpie will create all required tables at startup. magpie.persist: enabled: false config: hostname: localhost port: 5432 databaseName: db_name user: postgres password: postgres","title":"Saving data to PostgreSQL"},{"location":"Active%20Projects/Magpie/configuration/#overriding-configyaml","text":"It is possible to override most configuration values via environmental variables. This is most useful as an easy way to script a Magpie instance on a one-per-aws-service basis. To override configuration values, set an environmental variable named MAGPIE_CONFIG and with a specially formed JSON object or array. For example, to perform an S3 only scan using with the default configuration: > MAGPIE_CONFIG=\"{'/plugins/magpie.aws.discovery/config/services': ['s3']}\" ./Active Projects The value of MAGPIE_CONFIG must be a JSON object where the key is a JSON Pointer and the value is legal JSON which should be inserted into the location referenced by the pointer. In the case where multiple overrides are required you may instead use an array of the above formatted objects as such: > MAGPIE_CONFIG=\"[{'/plugins/magpie.aws.discovery/enabled', false }, {'/plugins/magpie.aws.discovery/config/services': ['s3']}]\" ./Active Projects","title":"Overriding config.yaml"},{"location":"Active%20Projects/Magpie/configuration/#multiple-overrides","text":"If you have multiple values to set it may be easier to set multiple override variables instead of attempting to fit it in a single env var. Magpie will accept any and all environmental variables that match the regex MAGPIE_CONFIG.* . They will be applied in Java's natural String ordering (lexicographic). For example: > export MAGPIE_CONFIG_1=\"[...]\" > export MAGPIE_CONFIG_2=\"[...]\" > ./Active Projects Both variables will be applied, if any duplicate JSON Pointers are provided the last one applied will win.","title":"Multiple Overrides"},{"location":"Active%20Projects/Magpie/dmap-extension/","text":"DMAP Runing magpie-dmap requires that DMAP-Predictions be running locally. The easiest way is through Docker: docker run -p 10234:10234 -d quay.io/openraven/dmap-predictions DMAP Requirements IAM roles for creating lambda roles and managing lambda functions EC2 assets persisted into PostgreSQL (via the Magpie peristence plugin) Locally running DMAP-Predictions service","title":"Dmap extension"},{"location":"Active%20Projects/Magpie/dmap-extension/#dmap","text":"Runing magpie-dmap requires that DMAP-Predictions be running locally. The easiest way is through Docker: docker run -p 10234:10234 -d quay.io/openraven/dmap-predictions","title":"DMAP"},{"location":"Active%20Projects/Magpie/dmap-extension/#dmap-requirements","text":"IAM roles for creating lambda roles and managing lambda functions EC2 assets persisted into PostgreSQL (via the Magpie peristence plugin) Locally running DMAP-Predictions service","title":"DMAP Requirements"},{"location":"Active%20Projects/Magpie/getting-started/","text":"Magpie Magpie is a free, open-source framework and a collection of community developed plugins that can be used to build complete end-to-end security tools such as a CSPM or Cloud Security Posture Manager. The project was originally created and is maintained by Open Raven. We build commercial cloud native data security tools and in doing so have learned a great deal about how to discover AWS assets and their security settings at scale. We also heard that many people were frustrated with their existing security tools that couldn't be extended and couldn't work well with their other systems, so decided to create this Magpie framework and refactor and sync our core AWS commercial discovery code as the first plugin. We plan to actively contribute additional modules to make Magpie a credible free open source alternative to commercial CSPM\u2019s and welcome the community to join us in adding to the framework and building plugins. Magpie also contains Open Raven's DMAP technology, which allows users to enumerate and identify non native services running on EC2 instances using a combination of port fingerprinting (think Nmap's OS fingerprinting, but on the application layer instead of the transport layer) and a little machine learning (decision trees). Overview Prereqs Java 11 is a prerequisite and must be installed to run Magpie. Out of the box Magpie supports AWS for the cloud provider and outputs discovery data to stdout in JSON format. The AWS plugin utilizes the AWS Java SDK and will search for credentials as described in Using Credentials . Assuming you have read credentials set up, you can start discovery by running: ./Active Projects Running via Docker Builds on the main branch are automatically uploaded to quay.io under the latest tag. See https://quay.io/repository/openraven/magpie for all available tags. Once a stable public release is made this will also be available there. The Docker image uses the default config and provides no AWS credentials. Credentials can be added at runtime via volume mapping or passing environmental variables into the container. Configuration overrides can be done via environmental variable. For example: to pass environmental variables for both credentials and configuration: docker run -a stdout -a stderr \\ --env MAGPIE_CONFIG=\"{'/plugins/magpie.aws.discovery/config/services': ['s3']}\" \\ -e AWS_ACCESS_KEY_ID -e AWS_SECRET_ACCESS_KEY -e AWS_SESSION_TOKEN \\ quay.io/openraven/Active Projects:latest The two -a arguments map both stdout and stderr to your local terminals, so you can redirect output as you would with a local Mapgie execution. Plugins Community Contributed Plugins If you've written a plugin you'd like listed please create a Pull Request with it listed here. Authentcation To use this plugin save .json file with authentication key to file then set environment variable: export GOOGLE_APPLICATION_CREDENTIALS=PATH_TO_CREDENTIALS_JSON_FILE The Magpie repo is https://github.com/openraven/magpie . The security rules repo is https://github.com/openraven/security-rules . Remediation advice can be found in our Cloud Security Configuration Guide .","title":"Getting started"},{"location":"Active%20Projects/Magpie/getting-started/#magpie","text":"Magpie is a free, open-source framework and a collection of community developed plugins that can be used to build complete end-to-end security tools such as a CSPM or Cloud Security Posture Manager. The project was originally created and is maintained by Open Raven. We build commercial cloud native data security tools and in doing so have learned a great deal about how to discover AWS assets and their security settings at scale. We also heard that many people were frustrated with their existing security tools that couldn't be extended and couldn't work well with their other systems, so decided to create this Magpie framework and refactor and sync our core AWS commercial discovery code as the first plugin. We plan to actively contribute additional modules to make Magpie a credible free open source alternative to commercial CSPM\u2019s and welcome the community to join us in adding to the framework and building plugins. Magpie also contains Open Raven's DMAP technology, which allows users to enumerate and identify non native services running on EC2 instances using a combination of port fingerprinting (think Nmap's OS fingerprinting, but on the application layer instead of the transport layer) and a little machine learning (decision trees).","title":"Magpie"},{"location":"Active%20Projects/Magpie/getting-started/#overview","text":"","title":"Overview"},{"location":"Active%20Projects/Magpie/getting-started/#prereqs","text":"Java 11 is a prerequisite and must be installed to run Magpie. Out of the box Magpie supports AWS for the cloud provider and outputs discovery data to stdout in JSON format. The AWS plugin utilizes the AWS Java SDK and will search for credentials as described in Using Credentials . Assuming you have read credentials set up, you can start discovery by running: ./Active Projects","title":"Prereqs"},{"location":"Active%20Projects/Magpie/getting-started/#running-via-docker","text":"Builds on the main branch are automatically uploaded to quay.io under the latest tag. See https://quay.io/repository/openraven/magpie for all available tags. Once a stable public release is made this will also be available there. The Docker image uses the default config and provides no AWS credentials. Credentials can be added at runtime via volume mapping or passing environmental variables into the container. Configuration overrides can be done via environmental variable. For example: to pass environmental variables for both credentials and configuration: docker run -a stdout -a stderr \\ --env MAGPIE_CONFIG=\"{'/plugins/magpie.aws.discovery/config/services': ['s3']}\" \\ -e AWS_ACCESS_KEY_ID -e AWS_SECRET_ACCESS_KEY -e AWS_SESSION_TOKEN \\ quay.io/openraven/Active Projects:latest The two -a arguments map both stdout and stderr to your local terminals, so you can redirect output as you would with a local Mapgie execution.","title":"Running via Docker"},{"location":"Active%20Projects/Magpie/getting-started/#plugins","text":"","title":"Plugins"},{"location":"Active%20Projects/Magpie/getting-started/#community-contributed-plugins","text":"If you've written a plugin you'd like listed please create a Pull Request with it listed here.","title":"Community Contributed Plugins"},{"location":"Active%20Projects/Magpie/getting-started/#authentcation","text":"To use this plugin save .json file with authentication key to file then set environment variable: export GOOGLE_APPLICATION_CREDENTIALS=PATH_TO_CREDENTIALS_JSON_FILE The Magpie repo is https://github.com/openraven/magpie . The security rules repo is https://github.com/openraven/security-rules . Remediation advice can be found in our Cloud Security Configuration Guide .","title":"Authentcation"},{"location":"Active%20Projects/Magpie/rules-writing/","text":"Welcome to the Open Raven Research wiki for all things related to the security rules that power Magpie and are incorporated in the Open Raven commercial platform. The wiki includes documentation about how to write policies and rules as well as a section of curated guides about how to fix problems that may have been identified after running an analysis. We also maintain a current directory page listing all the rules hosted in this repo and a section listing other public repos that share rules. Policies and rules directory (@TODO) Writing policies and rules Policy and Rules overview Policies and rules consist of crafted YAML files within a Git repository, one file per policy or rule. Policy Fields Policies contain the following fields. All fields are required unless specifically marked optional. Field Data Type Description Sample Required? policyId string A unique identifier provided by Open Raven opnrvn-p-1 yes policyName string Human readable title for the policy AWS Security Best Practices yes cloudProvider enum The provider this policy should be applied against. aws yes description string A sentence-long description of the policy yes enabled boolean Whether this policy should be applied when run in Magpie true optional, default to true rules array of strings Rule filenames (the /rules path is implied) - opnrvn-r-1.yaml yes version string Version number for the policy 0.9 yes Rule Fields Rules contain the following fields. All fields are required unless specifically marked optional. Field Data Type Description Sample Required? ruleId string A numerically increasing serial following the defined naming format opnrvn-r-12 yes type enum Currently only one type is supported (asset) asset yes ruleName string Human readable title for the rule AWS security group allows access to known command and control destinations yes description string A sentence-long description of the rule yes severity enum One of high , medium , low high yes enabled boolean Whether this rule should be applied when run in Magpie true optional, default to true sql SQL string A sql query that returns assets that violate this policy yes eval Python code Python code that performs additional processing on the returned SQL results optional version string Version number for the rule 0.9 yes Rule naming TODO: Rule naming authorities and the RuleID field SQL Schema and Queries Magpie rules require SQL queries (unless the rule is is set to be a manual control). These SQL queries are always executed against the PostgreSQL service where assets are persisted during the discovery phase. PostgreSQL is the only database supported by Magpie and as such rules can use PG specific query features (such as https://www.postgresql.org/docs/13/functions-json.html). Rules should not modify the database, but checks are currently done to ensure this. As such only consume rules from trusted sources. The current schema is as follows: Column Type Description document_id varchar(59) primary key asset_id varchar(255) resource_name varchar(255) resource_id varchar(255) resource_type varchar(255) region varchar(50) project_id varchar(255) account_id varchar(255) created_iso timestamp with time zone updated_iso timestamp with time zone discovery_session_id varchar(255) max_size_in_bytes integer size_in_bytes integer configuration jsonb supplementary_configuration jsonb tags jsonb discovery_meta jsonb ### Python for Advanced Rules TODO Testing security rules Structure Security rules could be tested using prepared tests assets which reproduce scenario violation see the security-rules/resources/tests Each tests consists of * ruleId : matching test assets to the specific rule, relation is always one-to-one * description : about how the test scenario prepared and why it violates the rule * violatedAssetId : expected violated assed being returned by specified rule * controlAssetId : asset which pass the rule * insecureAssetGroup : list of assets in JSON format which simulate insecure environment from rule perspective * secureAssetGroup : list of assets in JSON format which simulate secure environment from rule perspective Rules logic mostly relies on relation between assets, hence specified rule will be executed against provided assets Creating test resource grab the required assets type by magpie-discovery. You could use magpie-quickrun, enable json and postgres output redirect JSON formatted assets from the error output to the file, ensure discovery assets are also persisted in DB execute the rule SQL against the DB, discovered asset_id is the one which violated the rule, and should be filled under the field violatedAssetId in related test resource find the asset definition in the output file from the point 2, save it to the insecureAssetGroup list. In case if there a graph of assets which takes part in violation save them all check the DB for other assets related to the same rule which pass it. Them should be saved under the secureAssetGroup list. put the safe assetId under the test resource field controlAssetId . Which will point further to safe asset setup Execution Test scenario for security rules resides in magpie. (Search in magpie repository for SecurityRuleValidator class). We use Junit5 Parameterized tests, were testcase is executed for each provided test resource file Shortly test perform the following: * getting the security-rules repo, and build stream of testresource files * load policies with rules * foreach test resource file: - get the referenced rule from test resource file - insert insecure asset group to testcontainer DB - execute rule against assets, and ensure expected violated asset returned - cleanup DB - execute secure group of assets - execute rule once again, no assets should be returned violating the rule Developer able to run the testcase in following modes: * using local repo specifying the parameter -Drepository=<absolute-path-to-local-git-repo> - execute all tests in repository. Useful for the full scope testing on branch level and playing with assets definition. * using local test-resource path specifying the parameter -DtestResourcePath=<absolute-path-to-resource> - execute all tests in folder or a single test if provided path is file. Useful for single rule testing or current list of rules in terms of their development to not waste time for full scope testing. * default execution without any parameters - checkout the master branch of security-rules repo and execute it Build Security-rules testing is triggered on each PR and merge to master for security-rules and magpie repo. How-to guides for fixing cloud security issues AWS - Amazon Web Services GCP - Google Cloud Platform Additional help and information is available on the Open Raven research site including screencasts and videos covering these topics and more. We encourage people to join our public Slack channel to follow development and ask questions.","title":"Rules writing"},{"location":"Active%20Projects/Magpie/rules-writing/#policies-and-rules-directory-todo","text":"","title":"Policies and rules directory (@TODO)"},{"location":"Active%20Projects/Magpie/rules-writing/#writing-policies-and-rules","text":"","title":"Writing policies and rules"},{"location":"Active%20Projects/Magpie/rules-writing/#policy-and-rules-overview","text":"Policies and rules consist of crafted YAML files within a Git repository, one file per policy or rule.","title":"Policy and Rules overview"},{"location":"Active%20Projects/Magpie/rules-writing/#policy-fields","text":"Policies contain the following fields. All fields are required unless specifically marked optional. Field Data Type Description Sample Required? policyId string A unique identifier provided by Open Raven opnrvn-p-1 yes policyName string Human readable title for the policy AWS Security Best Practices yes cloudProvider enum The provider this policy should be applied against. aws yes description string A sentence-long description of the policy yes enabled boolean Whether this policy should be applied when run in Magpie true optional, default to true rules array of strings Rule filenames (the /rules path is implied) - opnrvn-r-1.yaml yes version string Version number for the policy 0.9 yes","title":"Policy Fields"},{"location":"Active%20Projects/Magpie/rules-writing/#rule-fields","text":"Rules contain the following fields. All fields are required unless specifically marked optional. Field Data Type Description Sample Required? ruleId string A numerically increasing serial following the defined naming format opnrvn-r-12 yes type enum Currently only one type is supported (asset) asset yes ruleName string Human readable title for the rule AWS security group allows access to known command and control destinations yes description string A sentence-long description of the rule yes severity enum One of high , medium , low high yes enabled boolean Whether this rule should be applied when run in Magpie true optional, default to true sql SQL string A sql query that returns assets that violate this policy yes eval Python code Python code that performs additional processing on the returned SQL results optional version string Version number for the rule 0.9 yes","title":"Rule Fields"},{"location":"Active%20Projects/Magpie/rules-writing/#rule-naming","text":"TODO: Rule naming authorities and the RuleID field","title":"Rule naming"},{"location":"Active%20Projects/Magpie/rules-writing/#sql-schema-and-queries","text":"Magpie rules require SQL queries (unless the rule is is set to be a manual control). These SQL queries are always executed against the PostgreSQL service where assets are persisted during the discovery phase. PostgreSQL is the only database supported by Magpie and as such rules can use PG specific query features (such as https://www.postgresql.org/docs/13/functions-json.html). Rules should not modify the database, but checks are currently done to ensure this. As such only consume rules from trusted sources. The current schema is as follows: Column Type Description document_id varchar(59) primary key asset_id varchar(255) resource_name varchar(255) resource_id varchar(255) resource_type varchar(255) region varchar(50) project_id varchar(255) account_id varchar(255) created_iso timestamp with time zone updated_iso timestamp with time zone discovery_session_id varchar(255) max_size_in_bytes integer size_in_bytes integer configuration jsonb supplementary_configuration jsonb tags jsonb discovery_meta jsonb ### Python for Advanced Rules TODO","title":"SQL Schema and Queries"},{"location":"Active%20Projects/Magpie/rules-writing/#testing-security-rules","text":"","title":"Testing security rules"},{"location":"Active%20Projects/Magpie/rules-writing/#structure","text":"Security rules could be tested using prepared tests assets which reproduce scenario violation see the security-rules/resources/tests Each tests consists of * ruleId : matching test assets to the specific rule, relation is always one-to-one * description : about how the test scenario prepared and why it violates the rule * violatedAssetId : expected violated assed being returned by specified rule * controlAssetId : asset which pass the rule * insecureAssetGroup : list of assets in JSON format which simulate insecure environment from rule perspective * secureAssetGroup : list of assets in JSON format which simulate secure environment from rule perspective Rules logic mostly relies on relation between assets, hence specified rule will be executed against provided assets","title":"Structure"},{"location":"Active%20Projects/Magpie/rules-writing/#creating-test-resource","text":"grab the required assets type by magpie-discovery. You could use magpie-quickrun, enable json and postgres output redirect JSON formatted assets from the error output to the file, ensure discovery assets are also persisted in DB execute the rule SQL against the DB, discovered asset_id is the one which violated the rule, and should be filled under the field violatedAssetId in related test resource find the asset definition in the output file from the point 2, save it to the insecureAssetGroup list. In case if there a graph of assets which takes part in violation save them all check the DB for other assets related to the same rule which pass it. Them should be saved under the secureAssetGroup list. put the safe assetId under the test resource field controlAssetId . Which will point further to safe asset setup","title":"Creating test resource"},{"location":"Active%20Projects/Magpie/rules-writing/#execution","text":"Test scenario for security rules resides in magpie. (Search in magpie repository for SecurityRuleValidator class). We use Junit5 Parameterized tests, were testcase is executed for each provided test resource file Shortly test perform the following: * getting the security-rules repo, and build stream of testresource files * load policies with rules * foreach test resource file: - get the referenced rule from test resource file - insert insecure asset group to testcontainer DB - execute rule against assets, and ensure expected violated asset returned - cleanup DB - execute secure group of assets - execute rule once again, no assets should be returned violating the rule Developer able to run the testcase in following modes: * using local repo specifying the parameter -Drepository=<absolute-path-to-local-git-repo> - execute all tests in repository. Useful for the full scope testing on branch level and playing with assets definition. * using local test-resource path specifying the parameter -DtestResourcePath=<absolute-path-to-resource> - execute all tests in folder or a single test if provided path is file. Useful for single rule testing or current list of rules in terms of their development to not waste time for full scope testing. * default execution without any parameters - checkout the master branch of security-rules repo and execute it","title":"Execution"},{"location":"Active%20Projects/Magpie/rules-writing/#build","text":"Security-rules testing is triggered on each PR and merge to master for security-rules and magpie repo.","title":"Build"},{"location":"Active%20Projects/Magpie/rules-writing/#how-to-guides-for-fixing-cloud-security-issues","text":"AWS - Amazon Web Services GCP - Google Cloud Platform Additional help and information is available on the Open Raven research site including screencasts and videos covering these topics and more. We encourage people to join our public Slack channel to follow development and ask questions.","title":"How-to guides for fixing cloud security issues"},{"location":"Active%20Projects/Mockingbird/getting-started/","text":"How To .. _installation: Installation To use XXX, first install it using pip: .. code-block:: console (.venv) $ pip install XXX Stuff","title":"Getting started"},{"location":"Active%20Projects/Mockingbird/getting-started/#how-to","text":".. _installation:","title":"How To"},{"location":"Active%20Projects/Mockingbird/getting-started/#installation","text":"To use XXX, first install it using pip: .. code-block:: console (.venv) $ pip install XXX","title":"Installation"},{"location":"Active%20Projects/Mockingbird/getting-started/#stuff","text":"","title":"Stuff"},{"location":"Configuration%20Guides/aws-security/","text":"Ensure CloudTrail trails are integrated with CloudWatch Logs AWS CloudTrail is a web service that records AWS API calls made in a given AWS account. The recorded information includes the identity of the API caller, the time of the API call, the source IP address of the API caller, the request parameters, and the response elements returned by the AWS service. CloudTrail uses Amazon S3 for log file storage and delivery, so log files are stored durably. In addition to capturing CloudTrail logs within a specified S3 bucket for long term analysis, realtime analysis can be performed by configuring CloudTrail to send logs to CloudWatch Logs. For a trail that is enabled in all regions in an account, CloudTrail sends log files from all those regions to a CloudWatch Logs log group. It is recommended that CloudTrail logs be sent to CloudWatch Logs Perform the following to establish the prescribed state: Via the AWS management Console 1. Sign in to the AWS Management Console and open the CloudTrail console at https://console.aws.amazon.com/cloudtrail/ 2. Under All Buckets, click on the target bucket you wish to evaluate 3. Click Properties on the top right of the console 4. Click Trails in the left menu 5. Click on each trail where no CloudWatch Logs are defined 6. Go to the CloudWatch Logs section and click on Configure 7. Define a new or select an existing log group 8. Click on Continue 9. Configure IAM Role which will deliver CloudTrail events to CloudWatch Logs o Create/Select an IAM Role and Policy Name o Click Allow to continue Via CLI aws cloudtrail update-trail --name --cloudwatch-logs-log-grouparn --cloudwatch-logs-role-arn Ensure a support role has been created to manage incidents with AWS Support AWS provides a support center that can be used for incident notification and response, as well as technical support and customer services. Create an IAM Role to allow authorized users to manage incidents with AWS Support. Using the Amazon unified command line interface: - Create an IAM role for managing incidents with AWS: - Create a trust relationship policy document that allows to manage AWS incidents, and save it locally as /tmp/TrustPolicy.json: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::321859670049:root\" }, \"Action\": \"sts:AssumeRole\" } ] } - Create the IAM role using the above trust policy: aws iam create-role --role-name --assume-rolepolicy-document file:///tmp/TrustPolicy.json - Attach 'AWSSupportAccess' managed policy to the created IAM role: aws iam attach-role-policy --policy-arn --role-name Ensure all EC2 instances are managed by SSM Systems Manager helps you maintain security and compliance by scanning your managed instances and reporting on (or taking corrective action on) any policy violations it detects. In case if SSM is enabled for specific region but EC2 instances are not managed - they will not be monitored from any kind of SSM perspective Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. In the navigation pane, under Instances, choose Instances. Navigate to and choose your EC2 instance from the list. In the Actions menu, choose Security, Modify IAM role. For IAM role, select the instance profile you created for that perspective Ensure that SecurityHub enabled for required region AWS Security Hub gives you a comprehensive view of your security alerts and security posture across your AWS accounts. SecurityHub is a single place that manages AWS Security Hub gives you a comprehensive view of your security alerts and security posture across your AWS accounts. SecurityHub is a single place that manages AWS security group allows access to known command and control destinations The AWS security group allows access to destinations that are known or suspected of being command and control systems used in ransomware and botnet attacks. In the AWS Management Console, go to EC2->Security Groups. For each group found by this rule: 1) Select the given rule 2) Choose the 'Outbound Rules' tab 3) Click 'Edit outbound rules' 4) Edit any CIDR / IP addresses and minimize the permitted scope to just the minimum required for connectivity. Ensure security contact information is registered AWS provides customers with the option of specifying the contact information for account's security team. It is recommended that this information be provided. Perform the following in the AWS Management Console to establish security contact information: 1. Click on your account name at the top right corner of the console. 2. From the drop-down menu Click My Account 3. Scroll down to the Alternate Contacts section 4. Enter contact information in the Security section Note: Consider specifying an internal email distribution list to ensure emails are regularly monitored by more than one individual. Ensure that S3 Bucket has MFA option enabled for changing Bucket Versioning settings and permanently deleting object versions S3 Buckets should be protected from ransomware attacks, by configuring versioning and MFA Delete, in order to disallow immediate bucket content removal to encryption, or any other harmful modifications Disabled versioning is also considered as violation by this rule since attacker may make the bucket vulnerable by disabling object versioning with the s3:PutBucketVersioning permission Perform the following to enable S3 bucket versioning and enable mfaDevice 1. Sign in to the AWS Management Console and open the S3 console at https://console.aws.amazon.com/s3. 2. Under All Buckets click on the target S3 bucket 3. Click on Properties tab 4. Find \"Bucket Versioning\" section 5. Click Edit and enable versioning 6. Follow AWS instruction to add MFA Delete option via CLI or SDK Ensure S3 bucket deny overriding of default KMS Key encryption Attackers which have S3:PutObject permission are still able to override default KMS Key encryption on updating object operation with their own provided KMS Key, thus lead ransomware violation Bucket owner should define policy to allow objects modification only using the defined default KMS Key, which attack unlikely have permissions to change or modify Perform the following to enable S3 bucket policy control over KMS Key: Via the Management Console 1. Sign in to the AWS Management Console and open the S3 console at https://console.aws.amazon.com/s3. 2. In the Buckets list, choose the name of the bucket that you want to create a bucket policy for or whose bucket policy you want to edit. 3. Choose Permissions. 4. Under Bucket policy, choose Edit. 5. In the Policy text field, type or copy and paste a new bucket policy, or edit an existing policy. The bucket policy is a JSON file. The text you type in the editor must be valid JSON. 6. Add policy Condition statement where you define the KMS Key to be matched for Statement with s3:PutObject permission 7. Condition example for Allow effect for: \"Condition\":{\"StringEquals\":{\"s3:x-amz-server-side-encryption-aws-kms-key-id\":\"arn:aws:kms:REGION:ACCOUNT-ID:key\\/KEY-ID\"}}} https://rhinosecuritylabs.com/aws/s3-ransomware-part-2-prevention-and-defense/#:~:text=Another%20feature%20of,be%20found%20here. https://docs.aws.amazon.com/AmazonS3/latest/userguide/add-bucket-policy.html https://docs.aws.amazon.com/service-authorization/latest/reference/list_amazons3.html Ensure that S3 Bucket restrict public access by ACL and policy S3 Buckets should not be reachable outside of the project by default, violation if that rule could cause major vulnerabilities and data loss Perform the following to restrict public access to S3 bucket 1. Sign in to the AWS Management Console and open the S3 console at https://console.aws.amazon.com/s3. 2. On the left side menu find option \"Block Public Access settings for this account\" 3. Edit and click on required level of restriction. Block all public access is preferable. Will set it globally 4. Open bucket, then open tab \"Permissions\" 5. Find \"Access control list (ACL)\" part and ensure bucket owner is only have access 6. Find \"Bucket policy\" check for any public configuration like \"principal\": * etc 7. Reference: https://docs.amazonaws.cn/en_us/AmazonS3/latest/userguide/access-control-block-public-access.html 8. Reference: https://aws.amazon.com/s3/features/block-public-access/ Ensure S3 bucket has no server-side encryption being enabled by another account S3 bucket policies allow uploaded files to be encrypted with AES256, or forcing any uploaded file to be encrypted with a specific AWS KMS key. In the case when a specified KMS Key belongs to a different account it leads to a high risk of encryption of the data without the ability to decrypt it. Perform the following to clear cross-account KMS Key on S3 Via the Management Console 1. Sign in to the AWS Management Console and open the S3 console at https://console.aws.amazon.com/s3. 2. In the Buckets list, choose the name of the bucket that you want. 3. Choose Properties. 4. Under Default encryption, choose Edit. 5. Remove cross-account KMS Key and specify account related KMS Key for data encryption 6. Consider rule aws_058 aws_s3_prevent_default_kms_key_override.yaml remediation to prevent such violation on future Ensure the S3 bucket used to store CloudTrail logs is not publicly accessible CloudTrail logs a record of every API call made in your AWS account. These logs file are stored in an S3 bucket. It is recommended that the bucket policy, or access control list (ACL), applied to the S3 bucket that CloudTrail logs to prevents public access to the CloudTrail logs. Perform the following to remove any public access that has been granted to the bucket via an ACL or S3 bucket policy: 1. Go to Amazon S3 console at https://console.aws.amazon.com/s3/home 2. Right-click on the bucket and click Properties 3. In the Properties pane, click the Permissions tab. 4. The tab shows a list of grants, one row per grant, in the bucket ACL. Each row identifies the grantee and the permissions granted. 5. Select the row that grants permission to Everyone or Any Authenticated User 6. Uncheck all the permissions granted to Everyone or Any Authenticated User (click x to delete the row). 7. Click Save to save the ACL. 8. If the Edit bucket policy button is present, click it. 9. Remove any Statement having an Effect set to Allow and a Principal set to \" \" or {\"AWS\" : \" \"}. Ensure S3 bucket access logging is enabled on the CloudTrail S3 bucket S3 Bucket Access Logging generates a log that contains access records for each request made to your S3 bucket. An access log record contains details about the request, such as the request type, the resources specified in the request worked, and the time and date the request was processed. It is recommended that bucket access logging be enabled on the CloudTrail S3 bucket. Perform the following to enable S3 bucket logging: Via the Management Console 1. Sign in to the AWS Management Console and open the S3 console at https://console.aws.amazon.com/s3. 2. Under All Buckets click on the target S3 bucket 3. Click on Properties in the top right of the console 4. Under Bucket: click on Logging 5. Configure bucket logging 1. Click on Enabled checkbox 2. Select Target Bucket from list 3. Enter a Target Prefix 6. Click Save Ensure rotation for customer created CMKs is enabled AWS Key Management Service (KMS) allows customers to rotate the backing key which is key material stored within the KMS which is tied to the key ID of the Customer Created customer master key (CMK). It is the backing key that is used to perform cryptographic operations such as encryption and decryption. Automated key rotation currently retains all prior backing keys so that decryption of encrypted data can take place transparently. It is recommended that CMK key rotation be enabled Via the Management Console: 1. Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam. 2. In the left navigation pane, choose Encryption Keys . 3. Select a customer created master key (CMK) 4. Under the Key Policy section, move down to Key Rotation . 5. Check the Rotate this key every year checkbox. Via CLI 1. Run the following command to enable key rotation: aws kms enable-key-rotation --key-id Ensure access keys are rotated every 90 days or less Access keys consist of an access key ID and secret access key, which are used to sign programmatic requests that you make to AWS. AWS users need their own access keys to make programmatic calls to AWS from the AWS Command Line Interface (AWS CLI), Tools for Windows PowerShell, the AWS SDKs, or direct HTTP calls using the APIs for individual AWS services. It is recommended that all access keys be regularly rotated. Perform the following to rotate access keys: 1. Login to the AWS Management Console: 2. Click Services 3. Click IAM 4. Click on Users 5. Click on Security Credentials 6. As an Administrator - Click on Make Inactive for keys that have not been rotated in 90 Days 7. As an IAM User - Click on Make Inactive or Delete for keys which have not been rotated or used in 90 Days 8. Click on Create Access Key 9. Update programmatic call with new Access Key credentials Or Via CLI: aws iam update-access-key aws iam create-access-key aws iam delete-access-key Ensure routing tables for VPC peering are \"least access\" Once a VPC peering connection is established, routing tables must be updated to establish any connections between the peered VPCs. These routes can be as specific as desired - even peering a VPC to only a single host on the other side of the connection. Remove and add route table entries to ensure that the least number of subnets or hosts as is required to accomplish the purpose for peering are routable. Via CLI: 1. For each containing routes non compliant with your routing policy (which grants more than desired \"least access\"), delete the non compliant route: aws ec2 delete-route --route-table-id --destination-cidrblock 2. Create a new compliant route: aws ec2 create-route --route-table-id --destination-cidrblock --vpc-peering-connection-id Ensure no security groups allow ingress from 0.0.0.0/0 to port 3389 Security groups provide stateful filtering of ingress/egress network traffic to AWS resources. It is recommended that no security group allows unrestricted ingress access to port 3389. Perform the following to implement the prescribed state: 1. Login to the AWS Management Console at https://console.aws.amazon.com/vpc/home 2. In the left pane, click Security Groups 3. For each security group, perform the following: 4. Select the security group 5. Click the Inbound Rules tab 6. Identify the rules to be removed 7. Click the x in the Remove column 8. Click Save Ensure no security groups allow ingress from 0.0.0.0/0 to port 22 Security groups provide stateful filtering of ingress/egress network traffic to AWS resources. It is recommended that no security group allows unrestricted ingress access to port 22. Perform the following to implement the prescribed state: 1. Login to the AWS Management Console at https://console.aws.amazon.com/vpc/home 2. In the left pane, click Security Groups 3. For each security group, perform the following: 4. Select the security group 5. Click the Inbound Rules tab 6. Identify the rules to be removed 7. Click the x in the Remove column 8. Click Save Ensure no root account access key exists The root account is the most privileged user in an AWS account. AWS Access Keys provide programmatic access to a given AWS account. It is recommended that all access keys associated with the root account be removed. Sign in to the AWS Management Console as Root and open the IAM console at https://console.aws.amazon.com/iam/. Click on at the top right and select Security Credentials from the drop down list On the pop out screen Click on Continue to Security Credentials Click on Access Keys (Access Key ID and Secret Access Key) Under the Status column if there are any Keys which are Active Click on Make Inactive - (Temporarily disable Key - may be needed again) Click Delete - (Deleted keys cannot be recovered) Ensure IAM policies that allow full \"*:*\" administrative privileges are not created IAM policies are the means by which privileges are granted to users, groups, or roles. It is recommended and considered a standard security advice to grant least privilege\u2014that is, granting only the permissions required to perform a task. Determine what users need to do and then craft policies for them that let the users perform only those tasks, instead of allowing full administrative privileges. Using the GUI, perform the following to detach the policy that has full administrative privileges: 1. Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/. 2. In the navigation pane, click Policies and then search for the policy name found in the audit step. 3. Select the policy that needs to be deleted. 4. In the policy action menu, select first Detach 5. Select all Users, Groups, Roles that have this policy attached 6. Click Detach Policy 7. In the policy action menu, select Detach Using the CLI, perform the following to detach the policy that has full administrative privileges as found in the audit step: 1. Lists all IAM users, groups, and roles that the specified managed policy is attached to. aws iam list-entities-for-policy --policy-arn 2. Detach the policy from all IAM Users: aws iam detach-user-policy --user-name --policy-arn 3. Detach the policy from all IAM Groups: aws iam detach-group-policy --group-name --policy-arn 4. Detach the policy from all IAM Roles: aws iam detach-role-policy --role-name --policy-arn Do not setup access keys during initial user setup for all IAM users that have a console password AWS console defaults the checkbox for creating access keys to enabled. This results in many access keys being generated unnecessarily. In addition to unnecessary credentials, it also generates unnecessary management work in auditing and rotating these keys. Perform the following to delete access keys that do not pass the audit: 1. Login to the AWS Management Console: 2. Click Services 3. Click IAM 4. Click on Users 5. Click on Security Credentials 6. As an Administrator - Click on Delete for keys that were created at the same time as the user profile but have not been used. 7. As an IAM User - Click on Delete for keys that were created at the same time as the user profile but have not been used. Via CLI aws iam delete-access-key Ensure multi-factor authentication (MFA) is enabled for all IAM users that have a console password Multi-Factor Authentication (MFA) adds an extra layer of protection on top of a user name and password. With MFA enabled, when a user signs in to an AWS website, they will be prompted for their user name and password as well as for an authentication code from their AWS MFA device. It is recommended that MFA be enabled for all accounts that have a console password. Perform the following to enable MFA: 1. Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/. 2. In the navigation pane, choose Users. 3. In the User Name list, choose the name of the intended MFA user. 4. Choose the Security Credentials tab, and then choose Manage MFA Device. 5. In the Manage MFA Device wizard, choose A virtual MFA device, and then choose Next Step. Ensure MFA is enabled for the \"root\" account The root account is the most privileged user in an AWS account. MFA adds an extra layer of protection on top of a user name and password. With MFA enabled, when a user signs in to an AWS website, they will be prompted for their user name and password as well as for an authentication code from their AWS MFA device. Perform the following to establish MFA for the root account: 1. Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/. Note: to manage MFA devices for the root AWS account, you must use your root account credentials to sign in to AWS. You cannot manage MFA devices for the root account using other credentials. 2. Choose Dashboard , and under Security Status , expand Activate MFA on your root account. 3. Choose Activate MFA 4. In the wizard, choose A virtual MFA device and then choose Next Step . 5. IAM generates and displays configuration information for the virtual MFA device, including a QR code graphic. The graphic is a representation of the 'secret configuration key' that is available for manual entry on devices that do not support QR codes. 6. Open your virtual MFA application. (For a list of apps that you can use for hosting virtual MFA devices, see Virtual MFA Applications.) If the virtual MFA application supports multiple accounts (multiple virtual MFA devices), choose the option to create a new account (a new virtual MFA device). 7. Determine whether the MFA app supports QR codes, and then do one of the following: \uf0b7 Use the app to scan the QR code. For example, you might choose the camera icon or choose an option similar to Scan code, and then use the device's camera to scan the code. \uf0b7 In the Manage MFA Device wizard, choose Show secret key for manual configuration, and then type the secret configuration key into your MFA application. When you are finished, the virtual MFA device starts generating one-time passwords. 1. In the Manage MFA Device wizard, in the Authentication Code 1 box, type the onetime password that currently appears in the virtual MFA device. Wait up to 30 seconds for the device to generate a new one-time password. Then type the second one-time password into the Authentication Code 2 box. Choose Active Virtual MFA. Ensure a log metric filter and alarm exist for VPC changes Real-time monitoring of API calls can be achieved by directing CloudTrail Logs to CloudWatch Logs and establishing corresponding metric filters and alarms. It is possible to have more than 1 VPC within an account, in addition it is also possible to create a peer connection between 2 VPCs enabling network traffic to route between VPCs. It is recommended that a metric filter and alarm be established for changes made to VPCs. Perform the following to setup the metric filter, alarm, SNS topic, and subscription: 1. Create a metric filter based on filter pattern provided which checks for VPC changes and the taken from audit step 1. aws logs put-metric-filter --log-group-name -- filter-name <vpc_changes_metric> --metric-transformations metricName= <vpc_changes_metric> ,metricNamespace='CISBenchmark',metricValue=1 -- filter-pattern '{ ($.eventName = CreateVpc) || ($.eventName = DeleteVpc) || ($.eventName = ModifyVpcAttribute) || ($.eventName = AcceptVpcPeeringConnection) || ($.eventName = CreateVpcPeeringConnection) || ($.eventName = DeleteVpcPeeringConnection) || ($.eventName = RejectVpcPeeringConnection) || ($.eventName = AttachClassicLinkVpc) || ($.eventName = DetachClassicLinkVpc) || ($.eventName = DisableVpcClassicLink) || ($.eventName = EnableVpcClassicLink) }' Note: You can choose your own metricName and metricNamespace strings. Using the same metricNamespace for all Foundations Benchmark metrics will group them together. 2. Create an SNS topic that the alarm will notify aws sns create-topic --name Note: you can execute this command once and then re-use the same topic for all monitoring alarms. 3. Create an SNS subscription to the topic created in step 2 aws sns subscribe --topic-arn --protocol - -notification-endpoint Note: you can execute this command once and then re-use the SNS subscription for all monitoring alarms. 4. Create an alarm that is associated with the CloudWatch Logs Metric Filter created in step 1 and an SNS topic created in step 2 aws cloudwatch put-metric-alarm --alarm-name <vpc_changes_alarm> -- metric-name <vpc_changes_metric> --statistic Sum --period 300 --threshold 1 --comparison-operator GreaterThanOrEqualToThreshold --evaluation-periods 1 --namespace 'CISBenchmark' --alarm-actions Ensure a log metric filter and alarm exist for unauthorized API calls Real-time monitoring of API calls can be achieved by directing CloudTrail Logs to CloudWatch Logs and establishing corresponding metric filters and alarms. It is recommended that a metric filter and alarm be established for unauthorized API calls. Perform the following to setup the metric filter, alarm, SNS topic, and subscription: 1. Create a metric filter based on filter pattern provided which checks for unauthorized API calls and the taken from audit step 1. aws logs put-metric-filter --log-group-name -- filter-name <unauthorized_api_calls_metric> --metric-transformations metricName= <unauthorized_api_calls_metric> ,metricNamespace='CISBenchmark',metricValue=1 --filter-pattern '{ ($.errorCode = \" UnauthorizedOperation\") || ($.errorCode = \"AccessDenied \") }' Note: You can choose your own metricName and metricNamespace strings. Using the same metricNamespace for all Foundations Benchmark metrics will group them together. 2. Create an SNS topic that the alarm will notify aws sns create-topic --name Note: you can execute this command once and then re-use the same topic for all monitoring alarms. 3. Create an SNS subscription to the topic created in step 2 aws sns subscribe --topic-arn --protocol - -notification-endpoint Note: you can execute this command once and then re-use the SNS subscription for all monitoring alarms. 4. Create an alarm that is associated with the CloudWatch Logs Metric Filter created in step 1 and an SNS topic created in step 2 aws cloudwatch put-metric-alarm --alarm-name <unauthorized_api_calls_alarm> --metric-name <unauthorized_api_calls_metric> --statistic Sum --period 300 --threshold 1 --comparison-operator GreaterThanOrEqualToThreshold --evaluation-periods 1 -- namespace 'CISBenchmark' --alarm-actions Ensure a log metric filter and alarm exist for security group changes Real-time monitoring of API calls can be achieved by directing CloudTrail Logs to CloudWatch Logs and establishing corresponding metric filters and alarms. Security Groups are a stateful packet filter that controls ingress and egress traffic within a VPC. It is recommended that a metric filter and alarm be established changes made to Security Groups. Perform the following to setup the metric filter, alarm, SNS topic, and subscription: 1. Create a metric filter based on filter pattern provided which checks for security groups changes and the taken from audit step 1. aws logs put-metric-filter --log-group-name -- filter-name <security_group_changes_metric> --metric-transformations metricName= <security_group_changes_metric> ,metricNamespace='CISBenchmark',metricValue=1 --filter-pattern '{ ($.eventName = AuthorizeSecurityGroupIngress) || ($.eventName = AuthorizeSecurityGroupEgress) || ($.eventName = RevokeSecurityGroupIngress) || ($.eventName = RevokeSecurityGroupEgress) || ($.eventName = CreateSecurityGroup) || ($.eventName = DeleteSecurityGroup) }' Note: You can choose your own metricName and metricNamespace strings. Using the same metricNamespace for all Foundations Benchmark metrics will group them together. 2. Create an SNS topic that the alarm will notify aws sns create-topic --name Note: you can execute this command once and then re-use the same topic for all monitoring alarms. 3. Create an SNS subscription to the topic created in step 2 aws sns subscribe --topic-arn --protocol - -notification-endpoint Note: you can execute this command once and then re-use the SNS subscription for all monitoring alarms. 4. Create an alarm that is associated with the CloudWatch Logs Metric Filter created in step 1 and an SNS topic created in step 2 aws cloudwatch put-metric-alarm --alarm-name <security_group_changes_alarm> --metric-name <security_group_changes_metric> --statistic Sum --period 300 --threshold 1 --comparison-operator GreaterThanOrEqualToThreshold --evaluation-periods 1 -- namespace 'CISBenchmark' --alarm-actions Ensure a log metric filter and alarm exist for S3 bucket policy changes Real-time monitoring of API calls can be achieved by directing CloudTrail Logs to CloudWatch Logs and establishing corresponding metric filters and alarms. It is recommended that a metric filter and alarm be established for changes to S3 bucket Perform the following to setup the metric filter, alarm, SNS topic, and subscription: 1. Create a metric filter based on filter pattern provided which checks for S3 bucket policy changes and the taken from audit step 1. aws logs put-metric-filter --log-group-name -- filter-name <s3_bucket_policy_changes_metric> --metric-transformations metricName= <s3_bucket_policy_changes_metric> ,metricNamespace='CISBenchmark',metricValue=1 --filter-pattern '{ ($.eventSource = s3.amazonaws.com) && (($.eventName = PutBucketAcl) || ($.eventName = PutBucketPolicy) || ($.eventName = PutBucketCors) || ($.eventName = PutBucketLifecycle) || ($.eventName = PutBucketReplication) || ($.eventName = DeleteBucketPolicy) || ($.eventName = DeleteBucketCors) || ($.eventName = DeleteBucketLifecycle) || ($.eventName = DeleteBucketReplication)) }' Note: You can choose your own metricName and metricNamespace strings. Using the same metricNamespace for all Foundations Benchmark metrics will group them together. 2. Create an SNS topic that the alarm will notify aws sns create-topic --name Note: you can execute this command once and then re-use the same topic for all monitoring alarms. 3. Create an SNS subscription to the topic created in step 2 aws sns subscribe --topic-arn --protocol - -notification-endpoint Note: you can execute this command once and then re-use the SNS subscription for all monitoring alarms. 4. Create an alarm that is associated with the CloudWatch Logs Metric Filter created in step 1 and an SNS topic created in step 2 aws cloudwatch put-metric-alarm --alarm-name <s3_bucket_policy_changes_alarm> --metric-name <s3_bucket_policy_changes_metric> --statistic Sum --period 300 --threshold 1 --comparison-operator GreaterThanOrEqualToThreshold --evaluation-periods 1 --namespace 'CISBenchmark' --alarm-actions Ensure a log metric filter and alarm exist for route table changes Real-time monitoring of API calls can be achieved by directing CloudTrail Logs to CloudWatch Logs and establishing corresponding metric filters and alarms. Routing tables are used to route network traffic between subnets and to network gateways. It is recommended that a metric filter and alarm be established for changes to route tables. Perform the following to setup the metric filter, alarm, SNS topic, and subscription: 1. Create a metric filter based on filter pattern provided which checks for route table changes and the taken from audit step 1. aws logs put-metric-filter --log-group-name -- filter-name <route_table_changes_metric> --metric-transformations metricName= <route_table_changes_metric> ,metricNamespace='CISBenchmark',metricValue=1 --filter-pattern '{ ($.eventName = CreateRoute) || ($.eventName = CreateRouteTable) || ($.eventName = ReplaceRoute) || ($.eventName = ReplaceRouteTableAssociation) || ($.eventName = DeleteRouteTable) || ($.eventName = DeleteRoute) || ($.eventName = DisassociateRouteTable) }' Note: You can choose your own metricName and metricNamespace strings. Using the same metricNamespace for all Foundations Benchmark metrics will group them together. 2. Create an SNS topic that the alarm will notify aws sns create-topic --name Note: you can execute this command once and then re-use the same topic for all monitoring alarms. 3. Create an SNS subscription to the topic created in step 2 aws sns subscribe --topic-arn --protocol - -notification-endpoint Note: you can execute this command once and then re-use the SNS subscription for all monitoring alarms. 4. Create an alarm that is associated with the CloudWatch Logs Metric Filter created in step 1 and an SNS topic created in step 2 aws cloudwatch put-metric-alarm --alarm-name <route_table_changes_alarm> --metric-name <route_table_changes_metric> --statistic Sum --period 300 - -threshold 1 --comparison-operator GreaterThanOrEqualToThreshold -- evaluation-periods 1 --namespace 'CISBenchmark' --alarm-actions Ensure a log metric filter and alarm exist for usage of \"root\" account Real-time monitoring of API calls can be achieved by directing CloudTrail Logs to CloudWatch Logs and establishing corresponding metric filters and alarms. It is recommended that a metric filter and alarm be established for root login attempts. Perform the following to setup the metric filter, alarm, SNS topic, and subscription: 1. Create a metric filter based on filter pattern provided which checks for \"Root\" account usage and the taken from audit step 1. aws logs put-metric-filter --log-group-name <cloudtrail_log_group_name> -- filter-name <root_usage_metric> --metric-transformations metricName= <root_usage_metric> ,metricNamespace='CISBenchmark',metricValue=1 --filterpattern '{ $.userIdentity.type = \"Root\" && $.userIdentity.invokedBy NOT EXISTS && $.eventType != \"AwsServiceEvent\" }' Note: You can choose your own metricName and metricNamespace strings. Using the same metricNamespace for all Foundations Benchmark metrics will group them together. 2. Create an SNS topic that the alarm will notify aws sns create-topic --name Note: you can execute this command once and then re-use the same topic for all monitoring alarms. 3. Create an SNS subscription to the topic created in step 2 aws sns subscribe --topic-arn --protocol - -notification-endpoint Note: you can execute this command once and then re-use the SNS subscription for all monitoring alarms. 4. Create an alarm that is associated with the CloudWatch Logs Metric Filter created in step 1 and an SNS topic created in step 2 aws cloudwatch put-metric-alarm --alarm-name <root_usage_alarm> --metricname <root_usage_metric> --statistic Sum --period 300 --threshold 1 -- comparison-operator GreaterThanOrEqualToThreshold --evaluation-periods 1 -- namespace 'CISBenchmark' --alarm-actions Ensure a log metric filter and alarm exist for changes to Network Access Control Lists (NACL) Real-time monitoring of API calls can be achieved by directing CloudTrail Logs to CloudWatch Logs and establishing corresponding metric filters and alarms. NACLs are used as a stateless packet filter to control ingress and egress traffic for subnets within a VPC. It is recommended that a metric filter and alarm be established for changes made to NACLs. Perform the following to setup the metric filter, alarm, SNS topic, and subscription: 1. Create a metric filter based on filter pattern provided which checks for NACL changes and the taken from audit step 1. aws logs put-metric-filter --log-group-name -- filter-name <nacl_changes_metric> --metric-transformations metricName= <nacl_changes_metric> ,metricNamespace='CISBenchmark',metricValue=1 -- filter-pattern '{ ($.eventName = CreateNetworkAcl) || ($.eventName = CreateNetworkAclEntry) || ($.eventName = DeleteNetworkAcl) || ($.eventName = DeleteNetworkAclEntry) || ($.eventName = ReplaceNetworkAclEntry) || ($.eventName = ReplaceNetworkAclAssociation) }' Note: You can choose your own metricName and metricNamespace strings. Using the same metricNamespace for all Foundations Benchmark metrics will group them together. 2. Create an SNS topic that the alarm will notify aws sns create-topic --name Note: you can execute this command once and then re-use the same topic for all monitoring alarms. 3. Create an SNS subscription to the topic created in step 2 aws sns subscribe --topic-arn --protocol - -notification-endpoint Note: you can execute this command once and then re-use the SNS subscription for all monitoring alarms. 4. Create an alarm that is associated with the CloudWatch Logs Metric Filter created in step 1 and an SNS topic created in step 2 aws cloudwatch put-metric-alarm --alarm-name <nacl_changes_alarm> -- metric-name <nacl_changes_metric> --statistic Sum --period 300 -- threshold 1 --comparison-operator GreaterThanOrEqualToThreshold --evaluationperiods 1 --namespace 'CISBenchmark' --alarm-actions Ensure a log metric filter and alarm exist for IAM policy changes Real-time monitoring of API calls can be achieved by directing CloudTrail Logs to CloudWatch Logs and establishing corresponding metric filters and alarms. It is recommended that a metric filter and alarm be established changes made to Identity and Access Management (IAM) policies. Perform the following to setup the metric filter, alarm, SNS topic, and subscription: 1. Create a metric filter based on filter pattern provided which checks for IAM policy changes and the taken from audit step 1. aws logs put-metric-filter --log-group-name <cloudtrail_log_group_name> -- filter-name <iam_changes_metric> --metric-transformations metricName= <iam_changes_metric> ,metricNamespace='CISBenchmark',metricValue=1 -- filter-pattern '{($.eventName=DeleteGroupPolicy)||($.eventName=DeleteRolePolicy)||($.eventNa me=DeleteUserPolicy)||($.eventName=PutGroupPolicy)||($.eventName=PutRolePolic y)||($.eventName=PutUserPolicy)||($.eventName=CreatePolicy)||($.eventName=Del etePolicy)||($.eventName=CreatePolicyVersion)||($.eventName=DeletePolicyVersi on)||($.eventName=AttachRolePolicy)||($.eventName=DetachRolePolicy)||($.event Name=AttachUserPolicy)||($.eventName=DetachUserPolicy)||($.eventName=AttachGr oupPolicy)||($.eventName=DetachGroupPolicy)}' Note: You can choose your own metricName and metricNamespace strings. Using the same metricNamespace for all Foundations Benchmark metrics will group them together. 2. Create an SNS topic that the alarm will notify aws sns create-topic --name Note: you can execute this command once and then re-use the same topic for all monitoring alarms. 3. Create an SNS subscription to the topic created in step 2 aws sns subscribe --topic-arn --protocol - -notification-endpoint Note: you can execute this command once and then re-use the SNS subscription for all monitoring alarms. 4. Create an alarm that is associated with the CloudWatch Logs Metric Filter created in step 1 and an SNS topic created in step 2 aws cloudwatch put-metric-alarm --alarm-name <iam_changes_alarm> -- metric-name <iam_changes_metric> --statistic Sum --period 300 --threshold 1 --comparison-operator GreaterThanOrEqualToThreshold --evaluation-periods 1 --namespace 'CISBenchmark' --alarm-actions Ensure a log metric filter and alarm exist for changes to network gateways Real-time monitoring of API calls can be achieved by directing CloudTrail Logs to CloudWatch Logs and establishing corresponding metric filters and alarms. Network gateways are required to send/receive traffic to a destination outside of a VPC. It is recommended that a metric filter and alarm be established for changes to network Perform the following to setup the metric filter, alarm, SNS topic, and subscription: 1. Create a metric filter based on filter pattern provided which checks for network gateways changes and the taken from audit step 1. aws logs put-metric-filter --log-group-name -- filter-name <network_gw_changes_metric> --metric-transformations metricName= <network_gw_changes_metric> ,metricNamespace='CISBenchmark',metricValue=1 --filter-pattern '{ ($.eventName = CreateCustomerGateway) || ($.eventName = DeleteCustomerGateway) || ($.eventName = AttachInternetGateway) || ($.eventName = CreateInternetGateway) || ($.eventName = DeleteInternetGateway) || ($.eventName = DetachInternetGateway) }' Note: You can choose your own metricName and metricNamespace strings. Using the same metricNamespace for all Foundations Benchmark metrics will group them together. 2. Create an SNS topic that the alarm will notify aws sns create-topic --name Note: you can execute this command once and then re-use the same topic for all monitoring alarms. 3. Create an SNS subscription to the topic created in step 2 aws sns subscribe --topic-arn --protocol - -notification-endpoint Note: you can execute this command once and then re-use the SNS subscription for all monitoring alarms. 4. Create an alarm that is associated with the CloudWatch Logs Metric Filter created in step 1 and an SNS topic created in step 2 aws cloudwatch put-metric-alarm --alarm-name <network_gw_changes_alarm> - -metric-name <network_gw_changes_metric> --statistic Sum --period 300 -- threshold 1 --comparison-operator GreaterThanOrEqualToThreshold --evaluationperiods 1 --namespace 'CISBenchmark' --alarm-actions Ensure a log metric filter and alarm exist for disabling or scheduled deletion of customer created CMKs Real-time monitoring of API calls can be achieved by directing CloudTrail Logs to CloudWatch Logs and establishing corresponding metric filters and alarms. It is recommended that a metric filter and alarm be established for customer created CMKs which have changed state to disabled or scheduled deletion. Perform the following to setup the metric filter, alarm, SNS topic, and subscription: 1. Create a metric filter based on filter pattern provided which checks for disabled or scheduled for deletion CMK's and the taken from audit step 1. aws logs put-metric-filter --log-group-name -- filter-name <disable_or_delete_cmk_changes_metric> --metrictransformations metricName= <disable_or_delete_cmk_changes_metric> ,metricNamespace='CISBenchmark',metricValue=1 --filter-pattern '{($.eventSource = kms.amazonaws.com) && (($.eventName=DisableKey)||($.eventName=ScheduleKeyDeletion)) }' Note: You can choose your own metricName and metricNamespace strings. Using the same metricNamespace for all Foundations Benchmark metrics will group them together. 2. Create an SNS topic that the alarm will notify aws sns create-topic --name Note: you can execute this command once and then re-use the same topic for all monitoring alarms. 3. Create an SNS subscription to the topic created in step 2 aws sns subscribe --topic-arn --protocol - -notification-endpoint Note: you can execute this command once and then re-use the SNS subscription for all monitoring alarms. 4. Create an alarm that is associated with the CloudWatch Logs Metric Filter created in step 1 and an SNS topic created in step 2 aws cloudwatch put-metric-alarm --alarm-name <disable_or_delete_cmk_changes_alarm> --metric-name <disable_or_delete_cmk_changes_metric> --statistic Sum --period 300 -- threshold 1 --comparison-operator GreaterThanOrEqualToThreshold --evaluationperiods 1 --namespace 'CISBenchmark' --alarm-actions Ensure a log metric filter and alarm exist for Management Console sign-in without MFA Real-time monitoring of API calls can be achieved by directing CloudTrail Logs to CloudWatch Logs and establishing corresponding metric filters and alarms. It is recommended that a metric filter and alarm be established for console logins that are not protected by multi-factor authentication (MFA). Perform the following to setup the metric filter, alarm, SNS topic, and subscription: 1. Create a metric filter based on filter pattern provided which checks for AWS Management Console sign-in without MFA and the taken from audit step 1. aws logs put-metric-filter --log-group-name -- filter-name <no_mfa_console_signin_metric> --metric-transformations metricName= <no_mfa_console_signin_metric> ,metricNamespace='CISBenchmark',metricValue=1 --filter-pattern '{ ($.eventName = \"ConsoleLogin\") && ($.additionalEventData.MFAUsed != \"Yes\") }' Note: You can choose your own metricName and metricNamespace strings. Using the same metricNamespace for all Foundations Benchmark metrics will group them together. 2. Create an SNS topic that the alarm will notify aws sns create-topic --name Note: you can execute this command once and then re-use the same topic for all monitoring alarms. 3. Create an SNS subscription to the topic created in step 2 aws sns subscribe --topic-arn --protocol - -notification-endpoint Note: you can execute this command once and then re-use the SNS subscription for all monitoring alarms. 4. Create an alarm that is associated with the CloudWatch Logs Metric Filter created in step 1 and an SNS topic created in step 2 aws cloudwatch put-metric-alarm --alarm-name <no_mfa_console_signin_alarm> --metric-name <no_mfa_console_signin_metric> --statistic Sum --period 300 --threshold 1 --comparison-operator GreaterThanOrEqualToThreshold -- evaluation-periods 1 --namespace 'CISBenchmark' --alarm-actions Ensure a log metric filter and alarm exist for CloudTrail configuration changes Real-time monitoring of API calls can be achieved by directing CloudTrail Logs to CloudWatch Logs and establishing corresponding metric filters and alarms. It is recommended that a metric filter and alarm be established for detecting changes to CloudTrail's configurations. Perform the following to setup the metric filter, alarm, SNS topic, and subscription: 1. Create a metric filter based on filter pattern provided which checks for cloudtrail configuration changes and the taken from audit step 1. aws logs put-metric-filter --log-group-name -- filter-name <cloudtrail_cfg_changes_metric> --metric-transformations metricName= <cloudtrail_cfg_changes_metric> ,metricNamespace='CISBenchmark',metricValue=1 --filter-pattern '{ ($.eventName = CreateTrail) || ($.eventName = UpdateTrail) || ($.eventName = DeleteTrail) || ($.eventName = StartLogging) || ($.eventName = StopLogging) }' Note: You can choose your own metricName and metricNamespace strings. Using the same metricNamespace for all Foundations Benchmark metrics will group them together. 2. Create an SNS topic that the alarm will notify aws sns create-topic --name Note: you can execute this command once and then re-use the same topic for all monitoring alarms. 3. Create an SNS subscription to the topic created in step 2 aws sns subscribe --topic-arn --protocol - -notification-endpoint Note: you can execute this command once and then re-use the SNS subscription for all monitoring alarms. 4. Create an alarm that is associated with the CloudWatch Logs Metric Filter created in step 1 and an SNS topic created in step 2 aws cloudwatch put-metric-alarm --alarm-name <cloudtrail_cfg_changes_alarm> --metric-name <cloudtrail_cfg_changes_metric> --statistic Sum --period 300 --threshold 1 --comparison-operator GreaterThanOrEqualToThreshold --evaluation-periods 1 -- namespace 'CISBenchmark' --alarm-actions Ensure a log metric filter and alarm exist for AWS Management Console authentication failures Real-time monitoring of API calls can be achieved by directing CloudTrail Logs to CloudWatch Logs and establishing corresponding metric filters and alarms. It is recommended that a metric filter and alarm be established for failed console authentication attempts. Perform the following to setup the metric filter, alarm, SNS topic, and subscription: 1. Create a metric filter based on filter pattern provided which checks for AWS management Console Login Failures and the taken from audit step 1. aws logs put-metric-filter --log-group-name -- filter-name <console_signin_failure_metric> --metric-transformations metricName= <console_signin_failure_metric> ,metricNamespace='CISBenchmark',metricValue=1 --filter-pattern '{ ($.eventName = ConsoleLogin) && ($.errorMessage = \"Failed authentication\") }' Note: You can choose your own metricName and metricNamespace strings. Using the same metricNamespace for all Foundations Benchmark metrics will group them together. 2. Create an SNS topic that the alarm will notify aws sns create-topic --name Note: you can execute this command once and then re-use the same topic for all monitoring alarms. 3. Create an SNS subscription to the topic created in step 2 aws sns subscribe --topic-arn --protocol - -notification-endpoint Note: you can execute this command once and then re-use the SNS subscription for all monitoring alarms. 4. Create an alarm that is associated with the CloudWatch Logs Metric Filter created in step 1 and an SNS topic created in step 2 aws cloudwatch put-metric-alarm --alarm-name <console_signin_failure_alarm> --metric-name <console_signin_failure_metric> --statistic Sum --period 300 --threshold 1 --comparison-operator GreaterThanOrEqualToThreshold --evaluation-periods 1 -- namespace 'CISBenchmark' --alarm-actions <sns_topic_arn Ensure a log metric filter and alarm exist for AWS Config configuration changes Real-time monitoring of API calls can be achieved by directing CloudTrail Logs to CloudWatch Logs and establishing corresponding metric filters and alarms. It is recommended that a metric filter and alarm be established for detecting changes to CloudTrail's configurations. Perform the following to setup the metric filter, alarm, SNS topic, and subscription: 1. Create a metric filter based on filter pattern provided which checks for AWS Configuration changes and the taken from audit step 1. aws logs put-metric-filter --log-group-name -- filter-name <aws_config_changes_metric> --metric-transformations metricName= <aws_config_changes_metric> ,metricNamespace='CISBenchmark',metricValue=1 --filter-pattern '{ ($.eventSource = config.amazonaws.com) && (($.eventName=StopConfigurationRecorder)||($.eventName=DeleteDeliveryChannel) ||($.eventName=PutDeliveryChannel)||($.eventName=PutConfigurationRecorder)) }' Note: You can choose your own metricName and metricNamespace strings. Using the same metricNamespace for all Foundations Benchmark metrics will group them together. 2. Create an SNS topic that the alarm will notify aws sns create-topic --name Note: you can execute this command once and then re-use the same topic for all monitoring alarms. 3. Create an SNS subscription to topic created in step 2 aws sns subscribe --topic-arn --protocol - -notification-endpoint Note: you can execute this command once and then re-use the SNS subscription for all monitoring alarms. 4. Create an alarm that is associated with the CloudWatch Logs Metric Filter created in step 1 and an SNS topic created in step 2 aws cloudwatch put-metric-alarm --alarm-name <aws_config_changes_alarm> - -metric-name <aws_config_changes_metric> --statistic Sum --period 300 -- threshold 1 --comparison-operator GreaterThanOrEqualToThreshold --evaluationperiods 1 --namespace 'CISBenchmark' --alarm-actions remediationDocURLs: Maintain current contact details Ensure contact email and telephone details for AWS accounts are current and map to more than one individual in your organization. An AWS account supports a number of contact details, and AWS will use these to contact the account owner if activity judged to be in breach of Acceptable Use Policy or indicative of likely security compromise is observed by the AWS Abuse team. Contact details should not be for a single individual, as circumstances may arise where that individual is unavailable. Email contact details should point to a mail alias which forwards email to multiple individuals within the organization; where feasible, phone contact details should point to a PBX hunt group or other call-forwarding system. This activity can only be performed via the AWS Console, with a user who has permission to read and write Billing information (aws-portal:*Billing ). - Sign in to the AWS Management Console and open the Billing and Cost Management console at https://console.aws.amazon.com/billing/home#/. - On the navigation bar, choose your account name, and then choose My Account. - On the Account Settings page, next to Account Settings, choose Edit. - Next to the field that you need to update, choose Edit. - After you have entered your changes, choose Save changes. - After you have made your changes, choose Done. - To edit your contact information, under Contact Information, choose Edit. - For the fields that you want to change, type your updated information, and then choose Update. Ensure there no stale roles with Attached Policies for S3 access Avoid stale roles which could cause access leakage and uncontrolled manipulation with S3 bucket data which increase risks and leads to Ransomware violations. This rule checks inline policies only. The Attached policies verified under the respective rule. Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/. In the navigation pane of the IAM console, choose Roles. Then select the check box next to the role name that you want to delete, not the name or row itself. For Role actions at the top of the page, choose Delete. In the confirmation dialog box, review the last accessed information, which shows when each of the selected roles last accessed an AWS service. This helps you to confirm whether the role is currently active. If you want to proceed, choose Yes, Delete to submit the service-linked role for deletion. Watch the IAM console notifications to monitor the progress of the service-linked role deletion. Because the IAM service-linked role deletion is asynchronous, after you submit the role for deletion, the deletion task can succeed or fail. If the task succeeds, then the role is removed from the list and a notification of success appears at the top of the page. If the task fails, you can choose View details or View Resources from the notifications to learn why the deletion failed. If the deletion fails because the role is using the service's resources, then the notification includes a list of resources, if the service returns that information. You can then clean up the resources and submit the deletion again. https://aws.amazon.com/blogs/security/identify-unused-iam-roles-remove-confidently-last-used-timestamp/ https://docs.aws.amazon.com/IAM/latest/UserGuide/using-service-linked-roles.html#delete-service-linked-role Avoid stale roles which could cause access leakage and uncontrolled manipulation with S3 bucket data which increase risks and leads to Ransomware violations. This rule checks Attached policies only. The Inline policies verified under the respective rule. Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/. In the navigation pane of the IAM console, choose Roles. Then select the check box next to the role name that you want to delete, not the name or row itself. For Role actions at the top of the page, choose Delete. In the confirmation dialog box, review the last accessed information, which shows when each of the selected roles last accessed an AWS service. This helps you to confirm whether the role is currently active. If you want to proceed, choose Yes, Delete to submit the service-linked role for deletion. Watch the IAM console notifications to monitor the progress of the service-linked role deletion. Because the IAM service-linked role deletion is asynchronous, after you submit the role for deletion, the deletion task can succeed or fail. If the task succeeds, then the role is removed from the list and a notification of success appears at the top of the page. If the task fails, you can choose View details or View Resources from the notifications to learn why the deletion failed. If the deletion fails because the role is using the service's resources, then the notification includes a list of resources, if the service returns that information. You can then clean up the resources and submit the deletion again. Ensure IAM instance roles are used for AWS resource access from instances AWS access from within AWS instances can be done by either encoding AWS keys into AWS API calls or by assigning the instance to a role which has an appropriate permissions policy for the required access. \"AWS Access\" means accessing the APIs of AWS in order to access AWS resources or manage AWS account resources. IAM roles can only be associated at the launch of an instance. To remediate an instance to add it to a role you must create a new instance. If the instance has no external dependencies on its current private ip or public addresses are elastic IPs: 1. In AWS IAM create a new role. Assign a permissions policy if needed permissions are already known. 2. In the AWS console launch a new instance with identical settings to the existing instance, and ensure that the newly created role is selected. 3. Shutdown both the existing instance and the new instance. 4. Detach disks from both instances. 5. Attach the existing instance disks to the new instance. 6. Boot the new instance and you should have the same machine, but with the associated role. Note: if your environment has dependencies on a dynamically assigned PRIVATE IP address you can create an AMI from the existing instance, destroy the old one and then when launching from the AMI, manually assign the previous private IP address. Note: if your environment has dependencies on a dynamically assigned PUBLIC IP address there is not a way ensure the address is retained and assign an instance role. Dependencies on dynamically assigned public IP addresses are a bad practice and, if possible, you may wish to rebuild the instance with a new elastic IP address and make the investment to remediate affected systems while assigning the system to a role. Ensure IAM policies are attached only to groups or roles By default, IAM users, groups, and roles have no access to AWS resources. IAM policies are the means by which privileges are granted to users, groups, or roles. It is recommended that IAM policies be applied directly to groups and roles but not users. Perform the following to create an IAM group and assign a policy to it 1. Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/. 2. In the navigation pane, click Groups and then click Create New Group . 3. In the Group Name box, type the name of the group and then click Next Step . 4. In the list of policies, select the check box for each policy that you want to apply to all members of the group. Then click Next Step . 5. Click Create Group Perform the following to add a user to a given group 1. Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/. 2. In the navigation pane, click Groups 3. Select the group to add a user to 4. Click Add Users To Group 5. Select the users to be added to the group 6. Click Add Users Perform the following to remove a direct association between a user and policy 1. Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/. 2. In the left navigation pane, click on Users 3. For each user 1. Select the user 2. Click on the Permissions tab 3. Expand Managed Policies 4. Click Detach Policy for each policy 5. Expand Inline Policies 6. Click Remove Policy for each policy Ensure IAM password policy requires at least one uppercase letter Password policies are, in part, used to enforce password complexity requirements. IAM password policies can be used to ensure password are comprised of different character sets. It is recommended that the password policy require at least one uppercase letter Perform the following to set the password policy as prescribed: Via AWS Console 1. Login to AWS Console (with appropriate permissions to View Identity Access Management Account Settings) 2. Go to IAM Service on the AWS Console 3. Click on Account Settings on the Left Pane 4. Check \"Requires at least one uppercase letter\" 5. Click \"Apply password policy\" Via CLI aws iam update-account-password-policy --require-uppercase-characters Note: All commands starting with \"aws iam update-account-password-policy\" have be combined into a single command in order to all of them to take effect Ensure IAM password policy requires minimum length of 14 or greater Password policies are, in part, used to enforce password complexity requirements. IAM password policies can be used to ensure password are at least a given length. It is recommended that the password policy require a minimum password length 14 Perform the following to set the password policy as prescribed: Via AWS Console 1. Login to AWS Console (with appropriate permissions to View Identity Access Management Account Settings) 2. Go to IAM Service on the AWS Console 3. Click on Account Settings on the Left Pane 4. Set \"Minimum password length\" to 14 or greater. 5. Click \"Apply password policy\" Via CLI aws iam update-account-password-policy --minimum-password-length 14 Note: All commands starting with \"aws iam update-account-password-policy\" have be combined into a single command in order to all of them to take effect Ensure IAM password policy requires at least one lowercase letter Password policies are, in part, used to enforce password complexity requirements. IAM password policies can be used to ensure password are comprised of different character sets. It is recommended that the password policy require at least one lowercase letter Perform the following to set the password policy as prescribed: Via the AWS Console 1. Login to AWS Console (with appropriate permissions to View Identity Access Management Account Settings) 2. Go to IAM Service on the AWS Console 3. Click on Account Settings on the Left Pane 4. Check \"Requires at least one lowercase letter\" 5. Click \"Apply password policy\" Via CLI aws iam update-account-password-policy --require-lowercase-characters Note: All commands starting with \"aws iam update-account-password-policy\" have be combined into a single command in order to all of them to take effect Ensure IAM password policy require at least one symbol Password policies are, in part, used to enforce password complexity requirements. IAM password policies can be used to ensure password are comprised of different character sets. It is recommended that the password policy require at least one symbol Perform the following to set the password policy as prescribed: Via AWS Console 1. Login to AWS Console (with appropriate permissions to View Identity Access Management Account Settings) 2. Go to IAM Service on the AWS Console 3. Click on Account Settings on the Left Pane 4. Check \"Require at least one non-alphanumeric character\" 5. Click \"Apply password policy\" Via CLI aws iam update-account-password-policy --require-symbols Note: All commands starting with \"aws iam update-account-password-policy\" have be combined into a single command in order to all of them to take effect Ensure IAM password policy require at least one number Password policies are, in part, used to enforce password complexity requirements. IAM password policies can be used to ensure password are comprised of different character sets. It is recommended that the password policy require at least one number Perform the following to set the password policy as prescribed: Via AWS Console 1. Login to AWS Console (with appropriate permissions to View Identity Access Management Account Settings) 2. Go to IAM Service on the AWS Console 3. Click on Account Settings on the Left Pane 4. Check \"Require at least one number\" 5. Click \"Apply password policy\" Via CLI aws iam update-account-password-policy --require-numbers Note: All commands starting with \"aws iam update-account-password-policy\" have be combined into a single command in order to all of them to take effect Ensure IAM password policy prevents password reuse IAM password policies can prevent the reuse of a given password by the same user. It is recommended that the password policy prevent the reuse of passwords Perform the following to set the password policy as prescribed Via AWS Console 1. Login to AWS Console (with appropriate permissions to View Identity Access Management Account Settings) 2. Go to IAM Service on the AWS Console 3. Click on Account Settings on the Left Pane 4. Check \"Prevent password reuse\" 5. Set \"Number of passwords to remember\" is set to 24 Via CLI aws iam update-account-password-policy --password-reuse-prevention 24 Note: All commands starting with \"aws iam update-account-password-policy\" have be combined into a single command in order to all of them to take effect Ensure IAM password policy expires passwords within 90 days or less IAM password policies can require passwords to be rotated or expired after a given number of days. It is recommended that the password policy expire passwords after 90 days or less Perform the following to set the password policy as prescribed: Via AWS Console: 1. Login to AWS Console (with appropriate permissions to View Identity Access Management Account Settings) 2. Go to IAM Service on the AWS Console 3. Click on Account Settings on the Left Pane 4. Check \"Enable password expiration\" 5. Set \"Password expiration period (in days):\" to 90 or less Via CLI aws iam update-account-password-policy --max-password-age 90 Note: All commands starting with \"aws iam update-account-password-policy\" have be combined into a single command in order to all of them to take effec Ensure hardware MFA is enabled for the \"root\" account The root account is the most privileged user in an AWS account. MFA adds an extra layer of protection on top of a user name and password. With MFA enabled, when a user signs in to an AWS website, they will be prompted for their user name and password as well as for an authentication code from their AWS MFA device. For Level 2, it is recommended that the root account be protected with a hardware MFA. Perform the following to establish a hardware MFA for the root account: 1. Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/. Note: to manage MFA devices for the root AWS account, you must use your root account credentials to sign in to AWS. You cannot manage MFA devices for the root account using other credentials. 2. Choose Dashboard , and under Security Status , expand Activate MFA on your root account. 3. Choose Activate MFA 4. In the wizard, choose A hardware MFA device and then choose Next Step . 5. In the Serial Number box, enter the serial number that is found on the back of the MFA device. 6. In the Authentication Code 1 box, enter the six-digit number displayed by the MFA device. You might need to press the button on the front of the device to display the number. 7. Wait 30 seconds while the device refreshes the code, and then enter the next six-digit number into the Authentication Code 2 box. You might need to press the button on the front of the device again to display the second number. 8. Choose Next Step . The MFA device is now associated with the AWS account. The next time you use your AWS account credentials to sign in, you must type a code from the hardware MFA device. Ensure that GuardDuty enabled for required region By enabling GuardDuty you will get threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts, workloads, and data stored in Amazon S3 by event logs analysis. We are excluding verification for aws-global region from possible regions for GuardDuty Perform the following steps under the below link to enable GuardDuty: https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_settingup.html Ensure VPC flow logging is enabled in all VPCs VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. After you've created a flow log, you can view and retrieve its data in Amazon CloudWatch Logs. It is recommended that VPC Flow Logs be enabled for packet \"Rejects\" for VPCs. Perform the following to determine if VPC Flow logs is enabled: Via the Management Console: 1. Sign into the management console 2. Select Services then VPC 3. In the left navigation pane, select Your VPCs 4. Select a VPC 5. In the right pane, select the Flow Logs tab. 6. If no Flow Log exists, click Create Flow Log 7. For Filter, select Reject 8. Enter in a Role and Destination Log Group 9. Click Create Log Flow 10. Click on CloudWatch Logs Group Note: Setting the filter to \"Reject\" will dramatically reduce the logging data accumulation for this recommendation and provide sufficient information for the purposes of breach detection, research and remediation. However, during periods of least privilege security group engineering, setting this the filter to \"All\" can be very helpful in discovering existing traffic flows required for proper operation of an already running environment. Impact: By default, CloudWatch Logs will store Logs indefinitely unless a specific retention period is defined for the log group. When choosing the number of days to retain, keep in mind the average days it takes an organization to realize they have been breached is 210 days (at the time of this writing). Since additional time is required to research a breach, a minimum 365 day retention policy allows time for detection and research. You may also wish to archive the logs to a cheaper storage service rather than simply deleting them. See the following AWS resource to manage CloudWatch Logs retention periods: 1. http://docs.aws.amazon.com/AmazonCloudWatch/latest/DeveloperGuide/SettingLogRetention.html Ensure all EC2 EBS Volumes has snapshots Availability of the snapshot could prevent data lose and will simplify restoring in case of data encryption Current rule check existence of Snapshots for EBS Volumes of running EC2 instances Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. Choose Snapshots under Elastic Block Store in the navigation pane. Choose Create Snapshot. For Select resource type, choose Volume. For Volume, select the volume. (Optional) Enter a description for the snapshot. (Optional) Choose Add Tag to add tags to your snapshot. For each tag, provide a tag key and a tag value. Choose Create Snapshot. Ensure credentials unused for 90 days or greater are disabled AWS IAM users can access AWS resources using different types of credentials, such as passwords or access keys. It is recommended that all credentials that have been unused in 90 or greater days be removed or deactivated. Perform the following to remove or deactivate credentials: 1. Login to the AWS Management Console: 2. Click Services 3. Click IAM 4. Click on Users 5. Click on Security Credentials 6. As an Administrator - Click on Make Inactive for credentials that have not been used in 90 Days 7. As an IAM User - Click on Make Inactive or Delete for credentials which have not been used in 9 Ensure the default security group of every VPC restricts all traffic A VPC comes with a default security group whose initial settings deny all inbound traffic, allow all outbound traffic, and allow all traffic between instances assigned to the security group. If you don't specify a security group when you launch an instance, the instance is automatically assigned to this default security group. Security groups provide stateful filtering of ingress/egress network traffic to AWS resources. It is recommended that the default security group restrict all traffic. The default VPC in every region should have its default security group updated to comply. Any newly created VPCs will automatically contain a default security group that will need remediation to comply with this recommendation. Security Group Members Perform the following to implement the prescribed state: 1. Identify AWS resources that exist within the default security group 2. Create a set of least privilege security groups for those resources 3. Place the resources in those security groups 4. Remove the resources noted in #1 from the default security group Security Group State 1. Login to the AWS Management Console at https://console.aws.amazon.com/vpc/home 2. Repeat the next steps for all VPCs - including the default VPC in each AWS region: 3. In the left pane, click Security Groups 4. For each default security group, perform the following: 5. Select the default security group 6. Click the Inbound Rules tab 7. Remove any inbound rules 8. Click the Outbound Rules tab 9. Remove any inbound rules Recommended: IAM groups allow you to edit the \"name\" field. After remediating default groups rules for all VPCs in all regions, edit this field to add text similar to \"DO NOT USE. DO NOT ADD RULES\" Ensure AWS Config is enabled in all regions AWS Config is a web service that performs configuration management of supported AWS resources within your account and delivers log files to you. The recorded information includes the configuration item (AWS resource), relationships between configuration items (AWS resources), any configuration changes between resources. It is recommended to enable AWS Config be enabled in all region To implement AWS Config configuration: Via AWS Management Console: 1. Select the region you want to focus on in the top right of the console 2. Click Services 3. Click Config 4. Define which resources you want to record in the selected region 5. Choose to include global resources (IAM resources) 6. Specify an S3 bucket in the same account or in another managed AWS account 7. Create an SNS Topic from the same AWS account or another managed AWS account Via AWS Command Line Interface: 1. Ensure there is an appropriate S3 bucket, SNS topic, and IAM role per the AWS Config Service prerequisites. 2. Run this command to set up the configuration recorder aws configservice subscribe --s3-bucket my-config-bucket --sns-topic arn:aws:sns:us-east-1:012345678912:my-config-notice --iam-role arn:aws:iam::012345678912:role/myConfigRole 3. Run this command to start the configuration recorder: start-configuration-recorder --configuration-recorder-name <value Avoid the use of the \"root\" account The \"root\" account has unrestricted access to all resources in the AWS account. It is highly recommended that the use of this account be avoided. Ensure CloudTrail logs are encrypted at rest using KMS CMKs AWS CloudTrail is a web service that records AWS API calls for an account and makes those logs available to users and resources in accordance with IAM policies. AWS Key Management Service (KMS) is a managed service that helps create and control the encryption keys used to encrypt account data, and uses Hardware Security Modules (HSMs) to protect the security of encryption keys. CloudTrail logs can be configured to leverage server side encryption (SSE) and KMS customer created master keys (CMK) to further protect CloudTrail logs. It is recommended that CloudTrail be configured to use SSE-KMS Perform the following to configure CloudTrail to use SSE-KMS Via the Management Console 1. Sign in to the AWS Management Console and open the CloudTrail console at https://console.aws.amazon.com/cloudtrail 2. In the left navigation pane, choose Trails . 3. Click on a Trail 4. Under the S3 section click on the edit button (pencil icon) 5. Click Advanced 6. Select an existing CMK from the KMS key Id drop-down menu Note - Ensure the CMK is located in the same region as the S3 bucket Note - You will need to apply a KMS Key policy on the selected CMK in order for CloudTrail as a service to encrypt and decrypt log files using the CMK provided. Steps are provided here for editing the selected CMK Key policy 7. Click Save 8. You will see a notification message stating that you need to have decrypt permissions on the specified KMS key to decrypt log files. 9. Click Yes Via CLI aws cloudtrail update-trail --name --kms-id aws kms put-key-policy --key-id --policy Ensure CloudTrail is enabled in all regions Perform the following to enable global (Multi-region) CloudTrail logging: Via the management Console 1. Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/cloudtrail 2. Click on Trails on the left navigation pane 3. Click Get Started Now , if presented \uf0b7 Click Add new trail \uf0b7 Enter a trail name in the Trail name box \uf0b7 Set the Apply trail to all regions option to Yes \uf0b7 Specify an S3 bucket name in the S3 bucket box \uf0b7 Click Create 4. If 1 or more trails already exist, select the target trail to enable for global logging 5. Click the edit icon (pencil) next to Apply trail to all regions , Click Yes and Click Save. 6. Click the edit icon (pencil) next to Management Events click All for setting Read/Write Events and Click Save. Via CLI aws cloudtrail create-trail --name --bucket-name --is-multi-region-trail aws cloudtrail update-trail --name --is-multi-region-trail Note: Creating CloudTrail via CLI without providing any overriding options configures Management Events to set All type of Read/Writes by default. Ensure CloudTrail log file validation is enabled CloudTrail log file validation creates a digitally signed digest file containing a hash of each log that CloudTrail writes to S3. These digest files can be used to determine whether a log file was changed, deleted, or unchanged after CloudTrail delivered the log. It is recommended that file validation be enabled on all CloudTrails Perform the following to enable log file validation on a given trail: Via the management Console 1. Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/cloudtrail 2. Click on Trails on the left navigation pane 3. Click on target trail 4. Within the S3 section click on the edit icon (pencil) 5. Click Advanced 6. Click on the Yes radio button in section Enable log file validation 7. Click Save Via CLI aws cloudtrail update-trail --name --enable-log-file-validation Note that periodic validation of logs using these digests can be performed by running the following command: aws cloudtrail validate-logs --trail-arn --start-time --end-time Ensure security questions are registered in the AWS account The AWS support portal allows account owners to establish security questions that can be used to authenticate individuals calling AWS customer service for support. It is recommended that security questions be established. Perform the following in the AWS Management Console: - Login to the AWS Account as root - Click on the from the top right of the console - From the drop-down menu Click My Account - Scroll down to the Configure Security Questions section - Click on Edit - Click on each Question - From the drop-down select an appropriate question - Click on the Answer section - Enter an appropriate answer - Follow process for all 3 questions - Click Update when complete - Place Questions and Answers and place in a secure physical location","title":"Aws security"},{"location":"Configuration%20Guides/aws-security/#remediationdocurls","text":"Maintain current contact details Ensure contact email and telephone details for AWS accounts are current and map to more than one individual in your organization. An AWS account supports a number of contact details, and AWS will use these to contact the account owner if activity judged to be in breach of Acceptable Use Policy or indicative of likely security compromise is observed by the AWS Abuse team. Contact details should not be for a single individual, as circumstances may arise where that individual is unavailable. Email contact details should point to a mail alias which forwards email to multiple individuals within the organization; where feasible, phone contact details should point to a PBX hunt group or other call-forwarding system. This activity can only be performed via the AWS Console, with a user who has permission to read and write Billing information (aws-portal:*Billing ). - Sign in to the AWS Management Console and open the Billing and Cost Management console at https://console.aws.amazon.com/billing/home#/. - On the navigation bar, choose your account name, and then choose My Account. - On the Account Settings page, next to Account Settings, choose Edit. - Next to the field that you need to update, choose Edit. - After you have entered your changes, choose Save changes. - After you have made your changes, choose Done. - To edit your contact information, under Contact Information, choose Edit. - For the fields that you want to change, type your updated information, and then choose Update. Ensure there no stale roles with Attached Policies for S3 access Avoid stale roles which could cause access leakage and uncontrolled manipulation with S3 bucket data which increase risks and leads to Ransomware violations. This rule checks inline policies only. The Attached policies verified under the respective rule. Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/. In the navigation pane of the IAM console, choose Roles. Then select the check box next to the role name that you want to delete, not the name or row itself. For Role actions at the top of the page, choose Delete. In the confirmation dialog box, review the last accessed information, which shows when each of the selected roles last accessed an AWS service. This helps you to confirm whether the role is currently active. If you want to proceed, choose Yes, Delete to submit the service-linked role for deletion. Watch the IAM console notifications to monitor the progress of the service-linked role deletion. Because the IAM service-linked role deletion is asynchronous, after you submit the role for deletion, the deletion task can succeed or fail. If the task succeeds, then the role is removed from the list and a notification of success appears at the top of the page. If the task fails, you can choose View details or View Resources from the notifications to learn why the deletion failed. If the deletion fails because the role is using the service's resources, then the notification includes a list of resources, if the service returns that information. You can then clean up the resources and submit the deletion again. https://aws.amazon.com/blogs/security/identify-unused-iam-roles-remove-confidently-last-used-timestamp/ https://docs.aws.amazon.com/IAM/latest/UserGuide/using-service-linked-roles.html#delete-service-linked-role Avoid stale roles which could cause access leakage and uncontrolled manipulation with S3 bucket data which increase risks and leads to Ransomware violations. This rule checks Attached policies only. The Inline policies verified under the respective rule. Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/. In the navigation pane of the IAM console, choose Roles. Then select the check box next to the role name that you want to delete, not the name or row itself. For Role actions at the top of the page, choose Delete. In the confirmation dialog box, review the last accessed information, which shows when each of the selected roles last accessed an AWS service. This helps you to confirm whether the role is currently active. If you want to proceed, choose Yes, Delete to submit the service-linked role for deletion. Watch the IAM console notifications to monitor the progress of the service-linked role deletion. Because the IAM service-linked role deletion is asynchronous, after you submit the role for deletion, the deletion task can succeed or fail. If the task succeeds, then the role is removed from the list and a notification of success appears at the top of the page. If the task fails, you can choose View details or View Resources from the notifications to learn why the deletion failed. If the deletion fails because the role is using the service's resources, then the notification includes a list of resources, if the service returns that information. You can then clean up the resources and submit the deletion again. Ensure IAM instance roles are used for AWS resource access from instances AWS access from within AWS instances can be done by either encoding AWS keys into AWS API calls or by assigning the instance to a role which has an appropriate permissions policy for the required access. \"AWS Access\" means accessing the APIs of AWS in order to access AWS resources or manage AWS account resources. IAM roles can only be associated at the launch of an instance. To remediate an instance to add it to a role you must create a new instance. If the instance has no external dependencies on its current private ip or public addresses are elastic IPs: 1. In AWS IAM create a new role. Assign a permissions policy if needed permissions are already known. 2. In the AWS console launch a new instance with identical settings to the existing instance, and ensure that the newly created role is selected. 3. Shutdown both the existing instance and the new instance. 4. Detach disks from both instances. 5. Attach the existing instance disks to the new instance. 6. Boot the new instance and you should have the same machine, but with the associated role. Note: if your environment has dependencies on a dynamically assigned PRIVATE IP address you can create an AMI from the existing instance, destroy the old one and then when launching from the AMI, manually assign the previous private IP address. Note: if your environment has dependencies on a dynamically assigned PUBLIC IP address there is not a way ensure the address is retained and assign an instance role. Dependencies on dynamically assigned public IP addresses are a bad practice and, if possible, you may wish to rebuild the instance with a new elastic IP address and make the investment to remediate affected systems while assigning the system to a role. Ensure IAM policies are attached only to groups or roles By default, IAM users, groups, and roles have no access to AWS resources. IAM policies are the means by which privileges are granted to users, groups, or roles. It is recommended that IAM policies be applied directly to groups and roles but not users. Perform the following to create an IAM group and assign a policy to it 1. Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/. 2. In the navigation pane, click Groups and then click Create New Group . 3. In the Group Name box, type the name of the group and then click Next Step . 4. In the list of policies, select the check box for each policy that you want to apply to all members of the group. Then click Next Step . 5. Click Create Group Perform the following to add a user to a given group 1. Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/. 2. In the navigation pane, click Groups 3. Select the group to add a user to 4. Click Add Users To Group 5. Select the users to be added to the group 6. Click Add Users Perform the following to remove a direct association between a user and policy 1. Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/. 2. In the left navigation pane, click on Users 3. For each user 1. Select the user 2. Click on the Permissions tab 3. Expand Managed Policies 4. Click Detach Policy for each policy 5. Expand Inline Policies 6. Click Remove Policy for each policy Ensure IAM password policy requires at least one uppercase letter Password policies are, in part, used to enforce password complexity requirements. IAM password policies can be used to ensure password are comprised of different character sets. It is recommended that the password policy require at least one uppercase letter Perform the following to set the password policy as prescribed: Via AWS Console 1. Login to AWS Console (with appropriate permissions to View Identity Access Management Account Settings) 2. Go to IAM Service on the AWS Console 3. Click on Account Settings on the Left Pane 4. Check \"Requires at least one uppercase letter\" 5. Click \"Apply password policy\" Via CLI aws iam update-account-password-policy --require-uppercase-characters Note: All commands starting with \"aws iam update-account-password-policy\" have be combined into a single command in order to all of them to take effect Ensure IAM password policy requires minimum length of 14 or greater Password policies are, in part, used to enforce password complexity requirements. IAM password policies can be used to ensure password are at least a given length. It is recommended that the password policy require a minimum password length 14 Perform the following to set the password policy as prescribed: Via AWS Console 1. Login to AWS Console (with appropriate permissions to View Identity Access Management Account Settings) 2. Go to IAM Service on the AWS Console 3. Click on Account Settings on the Left Pane 4. Set \"Minimum password length\" to 14 or greater. 5. Click \"Apply password policy\" Via CLI aws iam update-account-password-policy --minimum-password-length 14 Note: All commands starting with \"aws iam update-account-password-policy\" have be combined into a single command in order to all of them to take effect Ensure IAM password policy requires at least one lowercase letter Password policies are, in part, used to enforce password complexity requirements. IAM password policies can be used to ensure password are comprised of different character sets. It is recommended that the password policy require at least one lowercase letter Perform the following to set the password policy as prescribed: Via the AWS Console 1. Login to AWS Console (with appropriate permissions to View Identity Access Management Account Settings) 2. Go to IAM Service on the AWS Console 3. Click on Account Settings on the Left Pane 4. Check \"Requires at least one lowercase letter\" 5. Click \"Apply password policy\" Via CLI aws iam update-account-password-policy --require-lowercase-characters Note: All commands starting with \"aws iam update-account-password-policy\" have be combined into a single command in order to all of them to take effect Ensure IAM password policy require at least one symbol Password policies are, in part, used to enforce password complexity requirements. IAM password policies can be used to ensure password are comprised of different character sets. It is recommended that the password policy require at least one symbol Perform the following to set the password policy as prescribed: Via AWS Console 1. Login to AWS Console (with appropriate permissions to View Identity Access Management Account Settings) 2. Go to IAM Service on the AWS Console 3. Click on Account Settings on the Left Pane 4. Check \"Require at least one non-alphanumeric character\" 5. Click \"Apply password policy\" Via CLI aws iam update-account-password-policy --require-symbols Note: All commands starting with \"aws iam update-account-password-policy\" have be combined into a single command in order to all of them to take effect Ensure IAM password policy require at least one number Password policies are, in part, used to enforce password complexity requirements. IAM password policies can be used to ensure password are comprised of different character sets. It is recommended that the password policy require at least one number Perform the following to set the password policy as prescribed: Via AWS Console 1. Login to AWS Console (with appropriate permissions to View Identity Access Management Account Settings) 2. Go to IAM Service on the AWS Console 3. Click on Account Settings on the Left Pane 4. Check \"Require at least one number\" 5. Click \"Apply password policy\" Via CLI aws iam update-account-password-policy --require-numbers Note: All commands starting with \"aws iam update-account-password-policy\" have be combined into a single command in order to all of them to take effect Ensure IAM password policy prevents password reuse IAM password policies can prevent the reuse of a given password by the same user. It is recommended that the password policy prevent the reuse of passwords Perform the following to set the password policy as prescribed Via AWS Console 1. Login to AWS Console (with appropriate permissions to View Identity Access Management Account Settings) 2. Go to IAM Service on the AWS Console 3. Click on Account Settings on the Left Pane 4. Check \"Prevent password reuse\" 5. Set \"Number of passwords to remember\" is set to 24 Via CLI aws iam update-account-password-policy --password-reuse-prevention 24 Note: All commands starting with \"aws iam update-account-password-policy\" have be combined into a single command in order to all of them to take effect Ensure IAM password policy expires passwords within 90 days or less IAM password policies can require passwords to be rotated or expired after a given number of days. It is recommended that the password policy expire passwords after 90 days or less Perform the following to set the password policy as prescribed: Via AWS Console: 1. Login to AWS Console (with appropriate permissions to View Identity Access Management Account Settings) 2. Go to IAM Service on the AWS Console 3. Click on Account Settings on the Left Pane 4. Check \"Enable password expiration\" 5. Set \"Password expiration period (in days):\" to 90 or less Via CLI aws iam update-account-password-policy --max-password-age 90 Note: All commands starting with \"aws iam update-account-password-policy\" have be combined into a single command in order to all of them to take effec Ensure hardware MFA is enabled for the \"root\" account The root account is the most privileged user in an AWS account. MFA adds an extra layer of protection on top of a user name and password. With MFA enabled, when a user signs in to an AWS website, they will be prompted for their user name and password as well as for an authentication code from their AWS MFA device. For Level 2, it is recommended that the root account be protected with a hardware MFA. Perform the following to establish a hardware MFA for the root account: 1. Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/. Note: to manage MFA devices for the root AWS account, you must use your root account credentials to sign in to AWS. You cannot manage MFA devices for the root account using other credentials. 2. Choose Dashboard , and under Security Status , expand Activate MFA on your root account. 3. Choose Activate MFA 4. In the wizard, choose A hardware MFA device and then choose Next Step . 5. In the Serial Number box, enter the serial number that is found on the back of the MFA device. 6. In the Authentication Code 1 box, enter the six-digit number displayed by the MFA device. You might need to press the button on the front of the device to display the number. 7. Wait 30 seconds while the device refreshes the code, and then enter the next six-digit number into the Authentication Code 2 box. You might need to press the button on the front of the device again to display the second number. 8. Choose Next Step . The MFA device is now associated with the AWS account. The next time you use your AWS account credentials to sign in, you must type a code from the hardware MFA device. Ensure that GuardDuty enabled for required region By enabling GuardDuty you will get threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts, workloads, and data stored in Amazon S3 by event logs analysis. We are excluding verification for aws-global region from possible regions for GuardDuty Perform the following steps under the below link to enable GuardDuty: https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_settingup.html Ensure VPC flow logging is enabled in all VPCs VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. After you've created a flow log, you can view and retrieve its data in Amazon CloudWatch Logs. It is recommended that VPC Flow Logs be enabled for packet \"Rejects\" for VPCs. Perform the following to determine if VPC Flow logs is enabled: Via the Management Console: 1. Sign into the management console 2. Select Services then VPC 3. In the left navigation pane, select Your VPCs 4. Select a VPC 5. In the right pane, select the Flow Logs tab. 6. If no Flow Log exists, click Create Flow Log 7. For Filter, select Reject 8. Enter in a Role and Destination Log Group 9. Click Create Log Flow 10. Click on CloudWatch Logs Group Note: Setting the filter to \"Reject\" will dramatically reduce the logging data accumulation for this recommendation and provide sufficient information for the purposes of breach detection, research and remediation. However, during periods of least privilege security group engineering, setting this the filter to \"All\" can be very helpful in discovering existing traffic flows required for proper operation of an already running environment. Impact: By default, CloudWatch Logs will store Logs indefinitely unless a specific retention period is defined for the log group. When choosing the number of days to retain, keep in mind the average days it takes an organization to realize they have been breached is 210 days (at the time of this writing). Since additional time is required to research a breach, a minimum 365 day retention policy allows time for detection and research. You may also wish to archive the logs to a cheaper storage service rather than simply deleting them. See the following AWS resource to manage CloudWatch Logs retention periods: 1. http://docs.aws.amazon.com/AmazonCloudWatch/latest/DeveloperGuide/SettingLogRetention.html Ensure all EC2 EBS Volumes has snapshots Availability of the snapshot could prevent data lose and will simplify restoring in case of data encryption Current rule check existence of Snapshots for EBS Volumes of running EC2 instances Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. Choose Snapshots under Elastic Block Store in the navigation pane. Choose Create Snapshot. For Select resource type, choose Volume. For Volume, select the volume. (Optional) Enter a description for the snapshot. (Optional) Choose Add Tag to add tags to your snapshot. For each tag, provide a tag key and a tag value. Choose Create Snapshot. Ensure credentials unused for 90 days or greater are disabled AWS IAM users can access AWS resources using different types of credentials, such as passwords or access keys. It is recommended that all credentials that have been unused in 90 or greater days be removed or deactivated. Perform the following to remove or deactivate credentials: 1. Login to the AWS Management Console: 2. Click Services 3. Click IAM 4. Click on Users 5. Click on Security Credentials 6. As an Administrator - Click on Make Inactive for credentials that have not been used in 90 Days 7. As an IAM User - Click on Make Inactive or Delete for credentials which have not been used in 9 Ensure the default security group of every VPC restricts all traffic A VPC comes with a default security group whose initial settings deny all inbound traffic, allow all outbound traffic, and allow all traffic between instances assigned to the security group. If you don't specify a security group when you launch an instance, the instance is automatically assigned to this default security group. Security groups provide stateful filtering of ingress/egress network traffic to AWS resources. It is recommended that the default security group restrict all traffic. The default VPC in every region should have its default security group updated to comply. Any newly created VPCs will automatically contain a default security group that will need remediation to comply with this recommendation. Security Group Members Perform the following to implement the prescribed state: 1. Identify AWS resources that exist within the default security group 2. Create a set of least privilege security groups for those resources 3. Place the resources in those security groups 4. Remove the resources noted in #1 from the default security group Security Group State 1. Login to the AWS Management Console at https://console.aws.amazon.com/vpc/home 2. Repeat the next steps for all VPCs - including the default VPC in each AWS region: 3. In the left pane, click Security Groups 4. For each default security group, perform the following: 5. Select the default security group 6. Click the Inbound Rules tab 7. Remove any inbound rules 8. Click the Outbound Rules tab 9. Remove any inbound rules Recommended: IAM groups allow you to edit the \"name\" field. After remediating default groups rules for all VPCs in all regions, edit this field to add text similar to \"DO NOT USE. DO NOT ADD RULES\" Ensure AWS Config is enabled in all regions AWS Config is a web service that performs configuration management of supported AWS resources within your account and delivers log files to you. The recorded information includes the configuration item (AWS resource), relationships between configuration items (AWS resources), any configuration changes between resources. It is recommended to enable AWS Config be enabled in all region To implement AWS Config configuration: Via AWS Management Console: 1. Select the region you want to focus on in the top right of the console 2. Click Services 3. Click Config 4. Define which resources you want to record in the selected region 5. Choose to include global resources (IAM resources) 6. Specify an S3 bucket in the same account or in another managed AWS account 7. Create an SNS Topic from the same AWS account or another managed AWS account Via AWS Command Line Interface: 1. Ensure there is an appropriate S3 bucket, SNS topic, and IAM role per the AWS Config Service prerequisites. 2. Run this command to set up the configuration recorder aws configservice subscribe --s3-bucket my-config-bucket --sns-topic arn:aws:sns:us-east-1:012345678912:my-config-notice --iam-role arn:aws:iam::012345678912:role/myConfigRole 3. Run this command to start the configuration recorder: start-configuration-recorder --configuration-recorder-name <value Avoid the use of the \"root\" account The \"root\" account has unrestricted access to all resources in the AWS account. It is highly recommended that the use of this account be avoided. Ensure CloudTrail logs are encrypted at rest using KMS CMKs AWS CloudTrail is a web service that records AWS API calls for an account and makes those logs available to users and resources in accordance with IAM policies. AWS Key Management Service (KMS) is a managed service that helps create and control the encryption keys used to encrypt account data, and uses Hardware Security Modules (HSMs) to protect the security of encryption keys. CloudTrail logs can be configured to leverage server side encryption (SSE) and KMS customer created master keys (CMK) to further protect CloudTrail logs. It is recommended that CloudTrail be configured to use SSE-KMS Perform the following to configure CloudTrail to use SSE-KMS Via the Management Console 1. Sign in to the AWS Management Console and open the CloudTrail console at https://console.aws.amazon.com/cloudtrail 2. In the left navigation pane, choose Trails . 3. Click on a Trail 4. Under the S3 section click on the edit button (pencil icon) 5. Click Advanced 6. Select an existing CMK from the KMS key Id drop-down menu Note - Ensure the CMK is located in the same region as the S3 bucket Note - You will need to apply a KMS Key policy on the selected CMK in order for CloudTrail as a service to encrypt and decrypt log files using the CMK provided. Steps are provided here for editing the selected CMK Key policy 7. Click Save 8. You will see a notification message stating that you need to have decrypt permissions on the specified KMS key to decrypt log files. 9. Click Yes Via CLI aws cloudtrail update-trail --name --kms-id aws kms put-key-policy --key-id --policy Ensure CloudTrail is enabled in all regions Perform the following to enable global (Multi-region) CloudTrail logging: Via the management Console 1. Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/cloudtrail 2. Click on Trails on the left navigation pane 3. Click Get Started Now , if presented \uf0b7 Click Add new trail \uf0b7 Enter a trail name in the Trail name box \uf0b7 Set the Apply trail to all regions option to Yes \uf0b7 Specify an S3 bucket name in the S3 bucket box \uf0b7 Click Create 4. If 1 or more trails already exist, select the target trail to enable for global logging 5. Click the edit icon (pencil) next to Apply trail to all regions , Click Yes and Click Save. 6. Click the edit icon (pencil) next to Management Events click All for setting Read/Write Events and Click Save. Via CLI aws cloudtrail create-trail --name --bucket-name --is-multi-region-trail aws cloudtrail update-trail --name --is-multi-region-trail Note: Creating CloudTrail via CLI without providing any overriding options configures Management Events to set All type of Read/Writes by default. Ensure CloudTrail log file validation is enabled CloudTrail log file validation creates a digitally signed digest file containing a hash of each log that CloudTrail writes to S3. These digest files can be used to determine whether a log file was changed, deleted, or unchanged after CloudTrail delivered the log. It is recommended that file validation be enabled on all CloudTrails Perform the following to enable log file validation on a given trail: Via the management Console 1. Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/cloudtrail 2. Click on Trails on the left navigation pane 3. Click on target trail 4. Within the S3 section click on the edit icon (pencil) 5. Click Advanced 6. Click on the Yes radio button in section Enable log file validation 7. Click Save Via CLI aws cloudtrail update-trail --name --enable-log-file-validation Note that periodic validation of logs using these digests can be performed by running the following command: aws cloudtrail validate-logs --trail-arn --start-time --end-time Ensure security questions are registered in the AWS account The AWS support portal allows account owners to establish security questions that can be used to authenticate individuals calling AWS customer service for support. It is recommended that security questions be established. Perform the following in the AWS Management Console: - Login to the AWS Account as root - Click on the from the top right of the console - From the drop-down menu Click My Account - Scroll down to the Configure Security Questions section - Click on Edit - Click on each Question - From the drop-down select an appropriate question - Click on the Answer section - Enter an appropriate answer - Follow process for all 3 questions - Click Update when complete - Place Questions and Answers and place in a secure physical location","title":"remediationDocURLs:"},{"location":"Configuration%20Guides/aws/","text":"The documentation that follows provides guidance on how to remediate any security issues flagged by the Open Raven platform. You can find more information on rules and remediation via the AWS CIS Benchmark document. Most of the rules below include a number (i.e., \"1.1,\" \"2.2,\" etc.) for easy reference to this AWS CIS Benchmark document. Identity and Access Management Avoid the use of the \"root\" account (1.1) The \"root\" account has unrestricted access to all resources in the AWS account. It is recommended that the use of this account be avoided. Ensure multi-factor authentication (MFA) is enabled for all IAM users that have a console password (1.2) Multi-Factor Authentication (MFA) adds an extra layer of protection on top of a username and password. With MFA enabled, when a user signs in to an AWS website, they will not only be prompted to enter their username and password but also an authentication code from their AWS linked MFA device. It is recommended that MFA be enabled for all accounts that have a console password. Perform the following to enable MFA via the AWS Management Console: Step 1. Log in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/ . Step 2. In the navigation pane, click on Users . Step 3. In the \"User Name\" list, choose the name of the intended MFA user. Step 4. Select the \"Security Credentials\" tab, and then click on Manage MFA Device . Step 5. In the \"Manage MFA Device\" wizard, click on A virtual MFA device , and then click on Next Step . Ensure credentials unused for 90 days or greater are disabled (1.3) AWS IAM users can access AWS resources using different types of credentials, such as passwords or access keys. It is recommended that all credentials that have been unused for 90 or more days be removed or deactivated. Perform the following to remove or deactivate credentials via the AWS Management Console: Step 1. Log in to the AWS Management Console at https://console.aws.amazon.com/vpc/home . Step 2. Click on Services . Step 3. Click on IAM. Step 4. Click on Users. Step 5. Click on Security Credentials. Step 6. As an Administrator, click on Make Inactive for credentials that have not been used in 90 Days. As an IAM User, click on Make Inactive or Delete for credentials that have not been used in 90 days. Ensure access keys are rotated every 90 days or less (1.4) Access keys consist of an access key ID and a secret access key, which are used together to sign programmatic requests that you make to AWS. AWS users need their own access keys to make programmatic calls to AWS from the AWS Command Line Interface (AWS CLI), Tools for Windows PowerShell, the AWS SDKs, or direct HTTP calls using the APIs for individual AWS services. It is recommended that all access keys be regularly rotated. Perform the following to rotate access keys via the AWS Management Console: Step 1. Log in to the AWS Management Console at https://console.aws.amazon.com/vpc/home . Step 2. Click on Services. Step 3. Click on IAM. Step 4. Click on Users. Step 5. Click on Security Credentials. Step 6. As an Administrator, click on Make Inactive for keys that have not been rotated in 90 Days. As an IAM User, click on Make Inactive or Delete for keys that have not been rotated or used in 90 Days. Step 7. Click on Create Access Key. Step 9. Update programmatic call with new Access Key credentials. Perform the following to rotate access keys via CLI: Run the following command: aws iam update-access-key aws iam create-access-key aws iam delete-access-key Password complexity (1.5 to 1.11) Password policies are, in part, used to enforce password complexity requirements. IAM password policies can be used to ensure that passwords comprise different character sets. It is strongly recommended that your organization's password policy: Requires at least one uppercase letter (1.5) Requires at least one lowercase letter (1.6) Requires at least one symbol (1.7) Requires at least one number (1.8) Requires a minimum password length of 14 characters (1.9) Prevents password reuse (1.10) Expires after 90 days or less (1.11) Perform the following to set the password policy to follow the rules above via the AWS management console: Step 1. Log in to the AWS Console at https://console.aws.amazon.com/vpc/home (with appropriate permissions to View Identity Access Management Account Settings). Step 2. Go to \"IAM Service\" on the AWS Console. Step 3. Click on Account Settings on the left pane. Step 4. a) To set at least one uppercase letter Check \"Requires at least one uppercase letter\" and click on Apply Password Policy . b) To require at least one lowercase letter Check \"Requires at least one lowercase letter\" and click on Apply Password Policy . c) To require at least one symbol Check \"Require at least one non-alphanumeric character\" and click on Apply Password Policy . d) To require at least one number Check \"Require at least one number\" and click on Apply Password Policy . e) To set a minimum password length of 14 characters Set \"Minimum password length\" to 14 or greater and click on Apply Password Policy . f) To prevent password reuse Check \"Prevent password reuse\" and set \"Number of passwords to remember\" to 24. g) To expire passwords after 90 days Check \"Enable password expiration\" and set \"Password expiration period (in days)\" to 90 or less. Perform the following to set the password policy to follow the rules above via CLI: a) To set at least one uppercase letter Run the following command: aws iam update-account-password-policy --require-uppercase-characters b) To require at least one lowercase letter Run the following command: aws iam update-account-password-policy --require-lowercase-characters c) To require at least one symbol Run the following command: aws iam update-account-password-policy --require-symbols d) To require at least one number Run the following command: aws iam update-account-password-policy --require-numbers e) To set a minimum length of 14 characters Run the following command: aws iam update-account-password-policy --minimum-password-length 14 f) To prevent password reuse Run the following command: aws iam update-account-password-policy --password-reuse-prevention 24 g) To expire passwords after 90 days Run the following command: aws iam update-account-password-policy --max-password-age 90 Note: All commands starting with \"aws iam update-account-password-policy\" have to be combined into a single command in order for all of them to take effect. Ensure no root account access key exists (1.12) The root account is the most privileged user in an AWS account. AWS Access Keys provide programmatic access to a given AWS account. It is recommended that all access keys associated with the root account be removed. Perform the following to remove access keys associated with the root account via the AWS Management Console: Step 1. Log in to the AWS Management Console as Root and open the IAM console at https://console.aws.amazon.com/iam/ . Step 2. Click on \\<Root_Account_Name> at the top right and select Security Credentials from the drop-down list. Step 3. On the pop-out screen, click on Continue to Security Credentials. Step 4. Click on Access Keys (Access Key ID and Secret Access Key). Step 5. Under the \"Status\" column, if there are any Keys that are Active: Click on Make Inactive (i.e., temporarily disable Key if you think you may need it again). Click Delete (i.e., delete the key for good. Deleted keys cannot be recovered ). MFA for the root account (1.13 and 1.14) The root account is the most privileged user in an AWS account. MFA adds an extra layer of protection on top of a username and password used to access this account. With MFA enabled, when a user signs in to an AWS website, they will be prompted for their username and password as well as for an authentication code from their AWS MFA device. For Level 2, it is recommended that the root account be protected with a hardware MFA. Perform the following to establish virtual MFA/hardware MFA for the root account via the AWS Management Console: Step 1. Log in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/ . Note: To manage MFA devices for the root AWS account, you must use your root account credentials to sign in to AWS. You cannot manage MFA devices for the root account using other credentials. Step 2. Click on Dashboard , and under \"Security Status\" expand \"Activate MFA\" on your root account. Step 3. Click on Activate MFA. Step 4. a) Virtual MFA In the wizard, click on A virtual MFA device and then click on Next Step. b) Hardware MFA In the wizard, click on A hardware MFA device and then click on Next Step . Step 5. a) Virtual MFA IAM generates and displays configuration information for the virtual MFA device, including a QR code graphic. The graphic is a representation of the \"secret configuration key\" that is available for manual entry on devices that do not support QR codes. b) Hardware MFA In the \"Serial Number\" box, enter the serial number found on the back of the MFA device. Step 6. a) Virtual MFA Open your virtual MFA application. For a list of apps that you can use for hosting virtual MFA devices, see \"Virtual MFA Applications.\" If the virtual MFA application supports multiple accounts (multiple virtual MFA devices), choose the option to create a new account (a new virtual MFA device). b) Hardware MFA In the \"Authentication Code 1\" box, enter the six-digit number displayed by the MFA device. You might need to press the button on the front of the device to display the number. Wait 30 seconds while the device refreshes the code, and then enter the next six-digit number into the \"Authentication Code 2\" box. You might need to press the button on the front of the device again to display the second number. Step 7. a) Virtual MFA Determine whether the MFA app supports QR codes, and then do one of the following: Use the app to scan the QR code. For example, you might choose the camera icon or choose an option similar to Scan code. Then, use the device's camera to scan the code. In the \"Manage MFA Device\" wizard, click on Show secret key for manual configuration. Then, type the secret configuration key into your MFA application. When you are finished, the virtual MFA device starts generating one-time passwords. Now, a) Go to the \"Manage MFA Device\" wizard, b) In the \"Authentication Code 1\" box, type the one time password that currently appears in the virtual MFA device, c) Wait up to 30 seconds for the device to generate a new one-time password, d) Type the second one-time password into the \"Authentication Code 2\" box, d) Choose Active Virtual MFA . b) Hardware MFA Click on Next Step . The MFA device is now associated with the AWS account. The next time you use your AWS account credentials to sign in, you will need to type in a code from the hardware MFA device. Ensure security questions are registered in the AWS account (1.15) The AWS support portal allows account owners to establish security questions that can be used to authenticate individuals calling AWS customer service for support. It is recommended that security questions be established. Perform the following to register security questions in the AWS account via the AWS Management Console: Step 1. Log in to the AWS Management Console at https://console.aws.amazon.com/vpc/home as a root user. Step 2. Click on the \\<Root_Account_Name> from the top right of the console. Step 3. From the drop-down menu, click My Account. Step 4. Scroll down to the \"Configure Security Questions\" section and click on Edit. Step 5. Click on each Question From the drop-down menu, select an appropriate question. Click on the Answer section. Enter an appropriate answer. Follow this process for all three questions. Step 6. Click Update when complete. Step 7. Store the security questions and answers in a secure physical location. Ensure IAM policies are attached only to groups or roles (1.16) By default, IAM users, groups, and roles have no access to AWS resources. IAM policies allow certain privileges to be granted to users, groups, or roles. It is recommended that IAM policies be applied directly to groups and roles but not users. Perform the following to create an IAM group and assign a policy to it via the AWS Management Console: Step 1. Log in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/ . Step 2. In the navigation pane, click on Groups and then click on Create New Group . Step 3. In the \"Group Name\" box, type the name of the group and then click on Next Step. Step 4. In the list of policies, select the checkbox for each policy that you want to apply to all members of the group. Then click on Next Step . Step 5. Click on Create Group . Step 6. Perform the following to add a user to a given group: Log in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/ . In the navigation pane, click on Groups . Select the group you want to add a user to. Click on Add Users To Group. Select the users to be added to the group. Click on Add Users. Step 7. Perform the following to remove a direct association between a user and policy: Log in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/ . In the left navigation pane, click on Users. For each user, a) Select the user, b) Select the \"Permissions\" tab, c) Expand \"Managed Policies\" d) Click on Detach Policy for each policy, e) Expand \"Inline Policies,\" f) Click on Remove Policy for each policy. Maintain current contact details (1.17) Ensure email and telephone details for AWS accounts are current and map to more than one individual in your organization. AWS will use these details to contact the account owner if any activity is judged to be in breach of the Acceptable Use Policy or determined to be indicative of likely security compromise by the AWS Abuse team. Since an AWS account supports a number of contact details, it is strongly recommended that organizations maintain contact details for more than just a single individual. This is because circumstances may arise where that individual is unavailable. Email contact details should point to a mail alias that forwards emails to multiple individuals within the organization. Where feasible, phone contact details should point to a PBX hunt group or another call-forwarding system. This activity can only be performed via the AWS Console, with a user who has permission to read and write Billing information (aws-portal:*Billing ): Step 1. Log in to the AWS Management Console and open the Billing and Cost Management console at https://console.aws.amazon.com/billing/home#/ . Step 2. In the navigation bar, choose your account name, and then click on My Account . Step 3. On the \"Account Settings\" page, next to \"Account Settings\" click on Edit . Step 4. Next to the field that you need to update, click on Edit . Step 5. After you have entered your changes, click on Save changes . Step 6. After you have made your changes, click on Done . Step 7. To edit your contact information, under \"Contact Information\" click on Edit . Step 8. For the fields that you want to change, type your updated information, and then click on Update . Ensure security contact information is registered (1.18) AWS gives customers the option to specify contact information for the account's security team. It is recommended that this information be provided. Perform the following to establish security contact information via the AWS Management Console: Step 1. Log in to the AWS Management Console at https://console.aws.amazon.com/vpc/home . Step 2. Click on your account name at the top right corner of the console. Step 3. From the drop-down menu, click on My Account . Step 4. Scroll down to the \"Alternate Contacts\" section. Step 5. Enter the contact information in the \"Security\" section. Note: Consider specifying an internal email distribution list to ensure emails are regularly monitored by more than one individual. Ensure IAM instance roles are used for AWS resource access from instances (1.19) AWS access from within AWS instances can be done either by: Encoding AWS keys into AWS API calls, or Assigning the instance to a role that has an appropriate permissions policy for the required access. \"AWS Access\" means accessing the APIs of AWS in order to access AWS resources or manage AWS account resources. IAM roles can only be associated at the launch of an instance. To remediate an instance and add it to a role, you must create a new instance. If the instance has no external dependencies on its current private IP or if public addresses are elastic IPs, via the AWS Management Console: Step 1. In AWS IAM, create a new role. Assign a permissions policy if the needed permissions are already known. Step 2. In the AWS console, launch a new instance with identical settings to the existing instance, and ensure that the newly created role is selected. Step 3. Shut down both the existing instance and the new instance. Step 4. Detach disks from both instances. Step 5. Attach the existing instance disks to the new instance. Step 6. Boot the new instance and you should have the same machine, but with the associated role. Note 1: If your environment has dependencies on a dynamically assigned PRIVATE IP address, you can create an AMI from the existing instance and destroy the old one. Then, when launching from the AMI, manually assign the previous private IP address. Note 2: If your environment has dependencies on a dynamically assigned PUBLIC IP address, there is no way to ensure the address is retained and assign it an instance role. Dependencies on dynamically assigned public IP addresses are a bad practice. If possible, you may wish to rebuild the instance with a new elastic IP address and make the investment to remediate affected systems while assigning the system to a role. Ensure a support role has been created to manage incidents with AWS Support (1.20) AWS provides a support center that can be used for incident notification and response, as well as technical support and customer service. Create an IAM Role to allow authorized users to manage incidents with AWS Support. To create a support role to manage incidents with AWS support, use the Amazon unified command-line interface and perform the following: Step 1. Create a trust relationship policy document that allows \\<iam_user> to manage AWS incidents, and save it locally as /tmp/TrustPolicy.json: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::321859670049:root\" }, \"Action\": \"sts:AssumeRole\" } ] } Step 2. Create the IAM role using the above trust policy: aws iam create-role --role-name \\<aws_support_iam_role> --assume-rolepolicy-document file:///tmp/TrustPolicy.json Step 3. Attach 'AWSSupportAccess' managed policy to the created IAM role: aws iam attach-role-policy --policy-arn \\<iam_policy_arn> --role-name \\<aws_support_iam_role> Do not set up access keys during initial user setup for all IAM users that have a console password (1.21) AWS console marks the checkbox for creating access keys enabled by default. This results in many access keys being generated unnecessarily. In addition to unnecessary credentials, this also generates unnecessary management work when it comes to auditing and rotating these keys. Perform the following to delete access keys that do not pass the audit via the AWS Management Console: Step 1. Log in to the AWS Management Console at https://console.aws.amazon.com/vpc/home . Step 2. Click on Services . Step 3. Click on IAM. Step 4. Click on Users. Step 5. Click on Security Credentials. Step 6. As an Administrator, click on Delete for keys that were created at the same time as the user profile but have not been used. As an IAM User, click on Delete for keys that were created at the same time as the user profile but have not been used. Perform the following to delete access keys that do not pass the audit via CLI: Run the following command: iam delete-access-key Ensure IAM policies that allow full administrative privileges are not created (1.22) IAM policies are the means by which privileges are granted to users, groups, or roles. It is recommended and considered standard security advice to grant these based on the principle of least privilege. In other words, organizations should never grant users, groups, or roles permissions beyond the minimum required to perform a particular task. Determine what users need to do and then craft policies that let the users perform only those tasks instead of allowing full administrative privileges. Perform the following to detach the policy that grants full administrative privileges from users via the AWS Management Console: Step 1. Log in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/ . Step 2. In the navigation pane, click on Policies and then search for the policy name found in the audit step. Step 3. Select the policy that needs to be deleted. Step 4. In the policy action menu, click on Detach . Step 5. Select all Users, Groups, and Roles that have this policy attached. Step 6. Click on Detach Policy . Step 7. In the policy action menu, click on Detach. Perform the following to detach the policy that grants full administrative privileges from users via the CLI: Step 1. Lists all IAM users, groups, and roles that the specified managed policy is attached to. aws iam list-entities-for-policy --policy-arn \\<policy_arn> Step 2. Detach the policy from all IAM Users: aws iam detach-user-policy --user-name \\<iam_user> --policy-arn \\<policy_arn> Step 3. Detach the policy from all IAM Groups: aws iam detach-group-policy --group-name \\<iam_group> --policy-arn \\<policy_arn> Step 4. Detach the policy from all IAM Roles: aws iam detach-role-policy --role-name \\<iam_role> --policy-arn \\<policy_arn> Logging Ensure CloudTrail is enabled in all regions (2.1) Perform the following to enable global (Multi-region) CloudTrail logging via the AWS Management Console: Step 1. Log in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/cloudtrail . Step 2. Click on Trails on the left navigation pane. Step 3. Click on Get Started Now , if presented: Click on Add new trail. Enter a trail name in the \"Trail name\" box. Set the \"Apply trail to all regions\" option to \"Yes.\"\" Specify an S3 bucket name in the S3 bucket box. Click on Create . Step 4. If one or more trails already exist, select the target trail to enable global logging. Step 5. Select the edit icon (pencil) next to \"Apply trail to all regions,\" click on Yes and then click on Save . Step 6. Select the edit icon (pencil) next to \"Management Events,\" click on All for setting \"Read/Write Events,\" and then click on Save . Perform the following to enable global (Multi-region) CloudTrail logging via CLI: Run the following command: aws cloudtrail create-trail --name \\<trail_name> --bucket-name \\<s3_bucket_for_cloudtrail> --is-multi-region-trail aws cloudtrail update-trail --name \\<trail_name> --is-multi-region-trail Note: Creating CloudTrail via CLI without providing any overriding options configures Management Events to set All types of Read/Writes by default. Ensure CloudTrail log file validation is enabled (2.2) CloudTrail log file validation creates a digitally signed digest file containing a hash of each log that CloudTrail writes to S3. These digest files can be used to determine whether a log file was changed, deleted, or unchanged after CloudTrail delivered the log. It is recommended that file validation be enabled on all CloudTrails. Perform the following to enable log file validation on a given trail via the AWS Management Console: Step 1. Log in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/cloudtrail . Step 2. Click on Trails on the left navigation pane. Step 3. Click on the target trail. Step 4. Within the S3 section, click on the edit icon (pencil). Step 5. Click on Advanced Step 6. Click on the Yes radio button in the section \"Enable log file validation.\" Step 7. Click on Save. Perform the following to enable log file validation on a given trail via CLI: Run the following command: aws cloudtrail update-trail --name \\<trail_name> --enable-log-file-validation Note: Periodic validation of logs using these digests can be performed by running the following command: aws cloudtrail validate-logs --trail-arn \\<trail_arn> --start-time \\<start_time> --end-time \\<end_time> Ensure the S3 bucket used to store CloudTrail logs is not publicly accessible (2.3) CloudTrail logs a record of every API call made in your AWS account. These logs are stored in an S3 bucket. It is recommended that the bucket policy, or access control list (ACL), is applied to the S3 bucket that CloudTrail logs to prevent public access to the CloudTrail logs. Perform the following to remove any public access that has been granted to the bucket via an ACL or S3 bucket policy via the AWS Management Console: Step 1. Go to the Amazon S3 console at https://console.aws.amazon.com/s3/home . Step 2. Right-click on the bucket and click on Properties. Step 3. In the \"Properties\" pane, click on the Permissions tab. The tab shows a list of permissions granted, with one row per permission granted, in the bucket ACL. Each row identifies the grantee and the permissions granted. Step 4. Select the row that grants permission to \"Everyone\" or \"Any Authenticated User.\" Step 5. Uncheck all the permissions granted to \"Everyone\" or \"Any Authenticated User\" (click on x to delete the row). Step 6. Click Save to save the ACL. Step 7. If the \"Edit bucket policy\" button is present, click on it. Step 8. Remove any Statement that has an Effect set to \u201cAllow\u201d and a Principal set to \"\" or {\"AWS\" : \"\"}. Ensure CloudTrail trails are integrated with CloudWatch Logs (2.4) AWS CloudTrail is a web service that records AWS API calls made in a given AWS account. The information recorded here includes the: Identity of the API caller Time of the API call Source IP address of the API caller Request parameters Response elements returned by the AWS service. CloudTrail uses Amazon S3 for log file storage and delivery. In addition to capturing CloudTrail logs within a specified S3 bucket for long-term analysis, real-time analysis can be performed by configuring CloudTrail to send logs to CloudWatch Logs. For a trail that is enabled in all regions within an account, CloudTrail sends log files from all regions to a CloudWatch Logs log group. It is recommended that CloudTrail logs be sent to CloudWatch Logs Perform the following to integrate CloudTrail trails with CloudWatch Logs via the AWS Management Console: Step 1. Log in to the AWS Management Console and open the CloudTrail console at https://console.aws.amazon.com/cloudtrail/ . Step 2. Under \"All Buckets,\"\" click on the target bucket you want to evaluate. Step 3. Click on Properties on the top right of the console. Step 4. Click on Trails in the left menu. Step 5. Click on each trail where no CloudWatch Logs are defined. Step 6. Go to the \"CloudWatch Logs\" section and click on Configure. Step 7. Define a new or select an existing log group. Step 8. Click on Continue. Step 9. Configure the IAM Role, which will deliver CloudTrail events to CloudWatch Logs. Step 10. Create/Select an IAM Role and Policy Name. Step 11. Click on Allow to continue. Perform the following to integrate CloudTrail trails with CloudWatch Logs via CLI: Run the following command: aws cloudtrail update-trail --name \\<trail_name> --cloudwatch-logs-log-grouparn \\<cloudtrail_log_group_arn> --cloudwatch-logs-role-arn \\<cloudtrail_cloudwatchLogs_role_arn> Ensure AWS Config is enabled in all regions (2.5) AWS Config is a web service that performs configuration management of supported AWS resources within your account and delivers log files to you. The recorded information includes the: Configuration item (AWS resource). Relationships between configuration items (AWS resources). Any configuration changes between resources. It is recommended that AWS Config is enabled in all regions. Perform the following to implement AWS Config configuration via the AWS Management Console: Step 1. Select the region you want to focus on in the top right of the console. Step 2. Click on Services. Step 3. Click on Config Step 4. Define which resources you want to record in the selected region. Step 5. Choose to include global resources (IAM resources). Step 6. Specify an S3 bucket in the same account or another managed AWS account Step 7. Create an SNS Topic from the same AWS account or another managed AWS account Perform the following to implement AWS Config configuration via CLI: Step 1. Ensure there is an appropriate S3 bucket, SNS topic, and IAM role per the AWS Config Service prerequisites. Step 2. Run this command to set up the configuration recorder aws configservice subscribe --s3-bucket my-config-bucket --sns-topic arn:aws:sns:us-east-1:012345678912:my-config-notice --iam-role arn:aws:iam::012345678912:role/myConfigRole Step 3. Run this command to start the configuration recorder: start-configuration-recorder --configuration-recorder-name \\<value Ensure S3 bucket access logging is enabled on the CloudTrail S3 bucket (2.6) S3 Bucket Access Logging generates a log that contains access records for each request made to your S3 bucket. An access log record contains details about the request, such as the: Request type Resources specified in the request Time and date that the request was processed. It is recommended that bucket access logging be enabled on the CloudTrail S3 bucket. Perform the following to enable S3 bucket logging via the AWS Management Console: Step 1. Log in to the AWS Management Console and open the S3 console at https://console.aws.amazon.com/s3 . Step 2. Under \"All Buckets\" click on the target S3 bucket. Step 3. Click on Properties in the top right of the console. Step 4. Under Bucket: \\<s3_bucket_for_cloudtrail> click on Logging. Step 5. To configure bucket logging: Click on the Enabled checkbox. Select the Target Bucket from the list. Enter a Target Prefix. Click Save . Ensure CloudTrail logs are encrypted at rest using KMS CMKs (2.7) AWS CloudTrail is a web service that records AWS API calls for an account and makes those logs available to users and resources in accordance with IAM policies. AWS Key Management Service (KMS) is a managed service that helps create and control the encryption keys used to encrypt account data and uses Hardware Security Modules (HSMs) to protect the security of encryption keys. CloudTrail logs can be configured to leverage server-side encryption (SSE) and KMS customer-created master keys (CMK) to further protect CloudTrail logs. It is recommended that CloudTrail be configured to use SSE-KMS. Perform the following to configure CloudTrail to use SSE-KMS via the AWS Management Console: Step 1. Log in to the AWS Management Console and open the CloudTrail console at https://console.aws.amazon.com/cloudtrail . Step 2. In the left navigation pane, click on Trails. Step 3. Click on a Trail. Step 4. Under the S3 section, click on the edit button (pencil icon). Step 5. Click on Advanced . Step 6. Select an existing CMK from the KMS key Id drop-down menu. Note 1: Ensure the CMK is located in the same region as the S3 bucket. Note 2: You will need to apply a KMS Key policy on the selected CMK in order for CloudTrail as a service to encrypt and decrypt log files using the CMK provided. Steps are provided here for editing the selected CMK Key policy. Step 7. Click on Save . Step 8. You will see a notification message stating that you need to have decrypt permissions on the specified KMS key to decrypt log files. Step 9. Click on Yes. Perform the following to configure CloudTrail to use SSE-KMS via CLI Run the following command: aws cloudtrail update-trail --name \\<trail_name> --kms-id \\<cloudtrail_kms_key> aws kms put-key-policy --key-id \\<cloudtrail_kms_key> --policy \\<cloudtrail_kms_key_policy> Ensure rotation for customer created CMKs is enabled (2.8) The AWS Key Management Service (KMS) allows customers to rotate the backing key. The backing key is key material stored within the KMS and is tied to the key ID of the Customer Created Customer Master Key (CMK). It is the backing key that is used to perform cryptographic operations such as encryption and decryption. Automated key rotation currently retains all prior backing keys so that decryption of encrypted data can take place transparently. It is recommended that CMK key rotation be enabled. Perform the following to enable customer-created CMK rotation via the AWS Management Console: Step 1. Log in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam . Step 2. In the left navigation pane, click on Encryption Keys . Step 3. Select a customer-created master key (CMK). Step 4. Under the \"Key Policy\" section, move down to \"Key Rotation.\"\" Step 5. Check the \"Rotate this key every year\" checkbox. Perform the following to enable customer-created CMK rotation via CLI: Run the following command: aws kms enable-key-rotation --key-id \\<kms_key_id> Ensure VPC flow logging is enabled in all VPCs (2.9) VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. After you've created a flow log, you can view and retrieve its data in Amazon CloudWatch Logs. It is recommended that VPC Flow Logs be enabled for packet \"Rejects\" for VPCs. Perform the following to determine if the VPC Flow logs feature is enabled via the AWS Management Console: Step 1. Log in to the AWS Management Console at https://console.aws.amazon.com/vpc/home . Step 2. Click on Services and then click on VPC . Step 3. In the left navigation pane, click on Your VPCs. Step 4. Select a VPC. Step 5. In the right pane, click on the \"Flow Logs\" tab. Step 6. If no Flow Log exists, click on Create Flow Log. Step 7. For \"Filter,\"\" select Reject. Step 8. Enter in a \"Role\" and \"Destination Log Group.\"\" Step 9. Click on Create Log Flow. Step 10. Click on CloudWatch Logs Group Note: Setting the filter to \"Reject\" will dramatically reduce the accumulation of logging data while providing sufficient information for the purposes of breach detection, research, and remediation. However, during periods of least privilege security group engineering, setting this filter to \"All\" can be very helpful in discovering the existing traffic flows required for the proper operation of an already running environment. By default, CloudWatch Logs will store logs indefinitely unless a specific retention period is defined for the log group. When choosing the number of days to retain logs, keep in mind that, on average, it takes a typical organization 210 days (at the time of this writing) to realize they have been breached. Since additional time is required to research a breach, a minimum 365-day retention policy allows time for detection and research. You may also wish to archive the logs to a cheaper storage service rather than simply deleting them. See the following AWS resource to manage CloudWatch Logs retention periods: http://docs.aws.amazon.com/AmazonCloudWatch/latest/DeveloperGuide/SettingLogRetention.html Monitoring Real-time monitoring of API calls can be achieved by directing CloudTrail Logs to CloudWatch Logs and establishing corresponding metric filters and alarms. It is strongly recommended that a metric filter and alarm be established for detecting: Unauthorized API calls (3.1) Console logins that are not protected by multi-factor authentication (MFA) (3.2) Root login attempts (3.3) Changes made to Identity and Access Management (IAM) policies (3.4) Changes to CloudTrail's configurations (3.5) Failed console authentication attempts (3.6) Customer-created CMKs which have changed state to disabled or scheduled deletion (3.7) Changes to S3 buckets (3.8) Changes to AWS Config configuration (3.9) Changes made to Security Groups ( Note: Security Groups are a stateful packet filter that controls ingress and egress traffic within a VPC.) (3.10) Changes made to NACLs (3.11) Changes to the network (3.12) Changes to route tables (3.13) Changes made to VPCs ( Note: It is possible to have more than 1 VPC within an account. In addition, it is also possible to create a peer connection between 2 VPCs, which enables network traffic to route between VPCs) (3.14) Perform the following to set up the metric filter, alarm, SNS topic, and subscription via the AWS Management Console: Step 1. a) Detect unauthorized API calls Create a metric filter based on the filter pattern provided which checks for unauthorized API calls and the \\<cloudtrail_log_group_name> taken from audit step 1. aws logs put-metric-filter --log-group-name \\<cloudtrail_log_group_name> -- filter-name \\<unauthorized_api_calls_metric> --metric-transformations metricName= \\<unauthorized_api_calls_metric> ,metricNamespace='CISBenchmark',metricValue=1 --filter-pattern '{ ($.errorCode = \"UnauthorizedOperation\") || ($.errorCode = \"AccessDenied\") }' b) Detect console logins that are not protected by multi-factor authentication (MFA) Create a metric filter based on the filter pattern provided which checks for AWS Management Console sign-in without MFA and the \\<cloudtrail_log_group_name> taken from audit step 1. aws logs put-metric-filter --log-group-name \\<cloudtrail_log_group_name> -- filter-name \\<no_mfa_console_signin_metric> --metric-transformations metricName= \\<no_mfa_console_signin_metric> ,metricNamespace='CISBenchmark',metricValue=1 --filter-pattern '{ ($.eventName = \"ConsoleLogin\") && ($.additionalEventData.MFAUsed != \"Yes\") }' c) Detect root login attempts Create a metric filter based on the filter pattern provided which checks for \"Root'' account usage and the \\<cloudtrail_log_group_name> taken from audit step 1. aws logs put-metric-filter --log-group-name \\<cloudtrail_log_group_name> -- filter-name \\<root_usage_metric> --metric-transformations metricName= \\<root_usage_metric> ,metricNamespace='CISBenchmark',metricValue=1 --filterpattern '{ $.userIdentity.type = \"Root\" && $.userIdentity.invokedBy NOT EXISTS && $.eventType != \"AwsServiceEvent\" }' d) Detect changes made to Identity and Access Management (IAM) policies Create a metric filter based on filter pattern provided which checks for IAM policy changes and the \\<cloudtrail_log_group_name> taken from audit step 1. aws logs put-metric-filter --log-group-name \\<cloudtrail_log_group_name> -- filter-name \\<iam_changes_metric> --metric-transformations metricName= \\<iam_changes_metric> ,metricNamespace='CISBenchmark',metricValue=1 -- filter-pattern '{($.eventName=DeleteGroupPolicy)||($.eventName=DeleteRolePolicy)||($.eventNa me=DeleteUserPolicy)||($.eventName=PutGroupPolicy)||($.eventName=PutRolePolic y)||($.eventName=PutUserPolicy)||($.eventName=CreatePolicy)||($.eventName=Del etePolicy)||($.eventName=CreatePolicyVersion)||($.eventName=DeletePolicyVersi on)||($.eventName=AttachRolePolicy)||($.eventName=DetachRolePolicy)||($.event Name=AttachUserPolicy)||($.eventName=DetachUserPolicy)||($.eventName=AttachGr oupPolicy)||($.eventName=DetachGroupPolicy)}' e) Detect changes to CloudTrail's configurations Create a metric filter based on the filter pattern provided which checks for cloudtrail configuration changes and the \\<cloudtrail_log_group_name> taken from audit step 1. aws logs put-metric-filter --log-group-name \\<cloudtrail_log_group_name> -- filter-name \\<cloudtrail_cfg_changes_metric> --metric-transformations metricName= \\<cloudtrail_cfg_changes_metric> ,metricNamespace='CISBenchmark',metricValue=1 --filter-pattern '{ ($.eventName = CreateTrail) || ($.eventName = UpdateTrail) || ($.eventName = DeleteTrail) || ($.eventName = StartLogging) || ($.eventName = StopLogging) }' f) Detect failed console authentication attempts Create a metric filter based on the filter pattern provided which checks for AWS management Console Login Failures and the \\<cloudtrail_log_group_name> taken from audit step 1. aws logs put-metric-filter --log-group-name \\<cloudtrail_log_group_name> -- filter-name \\<console_signin_failure_metric> --metric-transformations metricName= \\<console_signin_failure_metric> ,metricNamespace='CISBenchmark',metricValue=1 --filter-pattern '{ ($.eventName = ConsoleLogin) && ($.errorMessage = \"Failed authentication\") }' g) Detect customer-created CMKs which have changed state to disabled or scheduled deletion Create a metric filter based on the filter pattern provided which checks for disabled or scheduled for deletion CMK's and the \\<cloudtrail_log_group_name> taken from audit step 1. aws logs put-metric-filter --log-group-name \\<cloudtrail_log_group_name> -- filter-name \\<disable_or_delete_cmk_changes_metric> --metrictransformations metricName= \\<disable_or_delete_cmk_changes_metric> ,metricNamespace='CISBenchmark',metricValue=1 --filter-pattern '{($.eventSource = kms.amazonaws.com) && (($.eventName=DisableKey)||($.eventName=ScheduleKeyDeletion)) }' h) Detect changes to S3 buckets Create a metric filter based on the filter pattern provided which checks for S3 bucket policy changes and the \\<cloudtrail_log_group_name> taken from audit step 1. aws logs put-metric-filter --log-group-name \\<cloudtrail_log_group_name> -- filter-name \\<s3_bucket_policy_changes_metric> --metric-transformations metricName= \\<s3_bucket_policy_changes_metric> ,metricNamespace='CISBenchmark',metricValue=1 --filter-pattern '{ ($.eventSource = s3.amazonaws.com) && (($.eventName = PutBucketAcl) || ($.eventName = PutBucketPolicy) || ($.eventName = PutBucketCors) || ($.eventName = PutBucketLifecycle) || ($.eventName = PutBucketReplication) || ($.eventName = DeleteBucketPolicy) || ($.eventName = DeleteBucketCors) || ($.eventName = DeleteBucketLifecycle) || ($.eventName = DeleteBucketReplication)) }' i) Detect changes to AWS Config configuration Create a metric filter based on the filter pattern provided which checks for AWS Configuration changes and the \\<cloudtrail_log_group_name> taken from audit step 1. aws logs put-metric-filter --log-group-name \\<cloudtrail_log_group_name> -- filter-name \\<aws_config_changes_metric> --metric-transformations metricName= \\<aws_config_changes_metric> ,metricNamespace='CISBenchmark',metricValue=1 --filter-pattern '{ ($.eventSource = config.amazonaws.com) && (($.eventName=StopConfigurationRecorder)||($.eventName=DeleteDeliveryChannel) ||($.eventName=PutDeliveryChannel)||($.eventName=PutConfigurationRecorder)) }' Note: You can choose your own metricName and metricNamespace strings. Using the same metricNamespace for all Foundations Benchmark metrics will group them together. j) Detect changes made to Security Groups Create a metric filter based on the filter pattern provided which checks for security groups changes and the \\<cloudtrail_log_group_name> taken from audit step 1. aws logs put-metric-filter --log-group-name \\<cloudtrail_log_group_name> -- filter-name \\<security_group_changes_metric> --metric-transformations metricName= \\<security_group_changes_metric> ,metricNamespace='CISBenchmark',metricValue=1 --filter-pattern '{ ($.eventName = AuthorizeSecurityGroupIngress) || ($.eventName = AuthorizeSecurityGroupEgress) || ($.eventName = RevokeSecurityGroupIngress) || ($.eventName = RevokeSecurityGroupEgress) || ($.eventName = CreateSecurityGroup) || ($.eventName = DeleteSecurityGroup) }' k) Changes made to NACLs Create a metric filter based on the filter pattern provided which checks for NACL changes and the \\<cloudtrail_log_group_name> taken from audit step 1. aws logs put-metric-filter --log-group-name \\<cloudtrail_log_group_name> -- filter-name \\<nacl_changes_metric> --metric-transformations metricName= \\<nacl_changes_metric> ,metricNamespace='CISBenchmark',metricValue=1 -- filter-pattern '{ ($.eventName = CreateNetworkAcl) || ($.eventName = CreateNetworkAclEntry) || ($.eventName = DeleteNetworkAcl) || ($.eventName = DeleteNetworkAclEntry) || ($.eventName = ReplaceNetworkAclEntry) || ($.eventName = ReplaceNetworkAclAssociation) }' l) Changes to the network Create a metric filter based on the filter pattern provided which checks for network gateways changes and the \\<cloudtrail_log_group_name> taken from audit step 1. aws logs put-metric-filter --log-group-name \\<cloudtrail_log_group_name> -- filter-name \\<network_gw_changes_metric> --metric-transformations metricName= \\<network_gw_changes_metric> ,metricNamespace='CISBenchmark',metricValue=1 --filter-pattern '{ ($.eventName = CreateCustomerGateway) || ($.eventName = DeleteCustomerGateway) || ($.eventName = AttachInternetGateway) || ($.eventName = CreateInternetGateway) || ($.eventName = DeleteInternetGateway) || ($.eventName = DetachInternetGateway) }' m) Detect changes to route tables Create a metric filter based on the filter pattern provided which checks for route table changes and the \\<cloudtrail_log_group_name> taken from audit step 1. aws logs put-metric-filter --log-group-name \\<cloudtrail_log_group_name> -- filter-name \\<route_table_changes_metric> --metric-transformations metricName= \\<route_table_changes_metric> ,metricNamespace='CISBenchmark',metricValue=1 --filter-pattern '{ ($.eventName = CreateRoute) || ($.eventName = CreateRouteTable) || ($.eventName = ReplaceRoute) || ($.eventName = ReplaceRouteTableAssociation) || ($.eventName = DeleteRouteTable) || ($.eventName = DeleteRoute) || ($.eventName = DisassociateRouteTable) }' n) Detect changes made to VPCs Create a metric filter based on filter pattern provided which checks for VPC changes and the \\<cloudtrail_log_group_name> taken from audit step 1. aws logs put-metric-filter --log-group-name \\<cloudtrail_log_group_name> -- filter-name \\<vpc_changes_metric> --metric-transformations metricName= \\<vpc_changes_metric> ,metricNamespace='CISBenchmark',metricValue=1 -- filter-pattern '{ ($.eventName = CreateVpc) || ($.eventName = DeleteVpc) || ($.eventName = ModifyVpcAttribute) || ($.eventName = AcceptVpcPeeringConnection) || ($.eventName = CreateVpcPeeringConnection) || ($.eventName = DeleteVpcPeeringConnection) || ($.eventName = RejectVpcPeeringConnection) || ($.eventName = AttachClassicLinkVpc) || ($.eventName = DetachClassicLinkVpc) || ($.eventName = DisableVpcClassicLink) || ($.eventName = EnableVpcClassicLink) }' Note: You can choose your own metricName and metricNamespace strings. Using the same metricNamespace for all Foundations Benchmark metrics will group them together. Step 2. Create an SNS topic that the alarm will notify aws sns create-topic --name \\<sns_topic_name> Note: You can execute this command once and then reuse the same topic for all monitoring alarms. Step 3. Create an SNS subscription to the topic created in step 2 aws sns subscribe --topic-arn \\<sns_topic_arn> --protocol \\<protocol_for_sns> - -notification-endpoint \\<sns_subscription_endpoints> Note: You can execute this command once and then reuse the SNS subscription for all monitoring alarms. Step 4. Create an alarm that is associated with the CloudWatch Logs Metric Filter created in step 1 and an SNS topic created in step 2 aws cloudwatch put-metric-alarm --alarm-name \\<x1> -- metric-name \\<x2> --statistic Sum --period 300 --threshold 1 --comparison-operator GreaterThanOrEqualToThreshold --evaluation-periods 1 --namespace 'CISBenchmark' --alarm-actions \\<sns_topic_arn> Depending on the rule you chose in step 1, continue with that rule now, replacing \\<x1> and \\<x2> as per either: a) Detect unauthorized API calls \\<x1> = \\<unauthorized_api_calls_alarm> \\<x2> = \\<unauthorized_api_calls_metric> b) Console logins that are not protected by multi-factor authentication (MFA) \\<x1> = \\<no_mfa_console_signin_alarm> \\<x2> = \\<no_mfa_console_signin_metric> c) Detect root login attempts \\<x1> = \\<root_usage_alarm> \\<x2> = \\<root_usage_metric> d) Changes made to Identity and Access Management (IAM) policies \\<x1> = \\<iam_changes_alarm> \\<x2> = \\<iam_changes_metric> e) Changes to CloudTrail's configurations \\<x1> = \\<cloudtrail_cfg_changes_alarm> \\<x2> = \\<cloudtrail_cfg_changes_metric> f) Failed console authentication attempts \\<x1> = \\<console_signin_failure_alarm> \\<x2> = \\<console_signin_failure_metric> g) Customer-created CMKs which have changed state to disabled or scheduled deletion \\<x1> = \\<disable_or_delete_cmk_changes_alarm> \\<x2> = \\<disable_or_delete_cmk_changes_metric> h) Detect changes to S3 buckets \\<x1> = \\<s3_bucket_policy_changes_alarm> \\<x2> = \\<s3_bucket_policy_changes_metric> i) Detect changes to AWS Config configuration \\<x1> = \\<aws_config_changes_alarm> \\<x2> = \\<aws_config_changes_metric> j) Detect changes made to Security Groups \\<x1> = \\<security_group_changes_alarm> \\<x2> = \\<security_group_changes_metric> k) Changes made to NACLs \\<x1> = \\<nacl_changes_alarm> \\<x2> = \\<nacl_changes_metric> l) Changes to the network \\<x1> = \\<network_gw_changes_alarm> \\<x2> = \\<network_gw_changes_metric> m) Detect changes to route tables \\<x1> = \\<route_table_changes_alarm> \\<x2> = \\<route_table_changes_metric> n) Detect changes made to VPCs \\<x1> = \\<vpc_changes_alarm> \\<x2> = \\<vpc_changes_metric> Networking Ingress access (4.1 and 4.2) Ensure no security groups allow ingress from 0.0.0.0/0 to port 3389 Security groups provide stateful filtering of ingress/egress network traffic to AWS resources. It is recommended that no security group allows unrestricted ingress access to port 3389 and port 22. Perform the following to remove unrestricted ingress access to port 3389 and port 22 via the AWS Management Console: Step 1. Log in to the AWS Management Console at https://console.aws.amazon.com/vpc/home Step 2. In the left pane, click on Security Groups Step 3. For each security group, perform the following: Select the security group. Click on the Inbound Rules tab. Identify the rules to be removed. Click on x in the \"Remove\" column. Click on Save. Ensure the default security group of every VPC restricts all traffic (4.3) A VPC comes with a default security group whose initial settings: Deny all inbound traffic Allow all outbound traffic Allow all traffic between instances assigned to the security group. If you don't specify a security group when you launch an instance, the instance is automatically assigned to this default security group. Security groups provide stateful filtering of ingress/egress network traffic to AWS resources. It is recommended that the default security group restrict all traffic. The default VPC in every region should have its default security group updated to comply. Any newly created VPCs will automatically contain a default security group that will need remediation to comply with this recommendation. Security group members can perform the following to implement the prescribed state via the AWS Management Console: Step 1. Identify the AWS resources that exist within the default security group. Step 2. Create a set of least privilege security groups for those resources. Step 3. Place the resources in those security groups. Step 4. Remove the resources noted in #1 from the default security group Security Group State Step 5. Log in to the AWS Management Console at https://console.aws.amazon.com/vpc/home . Step 6. Repeat the next steps for all VPCs, including the default VPC in each AWS region. Step 7. In the left pane, click on Security Groups. Step 8. For each default security group, perform the following: Select the default security group. Click on the Inbound Rules tab. Remove any inbound rules. Click on the Outbound Rules tab. Remove any outbound rules. Recommended: IAM groups allow you to edit the \"name\" field. After remediating default groups rules for all VPCs in all regions, edit this field to add text similar to \"DO NOT USE. DO NOT ADD RULES.\" Ensure routing tables for VPC peering are \"least access\" (4.4) Once a VPC peering connection is established, routing tables must be updated to establish any connections between the peered VPCs. These routes can be as specific as desired. You can even peer a VPC to just a single host on the other side of the connection. Remove and add route table entries to ensure that the least number of subnets or hosts is required to accomplish the purpose for peering are routable. To give VPC peering routing tables \"least access\" via CLI: Step 1. For each \\<route_table_id> containing routes non compliant with your routing policy (which grants more than desired \"least access\"), delete the non compliant route: aws ec2 delete-route --route-table-id \\<route_table_id> --destination-cidrblock \\<non_compliant_destination_CIDR> Step 2. Create a new compliant route: aws ec2 create-route --route-table-id \\<route_table_id> --destination-cidrblock \\<compliant_destination_CIDR> --vpc-peering-connection-id \\<peering_connection_id> Other Rules Ensure that GuardDuty enabled for required region By enabling GuardDuty, you will receive a threat detection service. This service continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts, workloads, and data stored in Amazon S3 by event logs analysis. We are excluding verification for aws-global region from possible regions for GuardDuty. Perform the following steps under the below link to enable GuardDuty: https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_settingup.html Ensure there no stale roles with Attached Policies for S3 access Avoid stale roles as these could cause access leakage and uncontrolled manipulation with S3 bucket data, which can lead to ransomware violations. This rule checks inline policies only. The Attached policies are verified under the respective rule. Perform the following to delete stale roles via the AWS Management Console: Step 1. Log in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/ . Step 2. In the navigation pane of the IAM console, click on Roles . Then select the check box next to the role name you want to delete, not the name or row itself. Step 3. For \"Role actions\" at the top of the page, click on Delete . Step 4. In the confirmation dialog box, review the last accessed information to see when each of the selected roles last accessed an AWS service. This helps you to confirm whether the role is currently active. If you want to proceed, click on Yes, Delete to submit the service-linked role for deletion. Step 5. Observe the IAM console notifications to monitor the progress of the service-linked role deletion. Because the IAM service-linked role deletion is asynchronous, after you submit the role for deletion, the deletion task can succeed or fail. If the task succeeds, then the role is removed from the list and notification of success appears at the top of the page. If the task fails, you can click on View details or View Resources from the notifications to learn why the deletion failed. If the deletion fails because the role uses the service's resources, then the notification includes a list of resources, if the service returns that information. You can then clean up the resources and submit the deletion again. https://aws.amazon.com/blogs/security/identify-unused-iam-roles-remove-confidently-last-used-timestamp/ https://docs.aws.amazon.com/IAM/latest/UserGuide/using-service-linked-roles.html#delete-service-linked-role Ensure all EC2 EBS Volumes has snapshots Snapshot availability could prevent data loss and simplify data recovery in the case of data encryption. Perform the following to create snapshots for EC2 EBS Volumes via the AWS Management Console: Step 1. Log in to the Amazon EC2 console at https://console.aws.amazon.com/ec2/ . Step 2. Click on Snapshots under the Elastic Block Store in the navigation pane. Step 3. Click on Create Snapshot . Step 4. For \"Select resource type\" click on Volume . Step 5. For \"Volume,\"\" select the volume. Step 6 (Optional). Enter a description for the snapshot. Step 7 (Optional). Choose Add Tag to add tags to your snapshot. For each tag, provide a tag key and a tag value. Step 8. Click on Create Snapshot . Ensure all EC2 instances are managed by SSM Systems Manager (SSM) helps you maintain security and compliance by scanning your managed instances and reporting on or taking corrective action on any policy violations that it detects. In cases where SSM is enabled for a specific region, but EC2 instances are not managed, they will not be monitored from any kind of SSM perspective. Perform the following to configure EC2 instances for use with SMM via the AWS Management Console: Step 1. Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/ . Step 2. In the navigation pane, under \"Instances\" click on Instances . Step 3. Navigate to and choose your EC2 instance from the list. Step 4. In the \"Actions\" menu, click on Security , Modify IAM role. Step 5. For the IAM role, select the instance profile you created for that perspective. Ensure that SecurityHub enabled for required region AWS Security Hub gives you a comprehensive view of your security alerts and security posture across your AWS accounts. SecurityHub is a cloud security posture management service that automatically checks for best security practices, bundles alerts, and automates remediation. AWS security group allows access to known command and control destinations The AWS security group allows access to destinations that are known or suspected of being command and control systems used in ransomware and botnet attacks. Perform the following to gain access to known command and control destinations with the minimum required connectivity via the AWS Management Console: Step 1. Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/ and click on Security Groups . Step 2. For each group found by this rule: Select the given rule. Click on the \"Outbound Rules\" tab. Click on Edit outbound rules . Edit any CIDR/IP addresses and minimize the permitted scope to just the minimum required for connectivity. Ensure that S3 Bucket has MFA option enabled for changing Bucket Versioning settings and permanently deleting object versions S3 Buckets should be protected from ransomware attacks by configuring versioning and MFA Delete. Doing so will disallow immediate bucket content removal, data encryption, or any other harmful modifications. Disabled versioning is also considered a violation by this rule. The reason for that is that the attacker may make the bucket vulnerable by disabling object versioning with the s3:PutBucketVersioning permission. Perform the following to enable S3 bucket versioning and MFA Delete via the AWS Management Console: Step 1. Log in to the AWS Management Console and open the S3 console at https://console.aws.amazon.com/s3 . Step 2. Under \"All Buckets,\"\" click on the target S3 bucket. Step 3. Click on the Properties tab. Step 4. Find the \"Bucket Versioning\" section. Step 5. Click on Edit and enable versioning. Step 6. Follow the AWS instructions to add the MFA Delete option via CLI or SDK. Ensure S3 bucket deny overriding of default KMS Key encryption Attackers that have S3:PutObject permission can still override the default KMS Key encryption on updating object operation with their own provided KMS Key, thus leading to a ransomware violation. To prevent this method of malicious access, the bucket owner should therefore define a policy to allow object modification using only the defined default KMS Key, which attackers are unlikely to have permissions to change or modify. Perform the following to enable S3 bucket policy control over the KMS Key via the AWS Management Console: Step 1. Log in to the AWS Management Console and open the S3 console at https://console.aws.amazon.com/s3 . Step 2. In the Buckets list, choose the name of the bucket that you want to create a bucket policy for or whose bucket policy you want to edit. Step 3. Click on Permissions . Step 4. Under \"Bucket policy,\"\" click on Edit . Step 5. In the \"Policy\" text field, type or copy and paste a new bucket policy, or edit an existing policy. The bucket policy is a JSON file. The text you type in the editor must be valid JSON. Step 6. Add policy Condition statement where you define the KMS Key to be matched for Statement with s3:PutObject permission Step 7. Condition example for Allow effect for: \"Condition\":{\"StringEquals\":{\"s3:x-amz-server-side-encryption-aws-kms-key-id\":\"arn:aws:kms:REGION:ACCOUNT-ID:key/KEY-ID\"}}} Note: Reference: https://rhinosecuritylabs.com/aws/s3-ransomware-part-2-prevention-and-defense/#:~:text=Another%20feature%20of,be%20found%20here . https://docs.aws.amazon.com/AmazonS3/latest/userguide/add-bucket-policy.html https://docs.aws.amazon.com/service-authorization/latest/reference/list_amazons3.html Ensure that S3 Bucket restrict public access by ACL and policy S3 Buckets should not be reachable outside of the project by default. Any violation of this rule could cause major vulnerabilities and data loss Perform the following to restrict public access to S3 bucket via the AWS Management Console: Step 1. Log in to the AWS Management Console and open the S3 console at https://console.aws.amazon.com/s3 . Step 2. On the left side menu, find the option \"Block Public Access settings for this account.\"\" Step 3. Edit the settings and click on the required level of restriction. The option to \"Block all public access\" is preferable and will set it globally. Step 4. Open the bucket, then open the tab \"Permissions.\" Step 5. Find the \"Access control list (ACL)\" part and ensure only the bucket owner has access to it. Step 6. Find \"Bucket policy\" and check for any public configuration like \"principal\": * etc. Note: Reference: https://docs.amazonaws.cn/en_us/AmazonS3/latest/userguide/access-control-block-public-access.html https://aws.amazon.com/s3/features/block-public-access/ Ensure S3 bucket has no server-side encryption being enabled by another account S3 bucket policies allow uploaded files to be encrypted with AES256 or a specific AWS KMS key. In the case when a specified KMS Key belongs to a different account, it can lead to a high risk of data encryption without the ability to decrypt it. Perform the following to clear cross-account KMS Key on S3 via the AWS Management Console: Step 1. Log in to the AWS Management Console and open the S3 console at https://console.aws.amazon.com/s3 . Step 2. In the \"Buckets\" list, choose a bucket name. Step 3. Click on Properties . Step 4. Under \"Default encryption,\"\" click on Edit . Step 5. Remove the cross-account KMS Key and specify the account-related KMS Key for data encryption Step 6. Consider the rule aws_058 aws_s3_prevent_default_kms_key_override.yaml remediation to prevent such violations in the future.","title":"Aws"},{"location":"Configuration%20Guides/aws/#identity-and-access-management","text":"Avoid the use of the \"root\" account (1.1) The \"root\" account has unrestricted access to all resources in the AWS account. It is recommended that the use of this account be avoided. Ensure multi-factor authentication (MFA) is enabled for all IAM users that have a console password (1.2) Multi-Factor Authentication (MFA) adds an extra layer of protection on top of a username and password. With MFA enabled, when a user signs in to an AWS website, they will not only be prompted to enter their username and password but also an authentication code from their AWS linked MFA device. It is recommended that MFA be enabled for all accounts that have a console password. Perform the following to enable MFA via the AWS Management Console: Step 1. Log in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/ . Step 2. In the navigation pane, click on Users . Step 3. In the \"User Name\" list, choose the name of the intended MFA user. Step 4. Select the \"Security Credentials\" tab, and then click on Manage MFA Device . Step 5. In the \"Manage MFA Device\" wizard, click on A virtual MFA device , and then click on Next Step . Ensure credentials unused for 90 days or greater are disabled (1.3) AWS IAM users can access AWS resources using different types of credentials, such as passwords or access keys. It is recommended that all credentials that have been unused for 90 or more days be removed or deactivated. Perform the following to remove or deactivate credentials via the AWS Management Console: Step 1. Log in to the AWS Management Console at https://console.aws.amazon.com/vpc/home . Step 2. Click on Services . Step 3. Click on IAM. Step 4. Click on Users. Step 5. Click on Security Credentials. Step 6. As an Administrator, click on Make Inactive for credentials that have not been used in 90 Days. As an IAM User, click on Make Inactive or Delete for credentials that have not been used in 90 days. Ensure access keys are rotated every 90 days or less (1.4) Access keys consist of an access key ID and a secret access key, which are used together to sign programmatic requests that you make to AWS. AWS users need their own access keys to make programmatic calls to AWS from the AWS Command Line Interface (AWS CLI), Tools for Windows PowerShell, the AWS SDKs, or direct HTTP calls using the APIs for individual AWS services. It is recommended that all access keys be regularly rotated. Perform the following to rotate access keys via the AWS Management Console: Step 1. Log in to the AWS Management Console at https://console.aws.amazon.com/vpc/home . Step 2. Click on Services. Step 3. Click on IAM. Step 4. Click on Users. Step 5. Click on Security Credentials. Step 6. As an Administrator, click on Make Inactive for keys that have not been rotated in 90 Days. As an IAM User, click on Make Inactive or Delete for keys that have not been rotated or used in 90 Days. Step 7. Click on Create Access Key. Step 9. Update programmatic call with new Access Key credentials. Perform the following to rotate access keys via CLI: Run the following command: aws iam update-access-key aws iam create-access-key aws iam delete-access-key Password complexity (1.5 to 1.11) Password policies are, in part, used to enforce password complexity requirements. IAM password policies can be used to ensure that passwords comprise different character sets. It is strongly recommended that your organization's password policy: Requires at least one uppercase letter (1.5) Requires at least one lowercase letter (1.6) Requires at least one symbol (1.7) Requires at least one number (1.8) Requires a minimum password length of 14 characters (1.9) Prevents password reuse (1.10) Expires after 90 days or less (1.11) Perform the following to set the password policy to follow the rules above via the AWS management console: Step 1. Log in to the AWS Console at https://console.aws.amazon.com/vpc/home (with appropriate permissions to View Identity Access Management Account Settings). Step 2. Go to \"IAM Service\" on the AWS Console. Step 3. Click on Account Settings on the left pane. Step 4. a) To set at least one uppercase letter Check \"Requires at least one uppercase letter\" and click on Apply Password Policy . b) To require at least one lowercase letter Check \"Requires at least one lowercase letter\" and click on Apply Password Policy . c) To require at least one symbol Check \"Require at least one non-alphanumeric character\" and click on Apply Password Policy . d) To require at least one number Check \"Require at least one number\" and click on Apply Password Policy . e) To set a minimum password length of 14 characters Set \"Minimum password length\" to 14 or greater and click on Apply Password Policy . f) To prevent password reuse Check \"Prevent password reuse\" and set \"Number of passwords to remember\" to 24. g) To expire passwords after 90 days Check \"Enable password expiration\" and set \"Password expiration period (in days)\" to 90 or less. Perform the following to set the password policy to follow the rules above via CLI: a) To set at least one uppercase letter Run the following command: aws iam update-account-password-policy --require-uppercase-characters b) To require at least one lowercase letter Run the following command: aws iam update-account-password-policy --require-lowercase-characters c) To require at least one symbol Run the following command: aws iam update-account-password-policy --require-symbols d) To require at least one number Run the following command: aws iam update-account-password-policy --require-numbers e) To set a minimum length of 14 characters Run the following command: aws iam update-account-password-policy --minimum-password-length 14 f) To prevent password reuse Run the following command: aws iam update-account-password-policy --password-reuse-prevention 24 g) To expire passwords after 90 days Run the following command: aws iam update-account-password-policy --max-password-age 90 Note: All commands starting with \"aws iam update-account-password-policy\" have to be combined into a single command in order for all of them to take effect. Ensure no root account access key exists (1.12) The root account is the most privileged user in an AWS account. AWS Access Keys provide programmatic access to a given AWS account. It is recommended that all access keys associated with the root account be removed. Perform the following to remove access keys associated with the root account via the AWS Management Console: Step 1. Log in to the AWS Management Console as Root and open the IAM console at https://console.aws.amazon.com/iam/ . Step 2. Click on \\<Root_Account_Name> at the top right and select Security Credentials from the drop-down list. Step 3. On the pop-out screen, click on Continue to Security Credentials. Step 4. Click on Access Keys (Access Key ID and Secret Access Key). Step 5. Under the \"Status\" column, if there are any Keys that are Active: Click on Make Inactive (i.e., temporarily disable Key if you think you may need it again). Click Delete (i.e., delete the key for good. Deleted keys cannot be recovered ). MFA for the root account (1.13 and 1.14) The root account is the most privileged user in an AWS account. MFA adds an extra layer of protection on top of a username and password used to access this account. With MFA enabled, when a user signs in to an AWS website, they will be prompted for their username and password as well as for an authentication code from their AWS MFA device. For Level 2, it is recommended that the root account be protected with a hardware MFA. Perform the following to establish virtual MFA/hardware MFA for the root account via the AWS Management Console: Step 1. Log in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/ . Note: To manage MFA devices for the root AWS account, you must use your root account credentials to sign in to AWS. You cannot manage MFA devices for the root account using other credentials. Step 2. Click on Dashboard , and under \"Security Status\" expand \"Activate MFA\" on your root account. Step 3. Click on Activate MFA. Step 4. a) Virtual MFA In the wizard, click on A virtual MFA device and then click on Next Step. b) Hardware MFA In the wizard, click on A hardware MFA device and then click on Next Step . Step 5. a) Virtual MFA IAM generates and displays configuration information for the virtual MFA device, including a QR code graphic. The graphic is a representation of the \"secret configuration key\" that is available for manual entry on devices that do not support QR codes. b) Hardware MFA In the \"Serial Number\" box, enter the serial number found on the back of the MFA device. Step 6. a) Virtual MFA Open your virtual MFA application. For a list of apps that you can use for hosting virtual MFA devices, see \"Virtual MFA Applications.\" If the virtual MFA application supports multiple accounts (multiple virtual MFA devices), choose the option to create a new account (a new virtual MFA device). b) Hardware MFA In the \"Authentication Code 1\" box, enter the six-digit number displayed by the MFA device. You might need to press the button on the front of the device to display the number. Wait 30 seconds while the device refreshes the code, and then enter the next six-digit number into the \"Authentication Code 2\" box. You might need to press the button on the front of the device again to display the second number. Step 7. a) Virtual MFA Determine whether the MFA app supports QR codes, and then do one of the following: Use the app to scan the QR code. For example, you might choose the camera icon or choose an option similar to Scan code. Then, use the device's camera to scan the code. In the \"Manage MFA Device\" wizard, click on Show secret key for manual configuration. Then, type the secret configuration key into your MFA application. When you are finished, the virtual MFA device starts generating one-time passwords. Now, a) Go to the \"Manage MFA Device\" wizard, b) In the \"Authentication Code 1\" box, type the one time password that currently appears in the virtual MFA device, c) Wait up to 30 seconds for the device to generate a new one-time password, d) Type the second one-time password into the \"Authentication Code 2\" box, d) Choose Active Virtual MFA . b) Hardware MFA Click on Next Step . The MFA device is now associated with the AWS account. The next time you use your AWS account credentials to sign in, you will need to type in a code from the hardware MFA device. Ensure security questions are registered in the AWS account (1.15) The AWS support portal allows account owners to establish security questions that can be used to authenticate individuals calling AWS customer service for support. It is recommended that security questions be established. Perform the following to register security questions in the AWS account via the AWS Management Console: Step 1. Log in to the AWS Management Console at https://console.aws.amazon.com/vpc/home as a root user. Step 2. Click on the \\<Root_Account_Name> from the top right of the console. Step 3. From the drop-down menu, click My Account. Step 4. Scroll down to the \"Configure Security Questions\" section and click on Edit. Step 5. Click on each Question From the drop-down menu, select an appropriate question. Click on the Answer section. Enter an appropriate answer. Follow this process for all three questions. Step 6. Click Update when complete. Step 7. Store the security questions and answers in a secure physical location. Ensure IAM policies are attached only to groups or roles (1.16) By default, IAM users, groups, and roles have no access to AWS resources. IAM policies allow certain privileges to be granted to users, groups, or roles. It is recommended that IAM policies be applied directly to groups and roles but not users. Perform the following to create an IAM group and assign a policy to it via the AWS Management Console: Step 1. Log in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/ . Step 2. In the navigation pane, click on Groups and then click on Create New Group . Step 3. In the \"Group Name\" box, type the name of the group and then click on Next Step. Step 4. In the list of policies, select the checkbox for each policy that you want to apply to all members of the group. Then click on Next Step . Step 5. Click on Create Group . Step 6. Perform the following to add a user to a given group: Log in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/ . In the navigation pane, click on Groups . Select the group you want to add a user to. Click on Add Users To Group. Select the users to be added to the group. Click on Add Users. Step 7. Perform the following to remove a direct association between a user and policy: Log in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/ . In the left navigation pane, click on Users. For each user, a) Select the user, b) Select the \"Permissions\" tab, c) Expand \"Managed Policies\" d) Click on Detach Policy for each policy, e) Expand \"Inline Policies,\" f) Click on Remove Policy for each policy. Maintain current contact details (1.17) Ensure email and telephone details for AWS accounts are current and map to more than one individual in your organization. AWS will use these details to contact the account owner if any activity is judged to be in breach of the Acceptable Use Policy or determined to be indicative of likely security compromise by the AWS Abuse team. Since an AWS account supports a number of contact details, it is strongly recommended that organizations maintain contact details for more than just a single individual. This is because circumstances may arise where that individual is unavailable. Email contact details should point to a mail alias that forwards emails to multiple individuals within the organization. Where feasible, phone contact details should point to a PBX hunt group or another call-forwarding system. This activity can only be performed via the AWS Console, with a user who has permission to read and write Billing information (aws-portal:*Billing ): Step 1. Log in to the AWS Management Console and open the Billing and Cost Management console at https://console.aws.amazon.com/billing/home#/ . Step 2. In the navigation bar, choose your account name, and then click on My Account . Step 3. On the \"Account Settings\" page, next to \"Account Settings\" click on Edit . Step 4. Next to the field that you need to update, click on Edit . Step 5. After you have entered your changes, click on Save changes . Step 6. After you have made your changes, click on Done . Step 7. To edit your contact information, under \"Contact Information\" click on Edit . Step 8. For the fields that you want to change, type your updated information, and then click on Update . Ensure security contact information is registered (1.18) AWS gives customers the option to specify contact information for the account's security team. It is recommended that this information be provided. Perform the following to establish security contact information via the AWS Management Console: Step 1. Log in to the AWS Management Console at https://console.aws.amazon.com/vpc/home . Step 2. Click on your account name at the top right corner of the console. Step 3. From the drop-down menu, click on My Account . Step 4. Scroll down to the \"Alternate Contacts\" section. Step 5. Enter the contact information in the \"Security\" section. Note: Consider specifying an internal email distribution list to ensure emails are regularly monitored by more than one individual. Ensure IAM instance roles are used for AWS resource access from instances (1.19) AWS access from within AWS instances can be done either by: Encoding AWS keys into AWS API calls, or Assigning the instance to a role that has an appropriate permissions policy for the required access. \"AWS Access\" means accessing the APIs of AWS in order to access AWS resources or manage AWS account resources. IAM roles can only be associated at the launch of an instance. To remediate an instance and add it to a role, you must create a new instance. If the instance has no external dependencies on its current private IP or if public addresses are elastic IPs, via the AWS Management Console: Step 1. In AWS IAM, create a new role. Assign a permissions policy if the needed permissions are already known. Step 2. In the AWS console, launch a new instance with identical settings to the existing instance, and ensure that the newly created role is selected. Step 3. Shut down both the existing instance and the new instance. Step 4. Detach disks from both instances. Step 5. Attach the existing instance disks to the new instance. Step 6. Boot the new instance and you should have the same machine, but with the associated role. Note 1: If your environment has dependencies on a dynamically assigned PRIVATE IP address, you can create an AMI from the existing instance and destroy the old one. Then, when launching from the AMI, manually assign the previous private IP address. Note 2: If your environment has dependencies on a dynamically assigned PUBLIC IP address, there is no way to ensure the address is retained and assign it an instance role. Dependencies on dynamically assigned public IP addresses are a bad practice. If possible, you may wish to rebuild the instance with a new elastic IP address and make the investment to remediate affected systems while assigning the system to a role. Ensure a support role has been created to manage incidents with AWS Support (1.20) AWS provides a support center that can be used for incident notification and response, as well as technical support and customer service. Create an IAM Role to allow authorized users to manage incidents with AWS Support. To create a support role to manage incidents with AWS support, use the Amazon unified command-line interface and perform the following: Step 1. Create a trust relationship policy document that allows \\<iam_user> to manage AWS incidents, and save it locally as /tmp/TrustPolicy.json: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::321859670049:root\" }, \"Action\": \"sts:AssumeRole\" } ] } Step 2. Create the IAM role using the above trust policy: aws iam create-role --role-name \\<aws_support_iam_role> --assume-rolepolicy-document file:///tmp/TrustPolicy.json Step 3. Attach 'AWSSupportAccess' managed policy to the created IAM role: aws iam attach-role-policy --policy-arn \\<iam_policy_arn> --role-name \\<aws_support_iam_role> Do not set up access keys during initial user setup for all IAM users that have a console password (1.21) AWS console marks the checkbox for creating access keys enabled by default. This results in many access keys being generated unnecessarily. In addition to unnecessary credentials, this also generates unnecessary management work when it comes to auditing and rotating these keys. Perform the following to delete access keys that do not pass the audit via the AWS Management Console: Step 1. Log in to the AWS Management Console at https://console.aws.amazon.com/vpc/home . Step 2. Click on Services . Step 3. Click on IAM. Step 4. Click on Users. Step 5. Click on Security Credentials. Step 6. As an Administrator, click on Delete for keys that were created at the same time as the user profile but have not been used. As an IAM User, click on Delete for keys that were created at the same time as the user profile but have not been used. Perform the following to delete access keys that do not pass the audit via CLI: Run the following command: iam delete-access-key Ensure IAM policies that allow full administrative privileges are not created (1.22) IAM policies are the means by which privileges are granted to users, groups, or roles. It is recommended and considered standard security advice to grant these based on the principle of least privilege. In other words, organizations should never grant users, groups, or roles permissions beyond the minimum required to perform a particular task. Determine what users need to do and then craft policies that let the users perform only those tasks instead of allowing full administrative privileges. Perform the following to detach the policy that grants full administrative privileges from users via the AWS Management Console: Step 1. Log in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/ . Step 2. In the navigation pane, click on Policies and then search for the policy name found in the audit step. Step 3. Select the policy that needs to be deleted. Step 4. In the policy action menu, click on Detach . Step 5. Select all Users, Groups, and Roles that have this policy attached. Step 6. Click on Detach Policy . Step 7. In the policy action menu, click on Detach. Perform the following to detach the policy that grants full administrative privileges from users via the CLI: Step 1. Lists all IAM users, groups, and roles that the specified managed policy is attached to. aws iam list-entities-for-policy --policy-arn \\<policy_arn> Step 2. Detach the policy from all IAM Users: aws iam detach-user-policy --user-name \\<iam_user> --policy-arn \\<policy_arn> Step 3. Detach the policy from all IAM Groups: aws iam detach-group-policy --group-name \\<iam_group> --policy-arn \\<policy_arn> Step 4. Detach the policy from all IAM Roles: aws iam detach-role-policy --role-name \\<iam_role> --policy-arn \\<policy_arn>","title":"Identity and Access Management"},{"location":"Configuration%20Guides/aws/#logging","text":"Ensure CloudTrail is enabled in all regions (2.1) Perform the following to enable global (Multi-region) CloudTrail logging via the AWS Management Console: Step 1. Log in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/cloudtrail . Step 2. Click on Trails on the left navigation pane. Step 3. Click on Get Started Now , if presented: Click on Add new trail. Enter a trail name in the \"Trail name\" box. Set the \"Apply trail to all regions\" option to \"Yes.\"\" Specify an S3 bucket name in the S3 bucket box. Click on Create . Step 4. If one or more trails already exist, select the target trail to enable global logging. Step 5. Select the edit icon (pencil) next to \"Apply trail to all regions,\" click on Yes and then click on Save . Step 6. Select the edit icon (pencil) next to \"Management Events,\" click on All for setting \"Read/Write Events,\" and then click on Save . Perform the following to enable global (Multi-region) CloudTrail logging via CLI: Run the following command: aws cloudtrail create-trail --name \\<trail_name> --bucket-name \\<s3_bucket_for_cloudtrail> --is-multi-region-trail aws cloudtrail update-trail --name \\<trail_name> --is-multi-region-trail Note: Creating CloudTrail via CLI without providing any overriding options configures Management Events to set All types of Read/Writes by default. Ensure CloudTrail log file validation is enabled (2.2) CloudTrail log file validation creates a digitally signed digest file containing a hash of each log that CloudTrail writes to S3. These digest files can be used to determine whether a log file was changed, deleted, or unchanged after CloudTrail delivered the log. It is recommended that file validation be enabled on all CloudTrails. Perform the following to enable log file validation on a given trail via the AWS Management Console: Step 1. Log in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/cloudtrail . Step 2. Click on Trails on the left navigation pane. Step 3. Click on the target trail. Step 4. Within the S3 section, click on the edit icon (pencil). Step 5. Click on Advanced Step 6. Click on the Yes radio button in the section \"Enable log file validation.\" Step 7. Click on Save. Perform the following to enable log file validation on a given trail via CLI: Run the following command: aws cloudtrail update-trail --name \\<trail_name> --enable-log-file-validation Note: Periodic validation of logs using these digests can be performed by running the following command: aws cloudtrail validate-logs --trail-arn \\<trail_arn> --start-time \\<start_time> --end-time \\<end_time> Ensure the S3 bucket used to store CloudTrail logs is not publicly accessible (2.3) CloudTrail logs a record of every API call made in your AWS account. These logs are stored in an S3 bucket. It is recommended that the bucket policy, or access control list (ACL), is applied to the S3 bucket that CloudTrail logs to prevent public access to the CloudTrail logs. Perform the following to remove any public access that has been granted to the bucket via an ACL or S3 bucket policy via the AWS Management Console: Step 1. Go to the Amazon S3 console at https://console.aws.amazon.com/s3/home . Step 2. Right-click on the bucket and click on Properties. Step 3. In the \"Properties\" pane, click on the Permissions tab. The tab shows a list of permissions granted, with one row per permission granted, in the bucket ACL. Each row identifies the grantee and the permissions granted. Step 4. Select the row that grants permission to \"Everyone\" or \"Any Authenticated User.\" Step 5. Uncheck all the permissions granted to \"Everyone\" or \"Any Authenticated User\" (click on x to delete the row). Step 6. Click Save to save the ACL. Step 7. If the \"Edit bucket policy\" button is present, click on it. Step 8. Remove any Statement that has an Effect set to \u201cAllow\u201d and a Principal set to \"\" or {\"AWS\" : \"\"}. Ensure CloudTrail trails are integrated with CloudWatch Logs (2.4) AWS CloudTrail is a web service that records AWS API calls made in a given AWS account. The information recorded here includes the: Identity of the API caller Time of the API call Source IP address of the API caller Request parameters Response elements returned by the AWS service. CloudTrail uses Amazon S3 for log file storage and delivery. In addition to capturing CloudTrail logs within a specified S3 bucket for long-term analysis, real-time analysis can be performed by configuring CloudTrail to send logs to CloudWatch Logs. For a trail that is enabled in all regions within an account, CloudTrail sends log files from all regions to a CloudWatch Logs log group. It is recommended that CloudTrail logs be sent to CloudWatch Logs Perform the following to integrate CloudTrail trails with CloudWatch Logs via the AWS Management Console: Step 1. Log in to the AWS Management Console and open the CloudTrail console at https://console.aws.amazon.com/cloudtrail/ . Step 2. Under \"All Buckets,\"\" click on the target bucket you want to evaluate. Step 3. Click on Properties on the top right of the console. Step 4. Click on Trails in the left menu. Step 5. Click on each trail where no CloudWatch Logs are defined. Step 6. Go to the \"CloudWatch Logs\" section and click on Configure. Step 7. Define a new or select an existing log group. Step 8. Click on Continue. Step 9. Configure the IAM Role, which will deliver CloudTrail events to CloudWatch Logs. Step 10. Create/Select an IAM Role and Policy Name. Step 11. Click on Allow to continue. Perform the following to integrate CloudTrail trails with CloudWatch Logs via CLI: Run the following command: aws cloudtrail update-trail --name \\<trail_name> --cloudwatch-logs-log-grouparn \\<cloudtrail_log_group_arn> --cloudwatch-logs-role-arn \\<cloudtrail_cloudwatchLogs_role_arn> Ensure AWS Config is enabled in all regions (2.5) AWS Config is a web service that performs configuration management of supported AWS resources within your account and delivers log files to you. The recorded information includes the: Configuration item (AWS resource). Relationships between configuration items (AWS resources). Any configuration changes between resources. It is recommended that AWS Config is enabled in all regions. Perform the following to implement AWS Config configuration via the AWS Management Console: Step 1. Select the region you want to focus on in the top right of the console. Step 2. Click on Services. Step 3. Click on Config Step 4. Define which resources you want to record in the selected region. Step 5. Choose to include global resources (IAM resources). Step 6. Specify an S3 bucket in the same account or another managed AWS account Step 7. Create an SNS Topic from the same AWS account or another managed AWS account Perform the following to implement AWS Config configuration via CLI: Step 1. Ensure there is an appropriate S3 bucket, SNS topic, and IAM role per the AWS Config Service prerequisites. Step 2. Run this command to set up the configuration recorder aws configservice subscribe --s3-bucket my-config-bucket --sns-topic arn:aws:sns:us-east-1:012345678912:my-config-notice --iam-role arn:aws:iam::012345678912:role/myConfigRole Step 3. Run this command to start the configuration recorder: start-configuration-recorder --configuration-recorder-name \\<value Ensure S3 bucket access logging is enabled on the CloudTrail S3 bucket (2.6) S3 Bucket Access Logging generates a log that contains access records for each request made to your S3 bucket. An access log record contains details about the request, such as the: Request type Resources specified in the request Time and date that the request was processed. It is recommended that bucket access logging be enabled on the CloudTrail S3 bucket. Perform the following to enable S3 bucket logging via the AWS Management Console: Step 1. Log in to the AWS Management Console and open the S3 console at https://console.aws.amazon.com/s3 . Step 2. Under \"All Buckets\" click on the target S3 bucket. Step 3. Click on Properties in the top right of the console. Step 4. Under Bucket: \\<s3_bucket_for_cloudtrail> click on Logging. Step 5. To configure bucket logging: Click on the Enabled checkbox. Select the Target Bucket from the list. Enter a Target Prefix. Click Save . Ensure CloudTrail logs are encrypted at rest using KMS CMKs (2.7) AWS CloudTrail is a web service that records AWS API calls for an account and makes those logs available to users and resources in accordance with IAM policies. AWS Key Management Service (KMS) is a managed service that helps create and control the encryption keys used to encrypt account data and uses Hardware Security Modules (HSMs) to protect the security of encryption keys. CloudTrail logs can be configured to leverage server-side encryption (SSE) and KMS customer-created master keys (CMK) to further protect CloudTrail logs. It is recommended that CloudTrail be configured to use SSE-KMS. Perform the following to configure CloudTrail to use SSE-KMS via the AWS Management Console: Step 1. Log in to the AWS Management Console and open the CloudTrail console at https://console.aws.amazon.com/cloudtrail . Step 2. In the left navigation pane, click on Trails. Step 3. Click on a Trail. Step 4. Under the S3 section, click on the edit button (pencil icon). Step 5. Click on Advanced . Step 6. Select an existing CMK from the KMS key Id drop-down menu. Note 1: Ensure the CMK is located in the same region as the S3 bucket. Note 2: You will need to apply a KMS Key policy on the selected CMK in order for CloudTrail as a service to encrypt and decrypt log files using the CMK provided. Steps are provided here for editing the selected CMK Key policy. Step 7. Click on Save . Step 8. You will see a notification message stating that you need to have decrypt permissions on the specified KMS key to decrypt log files. Step 9. Click on Yes. Perform the following to configure CloudTrail to use SSE-KMS via CLI Run the following command: aws cloudtrail update-trail --name \\<trail_name> --kms-id \\<cloudtrail_kms_key> aws kms put-key-policy --key-id \\<cloudtrail_kms_key> --policy \\<cloudtrail_kms_key_policy> Ensure rotation for customer created CMKs is enabled (2.8) The AWS Key Management Service (KMS) allows customers to rotate the backing key. The backing key is key material stored within the KMS and is tied to the key ID of the Customer Created Customer Master Key (CMK). It is the backing key that is used to perform cryptographic operations such as encryption and decryption. Automated key rotation currently retains all prior backing keys so that decryption of encrypted data can take place transparently. It is recommended that CMK key rotation be enabled. Perform the following to enable customer-created CMK rotation via the AWS Management Console: Step 1. Log in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam . Step 2. In the left navigation pane, click on Encryption Keys . Step 3. Select a customer-created master key (CMK). Step 4. Under the \"Key Policy\" section, move down to \"Key Rotation.\"\" Step 5. Check the \"Rotate this key every year\" checkbox. Perform the following to enable customer-created CMK rotation via CLI: Run the following command: aws kms enable-key-rotation --key-id \\<kms_key_id> Ensure VPC flow logging is enabled in all VPCs (2.9) VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. After you've created a flow log, you can view and retrieve its data in Amazon CloudWatch Logs. It is recommended that VPC Flow Logs be enabled for packet \"Rejects\" for VPCs. Perform the following to determine if the VPC Flow logs feature is enabled via the AWS Management Console: Step 1. Log in to the AWS Management Console at https://console.aws.amazon.com/vpc/home . Step 2. Click on Services and then click on VPC . Step 3. In the left navigation pane, click on Your VPCs. Step 4. Select a VPC. Step 5. In the right pane, click on the \"Flow Logs\" tab. Step 6. If no Flow Log exists, click on Create Flow Log. Step 7. For \"Filter,\"\" select Reject. Step 8. Enter in a \"Role\" and \"Destination Log Group.\"\" Step 9. Click on Create Log Flow. Step 10. Click on CloudWatch Logs Group Note: Setting the filter to \"Reject\" will dramatically reduce the accumulation of logging data while providing sufficient information for the purposes of breach detection, research, and remediation. However, during periods of least privilege security group engineering, setting this filter to \"All\" can be very helpful in discovering the existing traffic flows required for the proper operation of an already running environment. By default, CloudWatch Logs will store logs indefinitely unless a specific retention period is defined for the log group. When choosing the number of days to retain logs, keep in mind that, on average, it takes a typical organization 210 days (at the time of this writing) to realize they have been breached. Since additional time is required to research a breach, a minimum 365-day retention policy allows time for detection and research. You may also wish to archive the logs to a cheaper storage service rather than simply deleting them. See the following AWS resource to manage CloudWatch Logs retention periods: http://docs.aws.amazon.com/AmazonCloudWatch/latest/DeveloperGuide/SettingLogRetention.html","title":"Logging"},{"location":"Configuration%20Guides/aws/#monitoring","text":"Real-time monitoring of API calls can be achieved by directing CloudTrail Logs to CloudWatch Logs and establishing corresponding metric filters and alarms. It is strongly recommended that a metric filter and alarm be established for detecting: Unauthorized API calls (3.1) Console logins that are not protected by multi-factor authentication (MFA) (3.2) Root login attempts (3.3) Changes made to Identity and Access Management (IAM) policies (3.4) Changes to CloudTrail's configurations (3.5) Failed console authentication attempts (3.6) Customer-created CMKs which have changed state to disabled or scheduled deletion (3.7) Changes to S3 buckets (3.8) Changes to AWS Config configuration (3.9) Changes made to Security Groups ( Note: Security Groups are a stateful packet filter that controls ingress and egress traffic within a VPC.) (3.10) Changes made to NACLs (3.11) Changes to the network (3.12) Changes to route tables (3.13) Changes made to VPCs ( Note: It is possible to have more than 1 VPC within an account. In addition, it is also possible to create a peer connection between 2 VPCs, which enables network traffic to route between VPCs) (3.14) Perform the following to set up the metric filter, alarm, SNS topic, and subscription via the AWS Management Console: Step 1. a) Detect unauthorized API calls Create a metric filter based on the filter pattern provided which checks for unauthorized API calls and the \\<cloudtrail_log_group_name> taken from audit step 1. aws logs put-metric-filter --log-group-name \\<cloudtrail_log_group_name> -- filter-name \\<unauthorized_api_calls_metric> --metric-transformations metricName= \\<unauthorized_api_calls_metric> ,metricNamespace='CISBenchmark',metricValue=1 --filter-pattern '{ ($.errorCode = \"UnauthorizedOperation\") || ($.errorCode = \"AccessDenied\") }' b) Detect console logins that are not protected by multi-factor authentication (MFA) Create a metric filter based on the filter pattern provided which checks for AWS Management Console sign-in without MFA and the \\<cloudtrail_log_group_name> taken from audit step 1. aws logs put-metric-filter --log-group-name \\<cloudtrail_log_group_name> -- filter-name \\<no_mfa_console_signin_metric> --metric-transformations metricName= \\<no_mfa_console_signin_metric> ,metricNamespace='CISBenchmark',metricValue=1 --filter-pattern '{ ($.eventName = \"ConsoleLogin\") && ($.additionalEventData.MFAUsed != \"Yes\") }' c) Detect root login attempts Create a metric filter based on the filter pattern provided which checks for \"Root'' account usage and the \\<cloudtrail_log_group_name> taken from audit step 1. aws logs put-metric-filter --log-group-name \\<cloudtrail_log_group_name> -- filter-name \\<root_usage_metric> --metric-transformations metricName= \\<root_usage_metric> ,metricNamespace='CISBenchmark',metricValue=1 --filterpattern '{ $.userIdentity.type = \"Root\" && $.userIdentity.invokedBy NOT EXISTS && $.eventType != \"AwsServiceEvent\" }' d) Detect changes made to Identity and Access Management (IAM) policies Create a metric filter based on filter pattern provided which checks for IAM policy changes and the \\<cloudtrail_log_group_name> taken from audit step 1. aws logs put-metric-filter --log-group-name \\<cloudtrail_log_group_name> -- filter-name \\<iam_changes_metric> --metric-transformations metricName= \\<iam_changes_metric> ,metricNamespace='CISBenchmark',metricValue=1 -- filter-pattern '{($.eventName=DeleteGroupPolicy)||($.eventName=DeleteRolePolicy)||($.eventNa me=DeleteUserPolicy)||($.eventName=PutGroupPolicy)||($.eventName=PutRolePolic y)||($.eventName=PutUserPolicy)||($.eventName=CreatePolicy)||($.eventName=Del etePolicy)||($.eventName=CreatePolicyVersion)||($.eventName=DeletePolicyVersi on)||($.eventName=AttachRolePolicy)||($.eventName=DetachRolePolicy)||($.event Name=AttachUserPolicy)||($.eventName=DetachUserPolicy)||($.eventName=AttachGr oupPolicy)||($.eventName=DetachGroupPolicy)}' e) Detect changes to CloudTrail's configurations Create a metric filter based on the filter pattern provided which checks for cloudtrail configuration changes and the \\<cloudtrail_log_group_name> taken from audit step 1. aws logs put-metric-filter --log-group-name \\<cloudtrail_log_group_name> -- filter-name \\<cloudtrail_cfg_changes_metric> --metric-transformations metricName= \\<cloudtrail_cfg_changes_metric> ,metricNamespace='CISBenchmark',metricValue=1 --filter-pattern '{ ($.eventName = CreateTrail) || ($.eventName = UpdateTrail) || ($.eventName = DeleteTrail) || ($.eventName = StartLogging) || ($.eventName = StopLogging) }' f) Detect failed console authentication attempts Create a metric filter based on the filter pattern provided which checks for AWS management Console Login Failures and the \\<cloudtrail_log_group_name> taken from audit step 1. aws logs put-metric-filter --log-group-name \\<cloudtrail_log_group_name> -- filter-name \\<console_signin_failure_metric> --metric-transformations metricName= \\<console_signin_failure_metric> ,metricNamespace='CISBenchmark',metricValue=1 --filter-pattern '{ ($.eventName = ConsoleLogin) && ($.errorMessage = \"Failed authentication\") }' g) Detect customer-created CMKs which have changed state to disabled or scheduled deletion Create a metric filter based on the filter pattern provided which checks for disabled or scheduled for deletion CMK's and the \\<cloudtrail_log_group_name> taken from audit step 1. aws logs put-metric-filter --log-group-name \\<cloudtrail_log_group_name> -- filter-name \\<disable_or_delete_cmk_changes_metric> --metrictransformations metricName= \\<disable_or_delete_cmk_changes_metric> ,metricNamespace='CISBenchmark',metricValue=1 --filter-pattern '{($.eventSource = kms.amazonaws.com) && (($.eventName=DisableKey)||($.eventName=ScheduleKeyDeletion)) }' h) Detect changes to S3 buckets Create a metric filter based on the filter pattern provided which checks for S3 bucket policy changes and the \\<cloudtrail_log_group_name> taken from audit step 1. aws logs put-metric-filter --log-group-name \\<cloudtrail_log_group_name> -- filter-name \\<s3_bucket_policy_changes_metric> --metric-transformations metricName= \\<s3_bucket_policy_changes_metric> ,metricNamespace='CISBenchmark',metricValue=1 --filter-pattern '{ ($.eventSource = s3.amazonaws.com) && (($.eventName = PutBucketAcl) || ($.eventName = PutBucketPolicy) || ($.eventName = PutBucketCors) || ($.eventName = PutBucketLifecycle) || ($.eventName = PutBucketReplication) || ($.eventName = DeleteBucketPolicy) || ($.eventName = DeleteBucketCors) || ($.eventName = DeleteBucketLifecycle) || ($.eventName = DeleteBucketReplication)) }' i) Detect changes to AWS Config configuration Create a metric filter based on the filter pattern provided which checks for AWS Configuration changes and the \\<cloudtrail_log_group_name> taken from audit step 1. aws logs put-metric-filter --log-group-name \\<cloudtrail_log_group_name> -- filter-name \\<aws_config_changes_metric> --metric-transformations metricName= \\<aws_config_changes_metric> ,metricNamespace='CISBenchmark',metricValue=1 --filter-pattern '{ ($.eventSource = config.amazonaws.com) && (($.eventName=StopConfigurationRecorder)||($.eventName=DeleteDeliveryChannel) ||($.eventName=PutDeliveryChannel)||($.eventName=PutConfigurationRecorder)) }' Note: You can choose your own metricName and metricNamespace strings. Using the same metricNamespace for all Foundations Benchmark metrics will group them together. j) Detect changes made to Security Groups Create a metric filter based on the filter pattern provided which checks for security groups changes and the \\<cloudtrail_log_group_name> taken from audit step 1. aws logs put-metric-filter --log-group-name \\<cloudtrail_log_group_name> -- filter-name \\<security_group_changes_metric> --metric-transformations metricName= \\<security_group_changes_metric> ,metricNamespace='CISBenchmark',metricValue=1 --filter-pattern '{ ($.eventName = AuthorizeSecurityGroupIngress) || ($.eventName = AuthorizeSecurityGroupEgress) || ($.eventName = RevokeSecurityGroupIngress) || ($.eventName = RevokeSecurityGroupEgress) || ($.eventName = CreateSecurityGroup) || ($.eventName = DeleteSecurityGroup) }' k) Changes made to NACLs Create a metric filter based on the filter pattern provided which checks for NACL changes and the \\<cloudtrail_log_group_name> taken from audit step 1. aws logs put-metric-filter --log-group-name \\<cloudtrail_log_group_name> -- filter-name \\<nacl_changes_metric> --metric-transformations metricName= \\<nacl_changes_metric> ,metricNamespace='CISBenchmark',metricValue=1 -- filter-pattern '{ ($.eventName = CreateNetworkAcl) || ($.eventName = CreateNetworkAclEntry) || ($.eventName = DeleteNetworkAcl) || ($.eventName = DeleteNetworkAclEntry) || ($.eventName = ReplaceNetworkAclEntry) || ($.eventName = ReplaceNetworkAclAssociation) }' l) Changes to the network Create a metric filter based on the filter pattern provided which checks for network gateways changes and the \\<cloudtrail_log_group_name> taken from audit step 1. aws logs put-metric-filter --log-group-name \\<cloudtrail_log_group_name> -- filter-name \\<network_gw_changes_metric> --metric-transformations metricName= \\<network_gw_changes_metric> ,metricNamespace='CISBenchmark',metricValue=1 --filter-pattern '{ ($.eventName = CreateCustomerGateway) || ($.eventName = DeleteCustomerGateway) || ($.eventName = AttachInternetGateway) || ($.eventName = CreateInternetGateway) || ($.eventName = DeleteInternetGateway) || ($.eventName = DetachInternetGateway) }' m) Detect changes to route tables Create a metric filter based on the filter pattern provided which checks for route table changes and the \\<cloudtrail_log_group_name> taken from audit step 1. aws logs put-metric-filter --log-group-name \\<cloudtrail_log_group_name> -- filter-name \\<route_table_changes_metric> --metric-transformations metricName= \\<route_table_changes_metric> ,metricNamespace='CISBenchmark',metricValue=1 --filter-pattern '{ ($.eventName = CreateRoute) || ($.eventName = CreateRouteTable) || ($.eventName = ReplaceRoute) || ($.eventName = ReplaceRouteTableAssociation) || ($.eventName = DeleteRouteTable) || ($.eventName = DeleteRoute) || ($.eventName = DisassociateRouteTable) }' n) Detect changes made to VPCs Create a metric filter based on filter pattern provided which checks for VPC changes and the \\<cloudtrail_log_group_name> taken from audit step 1. aws logs put-metric-filter --log-group-name \\<cloudtrail_log_group_name> -- filter-name \\<vpc_changes_metric> --metric-transformations metricName= \\<vpc_changes_metric> ,metricNamespace='CISBenchmark',metricValue=1 -- filter-pattern '{ ($.eventName = CreateVpc) || ($.eventName = DeleteVpc) || ($.eventName = ModifyVpcAttribute) || ($.eventName = AcceptVpcPeeringConnection) || ($.eventName = CreateVpcPeeringConnection) || ($.eventName = DeleteVpcPeeringConnection) || ($.eventName = RejectVpcPeeringConnection) || ($.eventName = AttachClassicLinkVpc) || ($.eventName = DetachClassicLinkVpc) || ($.eventName = DisableVpcClassicLink) || ($.eventName = EnableVpcClassicLink) }' Note: You can choose your own metricName and metricNamespace strings. Using the same metricNamespace for all Foundations Benchmark metrics will group them together. Step 2. Create an SNS topic that the alarm will notify aws sns create-topic --name \\<sns_topic_name> Note: You can execute this command once and then reuse the same topic for all monitoring alarms. Step 3. Create an SNS subscription to the topic created in step 2 aws sns subscribe --topic-arn \\<sns_topic_arn> --protocol \\<protocol_for_sns> - -notification-endpoint \\<sns_subscription_endpoints> Note: You can execute this command once and then reuse the SNS subscription for all monitoring alarms. Step 4. Create an alarm that is associated with the CloudWatch Logs Metric Filter created in step 1 and an SNS topic created in step 2 aws cloudwatch put-metric-alarm --alarm-name \\<x1> -- metric-name \\<x2> --statistic Sum --period 300 --threshold 1 --comparison-operator GreaterThanOrEqualToThreshold --evaluation-periods 1 --namespace 'CISBenchmark' --alarm-actions \\<sns_topic_arn> Depending on the rule you chose in step 1, continue with that rule now, replacing \\<x1> and \\<x2> as per either: a) Detect unauthorized API calls \\<x1> = \\<unauthorized_api_calls_alarm> \\<x2> = \\<unauthorized_api_calls_metric> b) Console logins that are not protected by multi-factor authentication (MFA) \\<x1> = \\<no_mfa_console_signin_alarm> \\<x2> = \\<no_mfa_console_signin_metric> c) Detect root login attempts \\<x1> = \\<root_usage_alarm> \\<x2> = \\<root_usage_metric> d) Changes made to Identity and Access Management (IAM) policies \\<x1> = \\<iam_changes_alarm> \\<x2> = \\<iam_changes_metric> e) Changes to CloudTrail's configurations \\<x1> = \\<cloudtrail_cfg_changes_alarm> \\<x2> = \\<cloudtrail_cfg_changes_metric> f) Failed console authentication attempts \\<x1> = \\<console_signin_failure_alarm> \\<x2> = \\<console_signin_failure_metric> g) Customer-created CMKs which have changed state to disabled or scheduled deletion \\<x1> = \\<disable_or_delete_cmk_changes_alarm> \\<x2> = \\<disable_or_delete_cmk_changes_metric> h) Detect changes to S3 buckets \\<x1> = \\<s3_bucket_policy_changes_alarm> \\<x2> = \\<s3_bucket_policy_changes_metric> i) Detect changes to AWS Config configuration \\<x1> = \\<aws_config_changes_alarm> \\<x2> = \\<aws_config_changes_metric> j) Detect changes made to Security Groups \\<x1> = \\<security_group_changes_alarm> \\<x2> = \\<security_group_changes_metric> k) Changes made to NACLs \\<x1> = \\<nacl_changes_alarm> \\<x2> = \\<nacl_changes_metric> l) Changes to the network \\<x1> = \\<network_gw_changes_alarm> \\<x2> = \\<network_gw_changes_metric> m) Detect changes to route tables \\<x1> = \\<route_table_changes_alarm> \\<x2> = \\<route_table_changes_metric> n) Detect changes made to VPCs \\<x1> = \\<vpc_changes_alarm> \\<x2> = \\<vpc_changes_metric>","title":"Monitoring"},{"location":"Configuration%20Guides/aws/#networking","text":"Ingress access (4.1 and 4.2) Ensure no security groups allow ingress from 0.0.0.0/0 to port 3389 Security groups provide stateful filtering of ingress/egress network traffic to AWS resources. It is recommended that no security group allows unrestricted ingress access to port 3389 and port 22. Perform the following to remove unrestricted ingress access to port 3389 and port 22 via the AWS Management Console: Step 1. Log in to the AWS Management Console at https://console.aws.amazon.com/vpc/home Step 2. In the left pane, click on Security Groups Step 3. For each security group, perform the following: Select the security group. Click on the Inbound Rules tab. Identify the rules to be removed. Click on x in the \"Remove\" column. Click on Save. Ensure the default security group of every VPC restricts all traffic (4.3) A VPC comes with a default security group whose initial settings: Deny all inbound traffic Allow all outbound traffic Allow all traffic between instances assigned to the security group. If you don't specify a security group when you launch an instance, the instance is automatically assigned to this default security group. Security groups provide stateful filtering of ingress/egress network traffic to AWS resources. It is recommended that the default security group restrict all traffic. The default VPC in every region should have its default security group updated to comply. Any newly created VPCs will automatically contain a default security group that will need remediation to comply with this recommendation. Security group members can perform the following to implement the prescribed state via the AWS Management Console: Step 1. Identify the AWS resources that exist within the default security group. Step 2. Create a set of least privilege security groups for those resources. Step 3. Place the resources in those security groups. Step 4. Remove the resources noted in #1 from the default security group Security Group State Step 5. Log in to the AWS Management Console at https://console.aws.amazon.com/vpc/home . Step 6. Repeat the next steps for all VPCs, including the default VPC in each AWS region. Step 7. In the left pane, click on Security Groups. Step 8. For each default security group, perform the following: Select the default security group. Click on the Inbound Rules tab. Remove any inbound rules. Click on the Outbound Rules tab. Remove any outbound rules. Recommended: IAM groups allow you to edit the \"name\" field. After remediating default groups rules for all VPCs in all regions, edit this field to add text similar to \"DO NOT USE. DO NOT ADD RULES.\" Ensure routing tables for VPC peering are \"least access\" (4.4) Once a VPC peering connection is established, routing tables must be updated to establish any connections between the peered VPCs. These routes can be as specific as desired. You can even peer a VPC to just a single host on the other side of the connection. Remove and add route table entries to ensure that the least number of subnets or hosts is required to accomplish the purpose for peering are routable. To give VPC peering routing tables \"least access\" via CLI: Step 1. For each \\<route_table_id> containing routes non compliant with your routing policy (which grants more than desired \"least access\"), delete the non compliant route: aws ec2 delete-route --route-table-id \\<route_table_id> --destination-cidrblock \\<non_compliant_destination_CIDR> Step 2. Create a new compliant route: aws ec2 create-route --route-table-id \\<route_table_id> --destination-cidrblock \\<compliant_destination_CIDR> --vpc-peering-connection-id \\<peering_connection_id>","title":"Networking"},{"location":"Configuration%20Guides/aws/#other-rules","text":"Ensure that GuardDuty enabled for required region By enabling GuardDuty, you will receive a threat detection service. This service continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts, workloads, and data stored in Amazon S3 by event logs analysis. We are excluding verification for aws-global region from possible regions for GuardDuty. Perform the following steps under the below link to enable GuardDuty: https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_settingup.html Ensure there no stale roles with Attached Policies for S3 access Avoid stale roles as these could cause access leakage and uncontrolled manipulation with S3 bucket data, which can lead to ransomware violations. This rule checks inline policies only. The Attached policies are verified under the respective rule. Perform the following to delete stale roles via the AWS Management Console: Step 1. Log in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/ . Step 2. In the navigation pane of the IAM console, click on Roles . Then select the check box next to the role name you want to delete, not the name or row itself. Step 3. For \"Role actions\" at the top of the page, click on Delete . Step 4. In the confirmation dialog box, review the last accessed information to see when each of the selected roles last accessed an AWS service. This helps you to confirm whether the role is currently active. If you want to proceed, click on Yes, Delete to submit the service-linked role for deletion. Step 5. Observe the IAM console notifications to monitor the progress of the service-linked role deletion. Because the IAM service-linked role deletion is asynchronous, after you submit the role for deletion, the deletion task can succeed or fail. If the task succeeds, then the role is removed from the list and notification of success appears at the top of the page. If the task fails, you can click on View details or View Resources from the notifications to learn why the deletion failed. If the deletion fails because the role uses the service's resources, then the notification includes a list of resources, if the service returns that information. You can then clean up the resources and submit the deletion again. https://aws.amazon.com/blogs/security/identify-unused-iam-roles-remove-confidently-last-used-timestamp/ https://docs.aws.amazon.com/IAM/latest/UserGuide/using-service-linked-roles.html#delete-service-linked-role Ensure all EC2 EBS Volumes has snapshots Snapshot availability could prevent data loss and simplify data recovery in the case of data encryption. Perform the following to create snapshots for EC2 EBS Volumes via the AWS Management Console: Step 1. Log in to the Amazon EC2 console at https://console.aws.amazon.com/ec2/ . Step 2. Click on Snapshots under the Elastic Block Store in the navigation pane. Step 3. Click on Create Snapshot . Step 4. For \"Select resource type\" click on Volume . Step 5. For \"Volume,\"\" select the volume. Step 6 (Optional). Enter a description for the snapshot. Step 7 (Optional). Choose Add Tag to add tags to your snapshot. For each tag, provide a tag key and a tag value. Step 8. Click on Create Snapshot . Ensure all EC2 instances are managed by SSM Systems Manager (SSM) helps you maintain security and compliance by scanning your managed instances and reporting on or taking corrective action on any policy violations that it detects. In cases where SSM is enabled for a specific region, but EC2 instances are not managed, they will not be monitored from any kind of SSM perspective. Perform the following to configure EC2 instances for use with SMM via the AWS Management Console: Step 1. Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/ . Step 2. In the navigation pane, under \"Instances\" click on Instances . Step 3. Navigate to and choose your EC2 instance from the list. Step 4. In the \"Actions\" menu, click on Security , Modify IAM role. Step 5. For the IAM role, select the instance profile you created for that perspective. Ensure that SecurityHub enabled for required region AWS Security Hub gives you a comprehensive view of your security alerts and security posture across your AWS accounts. SecurityHub is a cloud security posture management service that automatically checks for best security practices, bundles alerts, and automates remediation. AWS security group allows access to known command and control destinations The AWS security group allows access to destinations that are known or suspected of being command and control systems used in ransomware and botnet attacks. Perform the following to gain access to known command and control destinations with the minimum required connectivity via the AWS Management Console: Step 1. Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/ and click on Security Groups . Step 2. For each group found by this rule: Select the given rule. Click on the \"Outbound Rules\" tab. Click on Edit outbound rules . Edit any CIDR/IP addresses and minimize the permitted scope to just the minimum required for connectivity. Ensure that S3 Bucket has MFA option enabled for changing Bucket Versioning settings and permanently deleting object versions S3 Buckets should be protected from ransomware attacks by configuring versioning and MFA Delete. Doing so will disallow immediate bucket content removal, data encryption, or any other harmful modifications. Disabled versioning is also considered a violation by this rule. The reason for that is that the attacker may make the bucket vulnerable by disabling object versioning with the s3:PutBucketVersioning permission. Perform the following to enable S3 bucket versioning and MFA Delete via the AWS Management Console: Step 1. Log in to the AWS Management Console and open the S3 console at https://console.aws.amazon.com/s3 . Step 2. Under \"All Buckets,\"\" click on the target S3 bucket. Step 3. Click on the Properties tab. Step 4. Find the \"Bucket Versioning\" section. Step 5. Click on Edit and enable versioning. Step 6. Follow the AWS instructions to add the MFA Delete option via CLI or SDK. Ensure S3 bucket deny overriding of default KMS Key encryption Attackers that have S3:PutObject permission can still override the default KMS Key encryption on updating object operation with their own provided KMS Key, thus leading to a ransomware violation. To prevent this method of malicious access, the bucket owner should therefore define a policy to allow object modification using only the defined default KMS Key, which attackers are unlikely to have permissions to change or modify. Perform the following to enable S3 bucket policy control over the KMS Key via the AWS Management Console: Step 1. Log in to the AWS Management Console and open the S3 console at https://console.aws.amazon.com/s3 . Step 2. In the Buckets list, choose the name of the bucket that you want to create a bucket policy for or whose bucket policy you want to edit. Step 3. Click on Permissions . Step 4. Under \"Bucket policy,\"\" click on Edit . Step 5. In the \"Policy\" text field, type or copy and paste a new bucket policy, or edit an existing policy. The bucket policy is a JSON file. The text you type in the editor must be valid JSON. Step 6. Add policy Condition statement where you define the KMS Key to be matched for Statement with s3:PutObject permission Step 7. Condition example for Allow effect for: \"Condition\":{\"StringEquals\":{\"s3:x-amz-server-side-encryption-aws-kms-key-id\":\"arn:aws:kms:REGION:ACCOUNT-ID:key/KEY-ID\"}}} Note: Reference: https://rhinosecuritylabs.com/aws/s3-ransomware-part-2-prevention-and-defense/#:~:text=Another%20feature%20of,be%20found%20here . https://docs.aws.amazon.com/AmazonS3/latest/userguide/add-bucket-policy.html https://docs.aws.amazon.com/service-authorization/latest/reference/list_amazons3.html Ensure that S3 Bucket restrict public access by ACL and policy S3 Buckets should not be reachable outside of the project by default. Any violation of this rule could cause major vulnerabilities and data loss Perform the following to restrict public access to S3 bucket via the AWS Management Console: Step 1. Log in to the AWS Management Console and open the S3 console at https://console.aws.amazon.com/s3 . Step 2. On the left side menu, find the option \"Block Public Access settings for this account.\"\" Step 3. Edit the settings and click on the required level of restriction. The option to \"Block all public access\" is preferable and will set it globally. Step 4. Open the bucket, then open the tab \"Permissions.\" Step 5. Find the \"Access control list (ACL)\" part and ensure only the bucket owner has access to it. Step 6. Find \"Bucket policy\" and check for any public configuration like \"principal\": * etc. Note: Reference: https://docs.amazonaws.cn/en_us/AmazonS3/latest/userguide/access-control-block-public-access.html https://aws.amazon.com/s3/features/block-public-access/ Ensure S3 bucket has no server-side encryption being enabled by another account S3 bucket policies allow uploaded files to be encrypted with AES256 or a specific AWS KMS key. In the case when a specified KMS Key belongs to a different account, it can lead to a high risk of data encryption without the ability to decrypt it. Perform the following to clear cross-account KMS Key on S3 via the AWS Management Console: Step 1. Log in to the AWS Management Console and open the S3 console at https://console.aws.amazon.com/s3 . Step 2. In the \"Buckets\" list, choose a bucket name. Step 3. Click on Properties . Step 4. Under \"Default encryption,\"\" click on Edit . Step 5. Remove the cross-account KMS Key and specify the account-related KMS Key for data encryption Step 6. Consider the rule aws_058 aws_s3_prevent_default_kms_key_override.yaml remediation to prevent such violations in the future.","title":"Other Rules"},{"location":"Configuration%20Guides/cloud-taxonomy/","text":"This is the cloud taxonomy, adopted from https://comparecloud.in that we use across our open-source projects and documentation. We use this taxonomy to name and group Magpie security rules and to organize associated configuration guidance. The mapping of which AWS , GCP and Azure services map to which category can be found on https://comparecloud.in and in the open-source Github repo https://github.com/ilyas-it83/CloudComparer/ Service Rule Name Prefix Compute compute Storage storage Database database Migration Services migration-services Application Delivery Services application-delivery-services Network and Content Delivery network-and-content-delivery Network/network Hybrid Cloud hybrid-cloud Developer Tools developer-tools Management Tools management-tools Disaster Recovery Services disaster-recovery-services Security and Identity, Compliance iam-and-security Big Data and Advanced Analytics big-data-and-analytics Artificial Intelligence / Machine Learning machine-learning Mobile Services mobile-services Application Services application-services Business Productivity business-productivity Internet of Things internet-of-things Game Development game-development Robotics Development robotics-development Development and Testing development-and-testing All others misc","title":"Cloud taxonomy"},{"location":"Configuration%20Guides/gcp/","text":"Ensure that instances are not configured to use the default service account It is recommended to configure your instance to not use the default Compute Engine service account because it has the Editor role on the project. From Console: 1. Go to the VM instances page by visiting: https://console.cloud.google.com/compute/instances. 2. Click on the instance name to go to its VM instance details page. 3. Click STOP and then click EDIT . 4. Under the section Service Account , select a service account other than the default Compute Engine service account. You may first need to create a new service account. 5. Click Save and then click START . From Command Line: 1. Stop the instance: gcloud compute instances stop INSTANCE_NAME 2. Update the instance: gcloud compute instances set-service-account INSTANCE_NAME --service-account=SERVICE_ACCOUNT 3. Restart the instance: gcloud compute instances start INSTANCE_NAME Ensure that IAM users are not assigned the Service Account User or Service Account Token Creator roles at project level It is recommended to assign the Service Account User (iam.serviceAccountUser) and Service Account Token Creator (iam.serviceAccountTokenCreator) roles to a user for a specific service account rather than assigning the role to a user at project level From Console: 1. Go to the IAM page in the GCP Console by visiting: https://console.cloud.google.com/iam-admin/iam. 2. Click on the filter table text bar. Type Role: Service Account User 3. Click the Delete Bin icon in front of the role Service Account User for every user listed as a result of a filter. 4. Click on the filter table text bar. Type Role: Service Account Token Creator 5. Click the Delete Bin icon in front of the role Service Account Token Creator for every user listed as a result of a filter. From Command Line: 1. Using a text editor, remove the bindings with the roles/iam.serviceAccountUser or roles/iam.serviceAccountTokenCreator. For example, you can use the iam.json file shown below as follows: ``` { \"bindings\": [ { \"members\": [ \"serviceAccount:our-project-123@appspot.gserviceaccount.com\", ], \"role\": \"roles/appengine.appViewer\" }, { \"members\": [ \"user:email1@gmail.com\" ], \"role\": \"roles/owner\" }, { \"members\": [ \"serviceAccount:our-project-123@appspot.gserviceaccount.com\", \"serviceAccount:123456789012-compute@developer.gserviceaccount.com\" ], \"role\": \"roles/editor\" } ], \"etag\": \"BwUjMhCsNvY=\" } 2. Update the project's IAM policy: gcloud projects set-iam-policy PROJECT_ID iam.json``` Ensure that there are only GCP-managed service account keys for each service account User managed service accounts should not have user-managed keys From Console 1. Go to the IAM page in the GCP Console using https://console.cloud.google.com/iam-admin/iam 2. In the left navigation pane, click Service accounts. All service accounts and their corresponding keys are listed. 3. Click the service account. 4. Click the edit and delete the keys. From CLI To delete a user managed Service Account Key: gcloud iam service-accounts keys delete --iam-account=","title":"Gcp"},{"location":"Configuration%20Guides/guides/","text":"As part of the Magpie project, we maintain a set of security configuration guides to help engineers remediate potential issues that have been found. There are currently two guides, both with content organized around a common multi-cloud taxonomy that our security rules also follow. AWS Security Configuration Guide GCP Security Configuration Guide","title":"Guides"},{"location":"Developer%20Docs/GCP-discovery-plugin/","text":"GCP Client dependencies Required maven dependencies are as follows: <dependencyManagement> <dependencies> <dependency> <groupId>com.google.cloud</groupId> <artifactId>libraries-bom</artifactId> <version>20.2.0</version> <type>pom</type> <scope>import</scope> </dependency> . . . </dependencies> </dependencyManagement> <dependencies> <dependency> <groupId>com.google.cloud</groupId> <artifactId>google-cloud-bigtable</artifactId> </dependency> <dependencies> Similar to AWS each service have different dependency. In above example it's bigtable Example usage: // Lists bigdata tables in the current instance. try { List<String> tableIds = adminClient.listTables(); for (String tableId : tableIds) { System.out.println(tableId); } } catch (NotFoundException e) { System.err.println(\"Failed to list tables from a non-existent instance: \" + e.getMessage()); } More detailed information available here https://cloud.google.com/java/docs/setup Authentication methods If no credentails are specified in code. GCP will look for credentials in environment varialbes such as GOOGLE_APPLICATION_CREDENTIALS To use this authenitication method use save .json file with authentication key to file then set: export GOOGLE_APPLICATION_CREDENTIALS=PATH_TO_CREDENTIALS_JSON_FILE Top services to support Some popular services we should start with: - Google Kubernates Engine - listClusters() - BigQuerry - listDatasets() - Cloud Functions - listFunctions() - Secret Manager - listSecrets() - Memorystore for Redis - listInstances() - Memorystore for Memcache - listInstances() - IoT Core - listDevices() - Data Catalog - listEntries() - Cloud Tasks - listTask(), listQueues() - Cloud Key Management Service - listCryptoKeys() - BigTable - listInstances(), listClusters() - AutoML - listModels(), listDatasets() Api references here https://cloud.google.com/java/docs/reference Persistence For persistance the logical solution is to create class similar to AWSResource flow, but with more general member and class names to fit GCP and other cloud services in future public class CloudResource { public String documentId; public String uniqueId; public String resourceName; public String resourceId; public String resourceType; public String region; public String accountId; public String createdIso; public String updatedIso; public String discoverySessionId; public Long maxSizeInBytes = null; public Long sizeInBytes = null; public JsonNode configuration; public JsonNode supplementaryConfiguration; public JsonNode tags; public JsonNode discoveryMeta; }","title":"GCP Client dependencies"},{"location":"Developer%20Docs/GCP-discovery-plugin/#gcp-client-dependencies","text":"Required maven dependencies are as follows: <dependencyManagement> <dependencies> <dependency> <groupId>com.google.cloud</groupId> <artifactId>libraries-bom</artifactId> <version>20.2.0</version> <type>pom</type> <scope>import</scope> </dependency> . . . </dependencies> </dependencyManagement> <dependencies> <dependency> <groupId>com.google.cloud</groupId> <artifactId>google-cloud-bigtable</artifactId> </dependency> <dependencies> Similar to AWS each service have different dependency. In above example it's bigtable Example usage: // Lists bigdata tables in the current instance. try { List<String> tableIds = adminClient.listTables(); for (String tableId : tableIds) { System.out.println(tableId); } } catch (NotFoundException e) { System.err.println(\"Failed to list tables from a non-existent instance: \" + e.getMessage()); } More detailed information available here https://cloud.google.com/java/docs/setup","title":"GCP Client dependencies"},{"location":"Developer%20Docs/GCP-discovery-plugin/#authentication-methods","text":"If no credentails are specified in code. GCP will look for credentials in environment varialbes such as GOOGLE_APPLICATION_CREDENTIALS To use this authenitication method use save .json file with authentication key to file then set: export GOOGLE_APPLICATION_CREDENTIALS=PATH_TO_CREDENTIALS_JSON_FILE","title":"Authentication methods"},{"location":"Developer%20Docs/GCP-discovery-plugin/#top-services-to-support","text":"Some popular services we should start with: - Google Kubernates Engine - listClusters() - BigQuerry - listDatasets() - Cloud Functions - listFunctions() - Secret Manager - listSecrets() - Memorystore for Redis - listInstances() - Memorystore for Memcache - listInstances() - IoT Core - listDevices() - Data Catalog - listEntries() - Cloud Tasks - listTask(), listQueues() - Cloud Key Management Service - listCryptoKeys() - BigTable - listInstances(), listClusters() - AutoML - listModels(), listDatasets() Api references here https://cloud.google.com/java/docs/reference","title":"Top services to support"},{"location":"Developer%20Docs/GCP-discovery-plugin/#persistence","text":"For persistance the logical solution is to create class similar to AWSResource flow, but with more general member and class names to fit GCP and other cloud services in future public class CloudResource { public String documentId; public String uniqueId; public String resourceName; public String resourceId; public String resourceType; public String region; public String accountId; public String createdIso; public String updatedIso; public String discoverySessionId; public Long maxSizeInBytes = null; public Long sizeInBytes = null; public JsonNode configuration; public JsonNode supplementaryConfiguration; public JsonNode tags; public JsonNode discoveryMeta; }","title":"Persistence"},{"location":"Developer%20Docs/aws-service-discovery/","text":"AWS Magpie supports AWS as a core plugin out of the box. Checked boxes are complete and available today, the unchecked are on the roadmap for completion. We have already built the code for all services in the list, but need to port them over from a previous framework. [x] EC2 [x] S3 [x] Athena [x] Batch [x] Backup [x] Cassandra [x] Cloudfront [x] Cloudsearch [x] Cloudtrail [x] CloudWatch [x] DynamoDB [x] EB [x] ECS [x] EFS [x] EKS [x] Elastic Cache [x] ELB [x] ELBv2 [x] EMR [x] ESS [x] FSX [x] Glacier [x] GuardDuty [x] IAM [x] KMS [x] Lakeformation [x] Lambda [x] Lightsail [x] QLDB [x] RDS [x] Redshift [x] Route 53 [x] Secrets Manager [x] SecurityHub [x] SNS [x] SSM [x] Storage Gateway [x] VPC Per region discovery By default the Magpie AWS Plugin will run discovery in all regions. To narrow down discovery to a subset of regions edit the plugins.magpie.aws.discovery.config.regions value to an array of desired region names, for example: plugins: magpie.aws.discovery: enabled: true config: regions: - us-east-2 - us-east-1","title":"Aws service discovery"},{"location":"Developer%20Docs/aws-service-discovery/#aws","text":"Magpie supports AWS as a core plugin out of the box. Checked boxes are complete and available today, the unchecked are on the roadmap for completion. We have already built the code for all services in the list, but need to port them over from a previous framework. [x] EC2 [x] S3 [x] Athena [x] Batch [x] Backup [x] Cassandra [x] Cloudfront [x] Cloudsearch [x] Cloudtrail [x] CloudWatch [x] DynamoDB [x] EB [x] ECS [x] EFS [x] EKS [x] Elastic Cache [x] ELB [x] ELBv2 [x] EMR [x] ESS [x] FSX [x] Glacier [x] GuardDuty [x] IAM [x] KMS [x] Lakeformation [x] Lambda [x] Lightsail [x] QLDB [x] RDS [x] Redshift [x] Route 53 [x] Secrets Manager [x] SecurityHub [x] SNS [x] SSM [x] Storage Gateway [x] VPC","title":"AWS"},{"location":"Developer%20Docs/aws-service-discovery/#per-region-discovery","text":"By default the Magpie AWS Plugin will run discovery in all regions. To narrow down discovery to a subset of regions edit the plugins.magpie.aws.discovery.config.regions value to an array of desired region names, for example: plugins: magpie.aws.discovery: enabled: true config: regions: - us-east-2 - us-east-1","title":"Per region discovery"},{"location":"Developer%20Docs/gcp-service-discovery/","text":"GCP Magpie also supports GCP as a core plugin out of the box. Checked boxes are complete and available today, the unchecked are on the roadmap for completion. [x] AI Platform Data Labeling Service [x] Access Approval [x] AutoML [x] BigQuery [ ] BigQuery Connection API [x] BigQuery Data Transfer Service [x] BigQuery Reservation API [ ] BigQuery Storage [x] Cloud Asset Inventory [x] Cloud Bigtable [x] Cloud Billing [x] Cloud Build [x] Cloud Data Loss Prevention [x] Cloud DNS [x] Cloud Functions [x] Cloud Key Management Service [x] Cloud Logging [x] Cloud Monitoring [x] Cloud Monitoring Dashboards [ ] Cloud Natural Language API [x] Cloud OS Config [ ] Cloud OS Login API [x] Cloud Spanner [x] Cloud Scheduler [x] Cloud Storage [x] Cloud Talent Solution Job Search [x] Cloud Tasks [x] Cloud Trace [x] Cloud Translation [x] Cloud Vision [x] Compute Engine [x] Container Analysis [x] Data Catalog [x] Dataproc [ ] Datastore [x] Dialogflow [x] Error Reporting [ ] Firestore [x] Game Servers [x] Google Kubernetes Engine [x] Identity and Access Management [x] Iam [x] IoT Core [ ] Media Translation [x] Memorystore for Memcached [x] Memorystore for Redis [ ] Phishing Protection [x] Pub/Sub [x] Pub/Sub Lite [ ] Recommendations AI [ ] Recommender [x] Resource Manager [x] Secret Manager [ ] Security Command Center [x] Service Directory [ ] Speech-to-Text [ ] Text-to-Speech [ ] Video Intelligence API [x] VPC [ ] Web Risk [x] Web Security Scanner [x] reCAPTCHA Enterprise","title":"Gcp service discovery"},{"location":"Developer%20Docs/gcp-service-discovery/#gcp","text":"Magpie also supports GCP as a core plugin out of the box. Checked boxes are complete and available today, the unchecked are on the roadmap for completion. [x] AI Platform Data Labeling Service [x] Access Approval [x] AutoML [x] BigQuery [ ] BigQuery Connection API [x] BigQuery Data Transfer Service [x] BigQuery Reservation API [ ] BigQuery Storage [x] Cloud Asset Inventory [x] Cloud Bigtable [x] Cloud Billing [x] Cloud Build [x] Cloud Data Loss Prevention [x] Cloud DNS [x] Cloud Functions [x] Cloud Key Management Service [x] Cloud Logging [x] Cloud Monitoring [x] Cloud Monitoring Dashboards [ ] Cloud Natural Language API [x] Cloud OS Config [ ] Cloud OS Login API [x] Cloud Spanner [x] Cloud Scheduler [x] Cloud Storage [x] Cloud Talent Solution Job Search [x] Cloud Tasks [x] Cloud Trace [x] Cloud Translation [x] Cloud Vision [x] Compute Engine [x] Container Analysis [x] Data Catalog [x] Dataproc [ ] Datastore [x] Dialogflow [x] Error Reporting [ ] Firestore [x] Game Servers [x] Google Kubernetes Engine [x] Identity and Access Management [x] Iam [x] IoT Core [ ] Media Translation [x] Memorystore for Memcached [x] Memorystore for Redis [ ] Phishing Protection [x] Pub/Sub [x] Pub/Sub Lite [ ] Recommendations AI [ ] Recommender [x] Resource Manager [x] Secret Manager [ ] Security Command Center [x] Service Directory [ ] Speech-to-Text [ ] Text-to-Speech [ ] Video Intelligence API [x] VPC [ ] Web Risk [x] Web Security Scanner [x] reCAPTCHA Enterprise","title":"GCP"},{"location":"Developer%20Docs/magpie-for-developers/","text":"Magpie Architecture Magpie relies on plugins for all its integration capabilities. They are the core of the framework and key to integration with both cloud providers and downstream processing and storage. Magpie is essentially a series of layers separated by FIFOs. Depending on the configuration, these FIFOs are either 1) Java queues (in the default configuration) or 2) Kafka queues . Using Kafka queues allows Magpie to run in a distributed and highly scalable fashion where each layer may exist on separate compute instances. Pre Reqs Java 11 is a prerequisite and must be installed to run Magpie. Out of the box Magpie supports AWS for the cloud provider and outputs discovery data to stdout in JSON format. The AWS plugin utilizes the AWS Java SDK and will search for credentials as described in Using Credentials . Assuming you have read credentials set up, you can start discovery by running: ./Active Projects Building Magpie from Source git clone git@github.com:openraven/Active Projects.git cd Active Projects mvn clean package install && mvn --projects Active Projects-cli assembly:single The distribution zip file will be located in magpie-cli/target/magpie-<version>.zip Alternatively you can download the latest snapshot build by going to Action->(choose latest) and click the magpie-cli artifact, which will download a zip distribution.","title":"Magpie for developers"},{"location":"Developer%20Docs/magpie-for-developers/#magpie-architecture","text":"Magpie relies on plugins for all its integration capabilities. They are the core of the framework and key to integration with both cloud providers and downstream processing and storage. Magpie is essentially a series of layers separated by FIFOs. Depending on the configuration, these FIFOs are either 1) Java queues (in the default configuration) or 2) Kafka queues . Using Kafka queues allows Magpie to run in a distributed and highly scalable fashion where each layer may exist on separate compute instances.","title":"Magpie Architecture"},{"location":"Developer%20Docs/magpie-for-developers/#pre-reqs","text":"Java 11 is a prerequisite and must be installed to run Magpie. Out of the box Magpie supports AWS for the cloud provider and outputs discovery data to stdout in JSON format. The AWS plugin utilizes the AWS Java SDK and will search for credentials as described in Using Credentials . Assuming you have read credentials set up, you can start discovery by running: ./Active Projects","title":"Pre Reqs"},{"location":"Developer%20Docs/magpie-for-developers/#building-magpie-from-source","text":"git clone git@github.com:openraven/Active Projects.git cd Active Projects mvn clean package install && mvn --projects Active Projects-cli assembly:single The distribution zip file will be located in magpie-cli/target/magpie-<version>.zip Alternatively you can download the latest snapshot build by going to Action->(choose latest) and click the magpie-cli artifact, which will download a zip distribution.","title":"Building Magpie from Source"},{"location":"Developer%20Docs/welcome/","text":"","title":"Welcome"}]}